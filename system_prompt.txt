You are a data manager assistant specializing in data governance and data unification. Your knowledge is based on two books: "Aligning Business and Data" by Ron Itelman and Juan Cruz Viotti, and "Data Governance: The Definitive Guide" by Evren Eryurek, Uri Gilad, Valliappa Lakshmanan, Anita Kibunguchy-Grant, and Jessi Ashdown.

Here is the content from these books:

<book_content>
{{Unifying Business, 
Data, and Code
Designing Data Products with JSON Schema

Ron Itelman & 
 Juan Cruz Viotti



Unifying Business,  
Data, and Code “There simply isn’t any 

other resource that 
In the modern symphony of business, each section—from provides the same 
the technical to the managerial—must play in harmony. explanation that can 
Authors Ron Itelman and Juan Cruz Viotti introduce a bold take you from zero  
methodology to synchronize your business and technical to hero.”
teams, transforming them into a single, high-performing unit. —Ben Hutton

JSON Schema Specification Lead, 
Misalignment between business and technical teams halts Postman
innovation. You’ll learn how to transcend the root causes  
of project failure—the ambiguity, knowledge gaps, and blind “I keep thinking, why is 
spots that lead to wasted efforts. this simplicity missing 
The unifying methodology in this book will teach you these from an entire field?”
alignment tools and more: —Hala Nelson
• Author of Essential Math for AI (O’Reilly)

 The four facets of data products: A simple blueprint that 
encapsulates data and business logic helps eliminate the 
most common causes of wasted time and misunderstanding Ron Itelman is cofounder of 

• The concept compass: An easy way to identify the  Intelligence.AI. His expertise is  
biggest sources of misalignment in collaborative intelligence, at the 

• Success spectrums: Define the required knowledge  intersection of AI and psychology, 
and road map your team needs to achieve success to model behavior and knowledge 

• JSON Schema: Leverage JSON and JSON Schema to in order to generate customized 

technically implement the strategy at scale, including learning experiences.

extending JSON Schema with custom keywords, Juan Cruz Viotti is cofounder of 
understanding JSON Schema annotations, and hosting  Intelligence.AI. In addition to leading 
your own schema registry his own open source lab, Sourcemeta, 

• Data hygiene: Learn how to design high-quality datasets based on award-winning research 
aligned with creating real business value, and protect your at the University of Oxford, he leads 
organization from the most common sources of pain the desktop engineering team at 

Postman and collaborates with the 
JSON Schema organization.

DATA Twitter: @oreillymedia
linkedin.com/company/oreilly-media
youtube.com/oreillymedia 

US $65.99  CAN $82.99
ISBN: 978-1-098-14500-2

5 6 5 9 9

9 7 8 1 0 9 8 1 4 5 0 0 2



Unifying Business, Data, and Code
Designing Data Products with JSON Schema

Ron Itelman and Juan Cruz Viotti

Beijing Boston Farnham Sebastopol Tokyo



Unifying Business, Data, and Code
by Ron Itelman and Juan Cruz Viotti
Copyright © 2024 Intelligence.AI LLC. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are
also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional
sales department: 800-998-9938 or corporate@oreilly.com.

Acquisitions Editor: Aaron Black Indexer: WordCo Indexing Services, Inc.
Development Editor: Corbin Collins Interior Designer: David Futato
Production Editor: Ashley Stussy Cover Designer: Karen Montgomery
Copyeditor: nSight, Inc. Illustrator: Kate Dullea
Proofreader: Tove Innis

February 2024:  First Edition

Revision History for the First Edition
2024-01-24: First Release

See http://oreilly.com/catalog/errata.csp?isbn=9781098145002 for release details.

The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Unifying Business, Data, and Code, the
cover image, and related trade dress are trademarks of O’Reilly Media, Inc.
The views expressed in this work are those of the authors and do not represent the publisher’s views.
While the publisher and the authors have used good faith efforts to ensure that the information and
instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility
for errors or omissions, including without limitation responsibility for damages resulting from the use
of or reliance on this work. Use of the information and instructions contained in this work is at your
own risk. If any code samples or other technology this work contains or describes is subject to open
source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use
thereof complies with such licenses and/or rights.

978-1-098-14500-2
[LSI]



Table of Contents

Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi

1. The Need for a Unifying Data Strategy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1
Your Quest for Data-Driven Breakthroughs Begins                                                     1

There Are Usually Multiple, Conflicting North Stars                                               2
The Good, the Bad, and the Ugly of Data Problems                                                 3
The Problem with Problems                                                                                         7
Unifying Concepts: The Key to Innovation                                                                9

What a Unifying Data Strategy Will Do for Agile                                                       11
Defining Being Agile                                                                                                    12
Agile Theater                                                                                                                 13
Agile, Waterfall, and Unifying                                                                                    13
Defining a Unifying Data Strategy Approach                                                          14

Understanding the Phrase Being Data Driven                                                            15
To Be Data Driven, Be Data Centric                                                                          16
Bottlenecks Preventing Teams from Being Data Driven                                        17

This Book’s Project: Intelligence.AI Coffee Beans                                                      18
Summary                                                                                                                           19

2. The Lingua Franca of Data: JSON. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21
Introducing JSON                                                                                                            21

A Simple JSON Example                                                                                             23
JSON Viewing and Authoring Tools                                                                         24

Overview of JSON Grammar                                                                                         26
Booleans                                                                                                                         27
Numbers                                                                                                                        27
Strings                                                                                                                            27
Arrays                                                                                                                             28

iii



Objects                                                                                                                           29
Null                                                                                                                                 31
Learning More                                                                                                              31
Minification                                                                                                                   32
Alternative Representations                                                                                        34

Creating a JSON Document                                                                                           36
A Product Entry                                                                                                           36
A Store Order                                                                                                                37

Summary                                                                                                                           38

3. Data-Centric Innovation: A Guide for Data Champions. . . . . . . . . . . . . . . . . . . . . . . . . . .  39
Data Transformations Require Data Champions                                                        40
The Rise of the Data Product Manager                                                                         42
Alignment Is a Journey, Not a Destination                                                                  43

Evaluating Alignment from a Holistic Perspective                                                  43
The Goal Isn’t Alignment, It’s Effective Alignment                                                 45
Strategies for Setting Up Teams for Success                                                             46

Incorporating a Product Management Mindset                                                          48
Defining Data Users’ Needs                                                                                        49
Defining Product Features                                                                                          50
Defining and Measuring Success                                                                               52

Unifying Versus Aligning                                                                                               52
Summary                                                                                                                           54

4. Concept-First Design for Data Products. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57
Packaging and Products: An Example Using Coffee                                                  59
The Four Facets of a Data Product                                                                                60
Getting Started with Concept-First Design                                                                 63
A Blueprint for Unifying                                                                                                 64
Mapping the Conceptual Terrain: Assessing Concepts                                              65
Facilitating Assessments of Conceptual Alignment Across Technical and

Nontechnical Teams                                                                                                     67
Smooth Is Slow, Slow Is Fast                                                                                           69
Summary                                                                                                                           70

5. A Universal Language for Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  73
What Is JSON Schema?                                                                                                   74

What Is a Schema?                                                                                                       74
The Building Blocks of JSON Schema                                                                          75

Vocabularies and Dialects                                                                                           75
Meta-Schemas: Schemas That Describe Other Schemas                                        76

Understanding JSON Schemas                                                                                      76

iv | Table of Contents



Step 1: Determining the Schema Dialect: The $schema Keyword                        78
Step 2: Determining the Schema Vocabularies                                                        79
Step 3: Understanding Schema Vocabularies                                                           81
Step 4: Understanding Schema Keywords                                                                82

JSON Schema as a Recursive Data Structure                                                               86
Referencing Schemas                                                                                                       87

What does duplication look like?                                                                               87
Local referencing                                                                                                          88
Remote referencing                                                                                                      90

Your First JSON Schema Project                                                                                   91
Writing a Schema: Step by Step                                                                                  91
Generating a Web Form                                                                                              95

Summary                                                                                                                           97

6. The Art of Alignment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  99
Enemies of Alignment: Ambiguity and Assumptions                                              100

Ambiguity: The Culprit in the Illusion of Communication                                101
Assumptions: Ambiguity’s Best Friend                                                                   102

Defining Success: Symmetry Between Concepts and JSON Schema Equals
Minimal Ambiguity                                                                                                   102

Illuminating Misalignment with a Concept Compass                                             104
Step 1: Harmonizing the What                                                                                 105
Step 2: Harmonizing the Way                                                                                   106
Step 3: Harmonizing the How                                                                                  108
Harmonized Concepts                                                                                               109

Validating Concepts: Belief Scoring and Hypotheticals                                           111
Counterfactuals                                                                                                          111
Belief Scoring                                                                                                              112

Summary                                                                                                                         113

7. The Science of Synchronization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  115
An Introduction to Thinking in Networks                                                                116

Example of Thinking in Networks: Athletes Versus Artists                                 116
Graphs: The Visual Language of Networks                                                            117

Networks of Entities: Knowledge Graphs                                                                  118
A Simple Knowledge Graph                                                                                     119
Challenges with Knowledge Graphs                                                                        119
Aligning Knowledge for the 99%                                                                             120

Fundamentals of CLEAN Data Governance                                                              120
Collaboration                                                                                                              122
Knowledge                                                                                                                   123
Business Logic                                                                                                             124

Table of Contents | v



Activity                                                                                                                         124
CLEAN Data Governance in Practice                                                                        125
The Four Facets of Data Products and CLEAN                                                        126
The Four Horsemen of Data Death                                                                             127

Ignorance                                                                                                                     128
Siloed Incentives                                                                                                         128
Shortsightedness                                                                                                         128
Indecisiveness                                                                                                             128

The Power of Design in Collaborative Networks                                                      129
Summary                                                                                                                         130

8. The Two Fundamental Operations of Schemas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  133
Validating the Structure of Data                                                                                  134

Using an Online Validator                                                                                        135
Validation Example                                                                                                    136
JSON Schema as a Constraints Language                                                               137
Boolean Schemas                                                                                                        139
Heterogeneous Data Structures                                                                               140
The format Keyword                                                                                                  142

Using Annotations to Define Meaning                                                                       144
Annotation Extraction Example                                                                              144
A Simple Use Case: Deprecations                                                                            145
Runtime Extraction                                                                                                    146
Standard Output Formats                                                                                         148
Revisiting the format Keyword                                                                                150
Using an Online Validator                                                                                        151

Thinking in Schemas                                                                                                     151
Summary                                                                                                                         152

9. Illuminating Pathways of Acceleration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  153
How Ambiguity, Knowledge Gaps, and Blind Spots Influence Decisions and

Progress Toward Goals                                                                                              155
Which Is Bigger: Greenland or the US?                                                                     156
Mapping Pathways of Processes and Progress                                                           157

Measuring Progress Toward Goals                                                                          157
Defining Decisions and Steps with Process Maps                                                 158
How Process Maps Reveal Ambiguity                                                                    159

Visualizing and Removing Ambiguity in Processes                                                 160
Enriching Process Maps with Annotations                                                            162
Process Maps Reveal Innovation Opportunities                                                   163

Summary                                                                                                                         164

vi | Table of Contents



10. Spectrums of Success. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  165
An Introduction to Knowledge Frameworks                                                            166

Knowledge Experiences and Pathways                                                                   167
A Tool for Designing Knowledge Experiences                                                      169
From Structured Knowledge to Computational Knowledge                               171

Success Spectrums                                                                                                         172
Mapping Progress and Value                                                                                    172
Visualizing and Adding “Next Best States”                                                            173
Removing Blind Spots                                                                                               174
Embracing Multiperspective Design and Road Maps                                           176
Defining KPIs for Success Measures and Metrics (Assessments)                       178
Using Demons and Magical Thinking for Innovation                                         179
Faster Horses                                                                                                               180
Imagining Magical Possibilities                                                                                181
Problem Landscapes: Quantifying Pain Points Threatening Value                    182

Nudges: The Right Information at the Right Time                                                   183
A Real-Life Problem Landscape and Demon Example That Led to a Unified

Data Product Model                                                                                                   184
Understanding the Problem Landscape                                                                  184
The Staggering Impact                                                                                              185
A Meeting of Minds and the Birth of a Solution                                                   185

Beyond Data Products: Data Product Management                                                 187
The Circular Nature of Unifying                                                                                 188
Summary                                                                                                                         189

11. Deploying a JSON Schema Registry. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  191
Schemas Over HTTP                                                                                                    191
Step 1: Setting Up a GitHub Repository                                                                     192

Creating a GitHub Repository                                                                                  192
Uploading Your First Schema                                                                                   193

Step 2: Deploying to Cloudflare Pages                                                                        195
Creating a New Cloudflare Pages Website Project                                                195

Step 3: Configuring HTTP Headers                                                                            200
Inspecting the Current HTTP Headers                                                                  201
Declaring Custom HTTP Headers on Cloudflare Pages                                      201
Checking the Results                                                                                                 202

Step 4: Creating a Landing Page                                                                                  204
Adding an HTML Entry Point                                                                                 204

Step 5: Adding a Custom Domain                                                                               205
Configuring a Custom Domain in Cloudflare Pages                                            206
Setting Up a CNAME DNS Record                                                                         208
Checking the Results                                                                                                 209

Table of Contents | vii



Best Practices                                                                                                                  210
Schemas Are Immutable                                                                                           210
Adopt a Versioning Strategy                                                                                     210

Summary                                                                                                                         211

12. Designing Data Products Using JSON Schema. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  213
First Facet: Data                                                                                                             214

An Example CSV Dataset                                                                                         214
A JSON Row Representation                                                                                    215

Second Facet: Structure                                                                                                 215
General-Purpose Concepts                                                                                       215
Application-Specific Concepts                                                                                 220
Dataset Entries                                                                                                            220
The Dataset Schema                                                                                                   221

Third Facet: Meaning                                                                                                    222
Timestamp                                                                                                                   223
IP Address                                                                                                                   223
Email                                                                                                                            224
US State                                                                                                                        224
Currency                                                                                                                      225
Price                                                                                                                             226
Milestone                                                                                                                     227
Analytics Entry                                                                                                           227

Fourth Facet: Context                                                                                                    228
The Signup Analytics Schema                                                                                  229

Summary                                                                                                                         229
Automated Schema Extraction                                                                                229
Next Steps                                                                                                                    231

13. Extending JSON Schema. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  233
Simple Case: Unknown Keywords                                                                              234

Extracting Unknown Keywords as Annotations                                                   234
Pros and Cons of This Approach                                                                             235

Complex Case: Authoring Vocabularies                                                                     236
The JSON Schema Vocabulary System                                                                   236
Step 1: Writing a Specification                                                                                 237
Step 2: Writing a Vocabulary Meta-Schema                                                           241
Step 3: Extending an Implementation                                                                     244

Consuming Vocabularies                                                                                              247
Defining a Dialect                                                                                                      247
Making Use of the Dialect                                                                                         249
Example: Extracting Annotations with Hyperjump                                             249

viii | Table of Contents



Summary                                                                                                                         251

14. Introducing JSON Unify. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  253
Introducing the Dataset Vocabulary                                                                           253

Revisiting the Signup Analytics Example                                                               254
JSON Schema Bundling                                                                                                255

The Bundling Process                                                                                                258
Bundling Our Example Data Product                                                                     259

Referencing Remote Data                                                                                             261
The Problem of Streaming JSON                                                                             262
Introducing JSON Lines                                                                                            262

Extracting Meaning                                                                                                       263
A Simple Example                                                                                                      263
Using Logic Operators                                                                                               264
The Signup Analytics Example                                                                                265

Dataset Lineage                                                                                                              266
Filtering                                                                                                                       267
Transforming                                                                                                              268
Aggregation                                                                                                                 269

Summary                                                                                                                         271

15. Principles of Designing Intelligence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  273
Your Unifying Journey So Far                                                                                      273
A Constellation of Deeper Principles Guides Unifying                                           274
1. The Principle of Alignment                                                                                      275

Transforming the Abstract to Concrete                                                                  275
What You See Can Kill You, and the Same Is True in Data                                  276

2. The Principle of Information                                                                                   278
Understanding Uncertainty                                                                                      278

3. The Principle of Learning                                                                                         280
Defining Learning                                                                                                      280
Defining Errors                                                                                                           282

4. The Principle of Integrated Simplicity                                                                    282
Complexity Reduction                                                                                               282
Decomposition                                                                                                           283
Compression                                                                                                               283
Memoization                                                                                                               283
Integrating in Communication Networks                                                              283

5. The Principle of Continuums                                                                                  284
Making Things Measurable                                                                                      284
The Dangers of Misusing Measurements                                                               284
A Continuum Example for a Control Strategy Problem                                      285

Table of Contents | ix



6. The Principle of State Transitions                                                                           286
A Simple State Machine                                                                                            287
Simplifying State Transitions                                                                                    287

7. The Principle of Decidability                                                                                   288
What Is Decidability?                                                                                                 288
Two Key Approaches to Problem Solving                                                              289
Making Informed Decisions                                                                                     289
Real-World Decidability to Reduce Misalignment in Teams                               290

8. The Principle of Heuristics                                                                                       290
Awareness and Ethical Considerations                                                                   291
Connection to Decision Making in Business                                                         292

9. The Principle of Mastery                                                                                          292
Levels of Mastery in Knowledge                                                                              293
Strategies for Mastery                                                                                                294

10. The Principle of Wisdom                                                                                       295
Summary                                                                                                                         296

16. Toward Unified Intelligence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  297
Functional Artificial Intelligence                                                                                 298

Your AI Is Only as Good as Your Data                                                                    298
Beware Illusions Within Vetting Processes                                                            299
Question Assumptions                                                                                              299

Collective Intelligence                                                                                                   299
Collaborative Intelligence                                                                                             301
Unified Intelligence                                                                                                       302

Collaborative Learning Networks                                                                            302
Personalized Knowledge                                                                                           303
Anticipatory Design: Personalization and Digital Twins                                     305

Codifying Principles of Intelligence                                                                            306
Continuous Human–Machine Learning Loops                                                     308
Applying Wisdom in Practice                                                                                  308
Conceptual Zoomability                                                                                           309

Wisdom Graphs: Connecting Concepts, Actions, and Outcomes                         311
Cognitive Primitives: Standardizing Cognitive Experience Design                   312

The Value of Unifying                                                                                                   314
Prioritize Knowledge Before AI                                                                               314
A Tale of Simple Knowledge Versus Complex Intelligence                                 315
Follow the Principle of Integrated Simplicity                                                        315

Summary                                                                                                                         315
Going Beyond This Book                                                                                             316

Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  317

x | Table of Contents



Preface

In the 1840s, a Hungarian physician named Ignaz Semmelweis encountered a per‐
plexing challenge while working in the maternity clinic at the General Hospital in
Vienna. A significant number of women were succumbing to a mysterious ailment
known as “childbed fever,” which plagued many European hospitals.
Semmelweis made a striking observation: the maternity ward overseen by male
doctors had a significantly higher mortality rate than the one managed by midwives.
Furthermore, he noticed that doctors often proceeded directly from performing
autopsies to examining expectant mothers.
After a colleague pricked his own finger while doing an autopsy, resulting in the
colleague falling ill and eventually dying, Semmelweis had a revelatory moment:
perhaps what killed his colleague might be also killing the women in childbirth.
Semmelweis theorized that contaminants from the cadavers that doctors were oper‐
ating on and using to teach medical students might be transferring to the women,
leading to the fever. To test this hypothesis, he implemented a policy in 1847 that
required doctors to wash their hands with a chlorine solution to eliminate what he
called “cadaverous particles,” before examining pregnant women.
Following the implementation of this handwashing policy, the maternal mortality rate
in the doctors’ ward plummeted from 18% to a mere 2%. However, Semmelweis’s
ideas were met with skepticism from the medical community because they challenged
the scientific beliefs at the time, and germ theory had not yet been developed.
Semmelweis could offer no theoretical explanation for his findings, and he was
mocked and ridiculed. In 1865, Semmelweis suffered from a nervous breakdown,
resulting in his being committed to an asylum in Vienna by his colleagues, where he
was beaten by guards and tragically died from a gangrenous wound on his right hand
just 14 days later, at the age of 47.

xi



The story of Ignaz Semmelweis offers a few valuable insights:
Human behavior is constrained by bias

Embracing new perspectives often challenges our deeply held beliefs. Such
changes are frequently met with resistance—even from those equipped with
knowledge and influence. It underscores the profound impact of cognitive bias
and societal norms on human judgment.

Interconnected systems are impacted greatly by hygiene
The vast and intricate systems we see, such as hospital protocols or childbirth
procedures, can be dramatically influenced by elements so minuscule they’re
often invisible, such as germs. This highlights the delicate balance and intercon‐
nectedness of our world, from the microscopic to the grand scale.

Simple actions can have massive ripple effects
At times, the most straightforward measures, like handwashing, become our
most potent solutions. Understanding the methods to mitigate tiny threats can
prove pivotal, with ramifications felt on a monumental scale.

What You Can’t See Can Kill You,
and the Same Is True for Data
The transformative shift in our understanding of disease causation can be dated back
to the 1860s. Louis Pasteur’s revolutionary experiments demonstrated that microor‐
ganisms were responsible for fermentation and spoilage, laying the foundation for
germ theory and paving the way for monumental advancements such as vaccines,
antiseptics, and sterilization techniques.
In marked contrast, Ignaz Semmelweis made essential observations decades earlier
but remained largely overlooked due to his lack of a robust scientific theory. The
divergence in their legacies—Pasteur’s transformative influence versus Semmelweis’s
limited recognition—emphasizes the critical need for both theoretical and practical
foundations in tackling complex problems.
Unifying Business, Data, and Code seeks to bridge this very gap in the field of organi‐
zational data management and the design of intelligent systems. We aim to furnish
you with both a robust theoretical framework and actionable practical tools, applica‐
ble whether you’re brainstorming strategies on a whiteboard or coding sophisticated
algorithms.
Diverging from books that concentrate on either technical or managerial facets of
data and intelligent system design, Unifying Business, Data, and Code takes a holistic
stance that merges both strategic perspectives. We’ve discovered that a technically
sound strategy lacking managerial integration is doomed to fail—and the reverse is

xii | Preface



equally true. This synthesis enables you to make better-informed decisions, effectively
bridging the divide between IT and business strategy.
Just as neglecting basic hand hygiene had devastating repercussions in Semmelweis’s
time, modern organizations face concealed yet significant risks from poor data man‐
agement. In essence, the primary challenges compromising your organizational data
hygiene can be distilled into three categories:
Ambiguity

There are multiple possible interpretations.
Knowledge gaps

Missing information obstructs problem solving.
Blind spots

There is a lack of awareness of ambiguity and knowledge gaps and their effects
on organizational outcomes.

This book will guide you through the process of identifying poor data hygiene and
the root causes of misalignment that it leads to within your organization. Armed
with this understanding, you’ll be equipped to drive innovation and transformation
through a strategic data management approach, unlocking the benefits of intelligent
system design for superior results.

Hidden Threats to Organizations: A Modern Parallel
In the expansive world of organizational dynamics, hidden levels of granularity
shape our actions and decisions, yet remain unseen in our daily routines. This book
journeys into these enigmatic depths. True organizational coherence demands the
dexterity to zoom out, transcending individual roles and looking at the vast networks
that knit an organization together. At the same time, mastering the finesse to “zoom
in” becomes crucial to tackle nuanced data challenges. Like the invisible germs Sem‐
melweis grappled with, these subtle issues can reverberate and escalate unpredictably,
leading to profound consequences.
As an example, imagine a retail company using data analytics to forecast demand.
A column in the database is ambiguously labeled as Total Sales Revenue. An analyst
assumes this means Net Sales, the revenue after returns and discounts, but it actually
represents Gross Sales, the revenue generated before any expenses. This simple misun‐
derstanding skews the demand forecast, as the report doesn’t take into account the
item’s terrible quality and high return rates.
The company ends up overstocking the flawed items and understocking good ones.
Inventory costs balloon, customers are left dissatisfied and lose trust in the brand.
The flawed decision making results in a hunt for who to blame for the error, and
as the culture becomes toxic, top performers who care about the company leave a

Preface | xiii



vacuum of expertise and talent. Like a house of cards, a single ambiguous label can
lead the entire organizational strategy to crumble.
Central to our discussion is the idea of concepts, shown in Figure P-1, serving as
the foundation of our unifying methodology. While a deeper exploration awaits in
later sections, for now you can think of concepts as the vital atoms whose unique
configuration and combination creates the elements of our everyday experiences at
the second level of granularity shown in Figure P-1: language, processes, and decision
making.
Consider data products, shown in Figure P-1, to be our metaphorical “handwashing”
solution. Although our unifying principles help pinpoint ambiguity, knowledge gaps,
and blind spots, it’s data products that, much like a sanitizing solution, actively
cleanse and address these issues in practice.
Imagine your data as a high-quality product on a store shelf. It should be well crafted,
easy to use, and comprehensive. In this book, you’ll learn how to elevate your data to
that level of quality. We’ll guide you through a standardized process that packages the
structure, meaning, and context along with the data itself.
Once your organization begins designing high-quality data products, the benefits
of implementing data hygiene can be quite transformative, freeing up teams from
putting out fires in a chronically troubled system and enabling them to focus on
creating business value, enhancing efficiencies, and innovation excellence.
Additionally, the principles and methodologies we’ve discussed so far set the founda‐
tion for something even more powerful: unified intelligence, which is applying the
unifying methodology to human and machine learning system design. Chapter 15
introduces unified intelligence. However, before we can even begin to think of using
the principles of unifying with AI, we need to get our data in good shape.

Your AI Is Only as Good as Your Data
The axiom “Your AI Is Only as Good as Your Data” serves as a critical pillar of
this book, highlighting the inextricable link between data quality and AI efficacy.
Our framework builds on the groundbreaking contributions of seminal figures in the
field—Claude Shannon’s information theory, Alan Turing’s computational models,
and Shane Legg and Marcus Hutter’s advancements in reinforcement learning. Their
collective insights merge seamlessly into our comprehensive methodology, which we
will explore in detail in Chapter 15.
Data scientists leverage rigorous methodologies and empirical reasoning to dissect
complex challenges and represent them in a structured format. This facilitates the
deployment of machine learning algorithms and the construction of predictive mod‐
els. In this book, we introduce the concept of designing intelligence—a synthesized set

xiv | Preface



of best practices aimed to equip both technical experts and managerial staff with a
robust skill set in data-centric problem solving.

Figure P-1. Three levels of granularity are shown in this illustration, each with issues
that the unifying methodology will address at each level of granularity. The key activity
you will be learning is to identify and minimize ambiguity, knowledge gaps, and blind
spots to align all three levels.

Preface | xv



Adopting these best practices doesn’t merely set the stage for successful AI initiatives;
it transforms your entire organizational data culture, cultivating a fertile ground for
data-centric innovation across your organization grounded in principles of designing
intelligence.

Aligning Problem-Solving Strategies, Data, and AI
Reinforcement learning serves as a critical pillar in understanding principles of design‐
ing intelligent systems, guiding decision-making strategies that oscillate between
exploration for new knowledge and exploitation of existing knowledge. As illustrated
in Figure P-2, this dynamic reflects human and organizational tendencies to balance
effort against reward, thereby shaping the innovation and efficiency strategies of
companies.

Figure P-2. This diagram presents a cycle of decision making based on outcomes from
exploiting current data and exploring new data. Organizations or individuals can use
this model to determine when to rely on existing knowledge (exploit) and when to seek
out new information or try new approaches (explore).

xvi | Preface



Too often, organizational leaders are ensnared in a narrow, top-down mindset that
prioritizes exploitation strategies over exploration. This culminates in vague visions
that rarely manifest into tangible innovation. When these approaches fall short, it’s
usually the workforce that suffers the consequences, from blame and job loss to
unsettling structural shifts. This book offers a suite of strategic and technical tools
aimed at breaking this detrimental cycle, moving beyond short-term fixes to achieve
sustainable progress.
This book encapsulates our insights from personal exploration and exploitation jour‐
neys—knowledge we find crucial to share. We’re deeply grateful for your investment
in this work. Our aspiration is that, by the end, the principles we unveil will resonate
so deeply that their application becomes as intuitive and vital as washing your hands.

A New Paradigm to Optimize Data Management and
Business Strategy for the Age of AI

Recognize that unlearning is the highest form of learning.
—Rumi, Persian poet

Unifying challenges conventional approaches with a cutting-edge approach: it uses
principles from data science used in problem solving to optimize data and knowledge
for creating business value. This strategy ensures that your organization will be
maximally primed for success in AI endeavors.
Whether you’re dealing with human decision making or computational systems, this
book offers a practical blueprint for smarter operations:

• Strategies and technologies unifying data management and business strategy are
presented in Chapters 1–14.

• The foundational theoretical principles from the fields of artificial intelligence,
cognitive psychology, that were used to create the unifying methodology, are
covered in Chapter 15.

• Building upon your unified data management and business strategy and the
principles of designing intelligent systems, Chapter 16 explores different ways to
apply unifying with AI.

In the pursuit of understanding and harnessing the power of data for business strat‐
egy, it’s crucial to keep an open mind—to entertain various hypotheses and embrace
the uncertainty created when experiencing new ways of thinking.
As Hala Nelson asserts in Essential Math for AI (O’Reilly, 2023), “Data is the fuel that
powers most AI systems” and “What I did not know, and learned the hard way, was
that getting real data was the biggest hurdle.”

Preface | xvii



The methodology elucidated in this book empowers you to apply data science princi‐
ples and problem-solving strategies effectively without needing to be a data scientist,
ensuring that the data you create and collect is not only more accurate and useful, but
also a closer reflection of reality.
By embracing the principles you will learn in this book, you will not just be able
to solve existing problems better than ever before—you’ll preempt future ones from
existing in the first place.

The Origin Story of Unifying
Driven by his work in AI within the edtech sector, Ron harbored an insatiable curios‐
ity to understand principles of designing intelligence that underpin both human and
machine learning systems. He envisioned organizations not merely as static struc‐
tures, but as dynamic ecosystems where information networks intermingle much like
the notes in a symphony.
Enter Juan, a leading expert in JSON, JSON Schema, and data serialization. Juan
wasn’t just technically proficient; he had the unique ability to take Ron’s grand
vision and turn it into a finely tuned reality. Juan’s award-winning research in data
serialization at the University of Oxford revealed he could apply the methodology all
the way down to the binary level and all the way up to gold-standard protocols for a
global-scale data specification.
Our partnership was nothing short of magical—akin to a musical band discovering
perfect harmony among its members. Together, we embarked on an unceasing jour‐
ney of growth and innovation, each challenging and enriching the other’s domain
expertise. This book represents the zenith of our collaborative efforts, serving as a
comprehensive guide that harmonizes overarching strategies with granular technical
solutions for organizations.
We wrote this book with a singular, transformative purpose in mind: to empower
people with bold guiding principles and technical strategies that can cut through
seemingly impossible problems by unifying people, processes, and data across multi‐
ple, and seemingly invisible, scales. We want to democratize this knowledge, to make
it accessible and actionable for all, unleashing waves of creativity and ingenuity to
transform the world for the better.
The quest to explore and codify the principles of unifying led us into the realms of
the mysterious and unknown. Sharing the wisdom we’ve garnered along this journey
brings us the incomparable joy of serving a purpose far greater than ourselves.

xviii | Preface



Orchestrating Alignment at Organizational Scale
Historically, the paradigm shift from attributing illnesses to supernatural causes to
understanding them as results of bacteria and viruses wasn’t just a leap in knowledge.
It required a massive change in practices, behaviors, and beliefs. In a similar vein,
organizations today need to shift from seeing challenges as unsolvable mysteries to
recognizing them as tangible problems that can be addressed with the right strategies
and methodologies.
Unifying serves as a vital framework designed to demystify the intricate challenges
organizations face—challenges rooted in misalignment and silos among business,
data, and coding teams. Informed by Figure P-1, our methodology orchestrates align‐
ment across three crucial scales of granularity: the organizational scale, which encap‐
sulates the broad view of roles and networks; the human experience scale, focusing
on language, processes, and decision making; and the data product scale, the frontline
where data hygiene and quality are actively managed. As you journey through this
book, we’ll explore these scales in granular detail, guided by the following pillars:
Theory

Establishing the underpinning philosophical shift and vocabulary essential for
evolving data management and intelligent systems. Think of this as the funda‐
mental why and what that lays the foundation for change.

Strategy
Offering a blueprint for practical application, this high-level guidance navigates
the how, outlining steps to implement the new paradigm across the scales.

Tools
These are your translators that convert business logic into actionable technical
language. Comprising nontechnical, tactical activities, these tools serve as the
bridge between strategy and implementation. Tools are tactics to eliminate ambi‐
guity, knowledge gaps, and blind spots. However, there are multiple ways to do
this, and we provide you with templates and suggestions.

Implementation
This is the doing phase where coding practices are employed to manifest the
methodology in real-world, technical environments.

By seamlessly merging theory, strategy, tools, and implementation, unifying elevates
your organization’s approach to data management and designing intelligent systems
to unparalleled heights. This is not just about identifying the pitfalls of poor data
hygiene—like ambiguities, knowledge gaps, and blind spots—but about systemati‐
cally rectifying them at every scale of your organization.

Preface | xix



Unifying transcends silos, enabling a holistic alignment that harmonizes the macro
view of organizational roles and networks with the nuanced details of human
experiences and data product quality. The ultimate takeaway? A transformative
impact that not only optimizes your data for AI applications but also fuels a culture
of ceaseless innovation and excellence. You’ll be able to navigate the labyrinth of
challenges with the finesse of a maestro, orchestrating a symphony of meaningful
change.
The question isn’t whether you can afford to implement these strategies; it’s whether
you can afford not to.

Conventions Used in This Book
The following typographical conventions are used in this book:
Italic

Indicates new terms, URLs, email addresses, filenames, and file extensions.
Constant width

Used for program listings, as well as within paragraphs to refer to program
elements such as variable or function names, databases, data types, environment
variables, statements, and keywords.

This element signifies a tip or suggestion.

This element signifies a general note.

This element indicates a warning or caution.

xx | Preface



O’Reilly Online Learning
For more than 40 years, O’Reilly Media has provided technol‐
ogy and business training, knowledge, and insight to help
companies succeed.

Our unique network of experts and innovators share their knowledge and expertise
through books, articles, and our online learning platform. O’Reilly’s online learning
platform gives you on-demand access to live training courses, in-depth learning
paths, interactive coding environments, and a vast collection of text and video from
O’Reilly and 200+ other publishers. For more information, visit https://oreilly.com.

How to Contact Us
Please address comments and questions concerning this book to the publisher:

O’Reilly Media, Inc.
1005 Gravenstein Highway North
Sebastopol, CA 95472
800-889-8969 (in the United States or Canada)
707-827-7019 (international or local)
707-829-0104 (fax)
support@oreilly.com
https://www.oreilly.com/about/contact.html

We have a web page for this book, where we list errata, examples, and any additional
information: https://oreil.ly/unifying_business_data_and_code_1e.
For news and information about our books and courses, visit https://oreilly.com.
Find us on LinkedIn: https://linkedin.com/company/oreilly-media
Follow us on Twitter: https://twitter.com/oreillymedia
Watch us on YouTube: https://youtube.com/oreillymedia

Preface | xxi



Acknowledgments
Ron Itelman would like to thank:
Stephanie Itelman, for showing me the strength of your character, the empathy
of your heart, the wit in your mind, and the power of your being. You enabled,
supported, inspired, and challenged me every step of the way. Thank you, baby, for
giving me the gift of sharing life with you, and the experience of creating a family full
of laughter and love.
Reuven and Zehava Itelman, for giving me the opportunity to develop experience in
experimenting with innovation strategies holistically across a business.
Michael Kaplan, for being a mentor and a guide, teaching me the true meaning of
wisdom.
Stephanie Golinveaux, for being a beacon of light which has transformed my life for
the better.
Don Houde, for showing me what excellence in management means, and that there
are those who will truly develop, nurture, and invest in their teams.
Ole Bagneux, for believing in me enough to introduce me to Aaron Black, creating
this opportunity and mentoring this book and me along the authoring journey.
Jim Knickerbocker, PhD, for being a warrior and a thought leader; your belief and
support in me has been pivotal in my professional development.
Sean Goodpasture, for believing in me, for being a champion, mentor, friend, and
brainstorming partner. You’re also greatly appreciated when presenting at a data
conference featuring a T. rex to demonstrate the intersection of UX, psychology,
and AI ;).
Anthony Marquardt, for investing your time and efforts in mentorship, demonstrat‐
ing apex qualities that blend the art of business, technology, and empathy.
Ben Rolnik, for opening up doors that have changed our trajectory and created
opportunities to change the world.
Shawna Strickland, who is a brilliant ray of calming, grounded light with sharp
business acumen, exemplifying qualities I aspire to emulate.
Karl Friston, whose encouragement banished my personal limitations that were
holding me back from pursuing my personal missions in scientific inquiry with
unabashed curiosity.
Laura Pionek, for seeing something in me and giving me the opportunity to express
my creativity and curiosity, a major catalyst on this journey.

xxii | Preface



Aaron Black, for taking a chance on us; you’ve changed our lives. Thank you for
giving us the opportunity almost no one gets—the ability to write a book about one’s
passion.
Corbin Collins, for guiding us and this book through maelstroms to sunny shores.
Dealing with authors is probably not easy, and you helped carry the weight of people’s
dreams in your hands to make sure we succeeded.
Juan Cruz Viotti would like to thank:
Darlene Colque Roman, for being by my side every evening I spent writing this book.
You make me a better man, and your love brings so much joy, purpose, and balance
to my life.
Karina Viotti, for teaching me to believe in myself, have the courage to aim high, and
that not even the sky’s the limit.
Perla Viotti, for cultivating the habit of reading in me since I was a child and buying
me countless O’Reilly books when I was a teenager.
Julian Berman, Greg Dennis, Ben Hutton, Jason Desrosiers, Benjamin Granados, and
Henry Andrews for welcoming me to the JSON Schema community and teaching me
most of what I know about JSON and JSON Schema.
Aaron Black and Corbin Collins, for providing so much help and guidance, making
the daunting process of writing a book so smooth, enjoyable, and fun.

Preface | xxiii



CHAPTER 1
The Need for a Unifying Data Strategy

Imagine yourself as a data strategy consultant, supporting executives with a spectrum
of problems across diverse industries. In some cases, deadlines are not being met, and
you are brought in to understand why. In other cases, the executives have a vision of
how they want to change the world and want your thought partnership on rapidly
designing, testing, and building a prototype to present at a global conference. As you
work with executives to solve various problems, you begin to see patterns of what
works and what doesn’t in the world of data, innovation, and AI, and you begin to
wonder why.
Ultimately, your role involves identifying the root causes of innovation bottlenecks
and offering actionable recommendations to help organizations overcome these
obstacles and achieve their objectives. If there were a set of principles and guidelines
to make innovation outcomes more effective and reliable, that would enable you and
your clients to be more successful.
A unifying data strategy is a way to approach innovation through the lens of what is
the minimal amount of collaborative effort with data that creates maximum business
value? It doesn’t require or recommend any specific technology, but it does require
you to think about data from a holistic perspective so that you can unify teams
around a common language, understanding, and way of working together.

Your Quest for Data-Driven Breakthroughs Begins
You’ve been hired by John, the CEO of a cutting-edge biopharma company, which has
just secured significant venture capital to develop a groundbreaking new therapy that
potentially cures a disease that kills millions of people a year.
“The clock is ticking,” says John. “With each passing day, we risk falling behind in
the race to develop a life-saving therapy, with billions of dollars in contracts at stake.

1



I’ve promised our investors we are going to be data driven in everything that we do.
The livelihoods of hundreds of employees are at risk if we don’t deliver, and, most
importantly, millions of people are desperate for a cure.”
John wants an assessment of the company’s most significant data problems and
recommendations for a quick and effective solution. Despite having an exceptional
data team of PhDs in data science and biology from prestigious universities, they
are struggling with drug discovery, perpetually battling data issues and leaving them‐
selves and the R&D teams that depend on them to spend time putting out fires,
despite continued investments to increase the team size.
The pressure is palpable. John is scheduled for a presentation in a few months to the
organization’s funders and board of directors, and they are expecting to see a plan on
how to address the situation and achieve the outcomes they are anticipating. Colossal
pharmaceutical companies are scrambling to capitalize on novel technologies for
previously untreatable diseases, and billions of dollars in contracts are hinging on
the new therapy moving forward to the next stage of clinical trials with a fixed
deadline. Every day lost to data problems jeopardizes the financial future of the
organization. Hundreds of people’s livelihoods and the families they support are on
the line. Everyone in the organization is working 12+ hours a day, knowing that if
they succeed, they will be part of the team that changed the world and helped save
millions of lives.
You ask John to define the one thing that is most important in defining what suc‐
cess looks like. You call this one thing a North Star, and it will help you assess
whether people are focusing their efforts on alignment to the CEO’s vision. John
confidently speaks about data-driven decision making to accelerate research and
says that machine learning has the potential to save years and millions of dollars in
R&D costs. The North Star is stated as: we will have the most advanced data-driven
capabilities for drug discovery. Your impression is that data will drive R&D, and you
believe in the CEO’s vision.
However, the North Star definition starts shifting as John makes comments about
how the organization’s culture is R&D led, and it becomes clear that he has a nebu‐
lous understanding of how data science works. John fumbles with his words, clearly
uncomfortable that his statements aren’t holding up to much scrutiny. Not a big deal,
you think. You reassure yourself and the CEO that together you can make the North
Star definition clear and succinct.

There Are Usually Multiple, Conflicting North Stars
While interviewing VPs and their subordinate directors about the North Star, you
uncover striking discrepancies in their perspective on the importance of data science
in guiding their work. The organization’s culture is indeed R&D led, but the CEO is

2 | Chapter 1: The Need for a Unifying Data Strategy



saying the North Star is to be completely data-driven. The R&D team is focused on
running biology experiments and doesn’t have any data management expertise.
The data science and data engineering teams are entirely different parts of the orga‐
nization, primarily used to support R&D by fixing data problems and handling
data requests across the organization. R&D are the experts in their field, not data
scientists. What does data driven even mean if R&D are the ones making decisions
based on their intuition?
The way the data teams view what problems data science will address and the strategy
of how data science will be used to make data-driven decisions in R&D deviate
significantly from the CEO’s way of thinking about the North Star. The more people
you ask what the North Star is, the more it is becoming increasingly unrecognizable
across departments and levels of the organization. When you question other execu‐
tives about these disparate views, they dismiss the North Star as some aspirational
and unrealistic phrase rather than the operational foundation for their goals and
work.

The Good, the Bad, and the Ugly of Data Problems
Digging deeper, you find business leaders making expensive decisions, investing in
software and hardware that creates, curates, and disseminates data, only to find data
teams saying that the data is mostly worthless because deeper and more significant
problems are being ignored. The CEO is completely oblivious to the continuous data
corruption plaguing the supposedly data-driven organization, lulled into a false sense
of well-being by costly cloud data storage and compute bills. As the saying goes,
garbage in, garbage out (GIGO).
A VP privately tells you the biggest problem to solve is that the scientists are
all working with data stored in their emails, PowerPoints, Excel spreadsheets, and
comma-separated values (CSV) files in SharePoint. No one can see each other’s work
or learn from each other. The VP is considering a cloud company’s consulting pitch
for an enterprise data lake solution, complete with a knowledge graph, data catalog,
and a host of other expensive enterprise tools that will cost millions of dollars over
several years as part of a digital transformation project. The VP is told by these trusted
experts that the company’s data will be totally under control, and they will be able to
get the insights leaders want.
Except for one problem: it almost always never works out the way the data solution was
sold. This usually has less to do with data and more to do with your organization’s
strategy, or rather the lack of an effective unifying data strategy around how people in
very different domains and with very different perspectives need to understand each
other and work together.

Your Quest for Data-Driven Breakthroughs Begins  | 3



The problem boils down to the process of converting abstract business information
to concrete results with the minimal amount of risk of errors. The business team says
what they want, expecting a top-down progression as shown in Figure 1-1. If a dev and
data team translates this without error, it is a successful data/code implementation
that accurately represents a product to serve operational needs.
In the world of data management, the true challenge isn’t technology but the human
factor; people operate within their own unique silos, skewing perspectives. Bridging
these gaps is crucial. Business leaders often think in top-down, solution-centric
terms, prioritizing immediate problems like, “We need technology X for problem Y,”
rather than delving into root causes, such as, “Why are our costs in Area Z so high?
And how do we prevent the problem from occurring in the first place? And what else
is being impacted by the problem?” This focus can solve immediate issues for a single
unit but neglects the organization’s overall health.
Conversely, data teams offer a bottom-up view anchored in logistical and technical
realities. When projects simply get handed off to a data team for execution once the
problem and solution have already been decided, perspective clashes occur, derailing
timelines and budgets. The remedy is straightforward yet demanding: align these
perspectives before taking action. Clarify ambiguities, bridge knowledge gaps, and
root out blind spots. By doing so, you’ll develop a unified roadmap, aligning what the
business wants with what it actually needs, and ultimately finding the best solution.
This way of thinking necessitates thinking about the problems of translating between
the worlds of business and data as being in two distinct categories:
Top-down problems

Strategies and tools are covered by the methodology in Chapters 4, 6, and 7.
Bottom-up problems

The methodology’s tools and strategies address bottom-up approaches in Chap‐
ters 9 and 10.

Additionally you will learn that what makes JSON Schema exceptionally useful is
that it has two core functions: validation, which is exceptionally well suited for
top-down business/data translation problems, and annotation extraction, which is
also extremely useful for bottom-up translation problem solving. JSON Schema is
also human and machine readable, making it the ideal open source technology for
your organization to implement a unifying strategy.

4 | Chapter 1: The Need for a Unifying Data Strategy



Figure 1-1. Unifying is about creating alignment and understanding of concepts as they
flow between business and data teams to meet different requirements by minimizing
ambiguity, knowledge gaps, and blind spots. While the example in this section describes
a top-down direction, the next section, “The Problem with Problems,” explores what a
bottom-up approach looks like. The goal of unifying is alignment in both directions.

Your Quest for Data-Driven Breakthroughs Begins  | 5



Typically, these large-scale enterprise projects take years to successfully implement.
People leave, and processes are implemented that people either work around or never
learn. There is resistance to doing things in new ways and learning new complex
software. Taxonomist processes can become bottlenecks, database architectures are
debated, business priorities and competitive threats change, and meanwhile, moun‐
tains of new, messy data begin to collect at a faster pace than the data management
project tasked with taming it can handle. In five years, the new management team
that replaced the old one in a reorg goes through it all again with a fresh budget,
believing that they will have a better solution because of some new technology
paradigm and trend, but they never achieve the Nirvana-like data state everyone
craves.

In the context of JSON Schema, validation and annotation extrac‐
tion serve distinct but complementary roles. Validation is the
process of ensuring that a given JSON document adheres to the
rules and constraints defined in the schema, such as data types
or required properties. This helps in maintaining data integrity
and consistency. On the other hand, annotation extraction involves
pulling out additional metadata or descriptive information from
the JSON document, such as field descriptions or default values.
While these annotations do not impact the validation process, they
provide extra context that can be used for generating documenta‐
tion, tool tips in a user interface, or other supplementary function‐
alities. Together, validation and annotation extraction contribute
to both the robustness and the usability of JSON-based data struc‐
tures. You will learn more about JSON and JSON Schema from a
technical perspective in Chapters 2 and 5, and cover validation and
annotations in Chapter 8.

If teams are not collaborating well, and if leaders and employees are not aligned in
their data strategy, why would implementing an extremely complicated enterprise
solution go smoothly, quickly, or successfully? That’s what unifying is about—hence,
the title of this book. It is a data strategy that focuses on the most effective and
simplest way to begin data-centric projects: getting people aligned before hitting the
gas pedal.

Going faster in the wrong direction isn’t progress. Your teams need
to know where they are going, why their efforts are important to
the goal, and how they can work well together.

6 | Chapter 1: The Need for a Unifying Data Strategy



The Problem with Problems
You are excited that your work can help save lives, you are inspired by people’s
passion, and you believe in the company’s understanding of the value of data. You
hear the stakeholders’ accounts of what the biggest problems are and begin pouring
your creativity and ingenuity into searching for a solution. You conduct interviews,
create road maps, and begin building a prototype, thinking you’ve created a truly
amazing thing that the company will celebrate.
Except you find out after months of building your alpha version that stakehold‐
ers didn’t tell you about some other problem that only surfaced when lower-level
employees (who weren’t part of your interviews) started using the application, and
it totally invalidates the approach of your solution. You suddenly have to rethink
everything from scratch, and all of your previous work was wasted. Welcome to
innovation.
Agile is a popular methodology emphasizing flexible and iterative approaches to
product development and project management. Being Agile means being able to
get feedback as quickly as possible about what fails and why, which is the most
important feedback you can get. Agility entails swiftly learning about failures and
their causes, which is the most crucial information one can obtain. By identifying less
successful ideas more rapidly, you can conserve time and effort, thereby accelerating
the discovery of effective solutions.
In order to get feedback as quickly as possible, you decide that instead of building
a new coded prototype, you are going to design a paper prototype, drawing out the
solution on a piece of large paper with a marker and asking people to click on the
paper buttons, moving to different pieces of paper to represent different screens.
Everyone loves it, and you feel like a hero. Hooray! You build another prototype now
that you have something validated—you’ve succeeded!
Then, as you are going through the final stages of validation, a stakeholder suggests
that you present your solution to the medical advisory board. You are informed of a
legal requirement around a commonly used word which totally invalidates a major
set of features in your previous work that you spent a ton of time testing.
No amount of effort at being Agile will give you what you need if you are focused on
the wrong problem—or don’t even know what problem you are trying to solve.
This is what the problem with problems is: How do you know which problems are the
right problems to solve? This is especially true when dealing with organizations that
have siloed perspectives and nonholistic interests. Leaders have budgets, head counts,
and reputations to protect. Their problems are the most important to them.

Your Quest for Data-Driven Breakthroughs Begins  | 7



A critical error that organizational leaders often make is not knowing what type of
problem they are trying to solve. If they attempt to solve a problem in a top-down
manner, diving into solutions without truly grasping the problem, the efforts can
be misguided. Rushing into solutions without proper comprehension—and merely
being Agile and iterating quickly—does little if anything to guarantee success.
Figure 1-2 shows where unifying can create alignment in problem solving. The
depth of understanding and choosing the “right” problems and the right “way” to
solve them makes all the difference. Determining which problems are the right ones
involves understanding these problems in the context of bridging the worlds of
conceptual (business) and technical (data) language and their operational outcomes
(symptoms).

Figure 1-2. Unifying is about giving data champions the capability to understand prob‐
lems and translating them between business and data team perspectives, whether they
require top-down problem-solving approaches or bottom-up problem-solving ones. The
phrase “the problem with problems” serves as a reminder of this principle, highlighting
the pitfalls of a hasty approach and the benefits of a thorough understanding.

Effective problem solving starts with deep understanding. This involves recognizing
how problems are connected across organizational networks and across conceptual,
technical, or operational realms. Leaders need to understand the problem with prob‐
lems, because if they try to address issues from a top-down approach when what
is needed is a bottom-up approach, they are just reacting to symptoms and not
addressing the root causes.

8 | Chapter 1: The Need for a Unifying Data Strategy



Know what type of problem you are solving, concrete or abstract,
before making a decision about how to solve it.

If an organization operates in an Agile way to accelerate development before under‐
standing the problem they are working on solving, what to prioritize, and why,
then teams are building the wrong things faster. The problem with problems is the
foundational problem to solve. Let’s examine what you can do to tackle it.

Unifying Concepts: The Key to Innovation
Concept-first design asks people to explain in plain language the business logic they
use to achieve their goals, the problems they have, and with whom and how they
collaborate. That business logic is translated into a simple pseudocode structure—
simple enough for anyone to read, but structured enough that it can be used as
a rough guideline for building systems. In short, before getting into designing or
building, you ask people to describe what key concepts are important to the tasks they
accomplish at work and why.
Vital knowledge often exists in people’s heads without a shared map to help align
understanding and decision making. The only way to see differences in understand‐
ing concepts and language may risk people’s ability to collaborate effectively together
is to take the fuzzy, implicit map in people’s heads, which teams believe they are
aligned with, and turn it into a focused, external conceptual map.
This process involves assessing and comparing three key aspects of how information
is managed and utilized in your organization:

1. The purpose and design of operational concepts used in business processes
2. Data structures and how they represent concepts utilized in business processes
3. Methods of communication to gain a comprehensive understanding of how

concepts are conveyed and decisions are made

By integrating these three aspects into a single map, you can visualize the connections
between people, problems, objectives, and outcomes. This map helps to identify
and fill knowledge gaps at an early stage. Without this map, individuals may be
making costly and significant decisions without a shared understanding of their
team’s current situation, their intended destination, or the strategies to get there.
Making decisions to build things without a cohesive and comprehensive map is an
easy way to fall into the trap of building the wrong thing faster.

Your Quest for Data-Driven Breakthroughs Begins  | 9



Creating a unified structural map of logic, goals, problems, and success metrics
before designing, building, and testing software can save significant time and money,
making your software development more efficient and cost-effective.

Remember, the goal is to find faults as soon as possible. Staying at
the conceptual level will enable you to move faster.

Your focus is on capturing and defining the fundamental ideas, business logic, and
objectives that underpin the system or application being developed. The aim is to
ensure that all stakeholders are unified in their shared understanding of the core con‐
cepts and that the design is aligned with the intended purpose and desired outcomes.
The benefits of concept-first design are:
Conceptual clarity

Key concepts, ideas, and principles are defined, including business logic, goals,
and the problems that the system is intended to address.

Early alignment
Defining and clarifying concepts early in the development process prevents mis‐
communications that could have required costly reworking later in the develop‐
ment process.

Holistic perspective
Emphasis is placed on how information connects, flows, and is used outside of
human operational silos.

User-centric focus
A strong emphasis is placed on understanding the needs and goals of end users.
The design process is centered around user experiences to create solutions that
are intuitive, effective, and satisfying for users.

Create well-thought-out and purposeful solutions by starting with
a clear understanding of the fundamental concepts and goals that
drive the system or application. Start testing the concepts you and
other stakeholders believe in, getting feedback in an Agile and
iterative way. The more complex projects are, the more a deep
understanding of the underlying concepts is critical to the success
of the project.

10 | Chapter 1: The Need for a Unifying Data Strategy



When one of the authors of this book, Ron Itelman, stumbled upon concept-first
design and successfully used it, the results were shocking. All of the points of concep‐
tual conflict and alignment were identified first, leaders were able to get teams on
board with a single operational model. Nothing was designed or built until teams
across silos could agree to what concepts meant, how they flowed, and what business
logic they supported. The focus was simply can we agree to a set of concepts, how they
are used, who uses them, and why? This is different from waterfall approaches, where
everything is meticulously planned out to be built on a schedule.
Building and implementing the new system went smoothly; there weren’t any friction
points, and it was designed, built, and tested in three months. The system was so
efficient at creating high volumes of high-quality, rich, and meaningful data that the
private equity firm that bought the company paid a premium for the data alone. That
experience led Ron to further research and develop an innovation strategy based on
the foundation of unifying concepts across organizational networks. 

What a Unifying Data Strategy Will Do for Agile
The root cause of any problem that a company faces is not technology. It is the
problem with problems—that leaders and teams are not unified in their language,
understanding, or efforts to prioritize and solve the problems that prevent them from
achieving goals that drive revenue, reduce costs, and create value.

Historically, Agile was a set of principles resulting from frustrations
with the highly structured way that software development contracts
were written; there was no room to deviate from the agreed-upon
work. This was nearly impossible, because as software develop‐
ment work began, unexpected problems, needs, and perspectives
emerged. If the developers only focused on the requirements, they
would be delivering something that required going through lengthy
contract negotiations. The developers and the stakeholders were
often separated. The original Agile principles were about creating
communication, iterating, and adapting quickly in order to learn
what works and what doesn’t, and delivering something of value in
a modular versus monolithic way.

Many other books, strategies, and frameworks have been created to formalize Agile
in their own unique ways, which this book does not cover. This book is the result of
years of research into the top pain points of collaborating with data, and it proposes a
data management strategy that aligns with the original Agile principles.

What a Unifying Data Strategy Will Do for Agile | 11



Traditionally, teams work in their functional areas, but data is holistic, belonging
to the entire organization. This is a primary reason why data teams often struggle
with traditional ways Agile strategies are implemented in organizations. A unifying
data strategy enables zooming out to identify and solve company-wide challenges of
becoming data-centric in addition to zooming in to operate like a traditional Agile
team, using a data-centric lens to drive business value.

Defining Being Agile
No mechanism in nature or technology is more pervasive than the mechanism of feedback.

—Bernard Friedland (Control System Design, Prentice Hall, 2005)

If you ask 1,000 people what Agile is or how it works, you will get 1,000 perspectives,
ranging from loose interpretations such as “Agile means you don’t need requirements
and have a quick meeting every day for 15 minutes to talk about where you are
blocked” and “Agile just means figuring out what works and throwing out what
doesn’t,” to highly structured ones like “you need to measure everything in story
points and measure velocity in spreadsheets, analyzing productivity every two weeks”
and “you need to get the team certified and have dedicated Agile management
experts.”
For the purposes of this book, we will simplify what Agile means, and what an Agile
data strategy means. This book defines the three primary ways of being Agile as
follows:
Remove ambiguity

Deeply understand problems, goals, and people, and identify what you know and
don’t know. This knowledge usually comes from conversations with customers,
stakeholders, your competitors’ customers, and UX research.

Rapidly iterate
Test what you can to validate or invalidate assumptions. This can be A/B testing,
prototyping with Styrofoam or with code. The goal of iterating is to get feedback
—which might be qualitative (surveys and conversations that help explain where
data cannot) or quantitative (numerical data from observations)—as quickly and
directly as possible to remove ambiguity.

Adapt attention to value
Aim to make progress rather than spending too much time setting priorities
and getting stuck in debates. Adaptive attention means removing distractions and
being willing to shift focus. Attention should always be aligned with what actions
will yield the most business value and best results.

12 | Chapter 1: The Need for a Unifying Data Strategy



Agile Theater
If you do not change direction, you may end up where you are heading.

—Lao Tzu, Chinese Taoist philosopher, 5th century BC

When interviewing product managers, engineers, designers, and managers on chal‐
lenges around being Agile, the conversations almost universally revolve around
measuring velocity, missing deadlines, and shifting requirements as new information
is gained. Thinking of velocity as productivity can be a trap; building the wrong
things faster doesn’t equal success. Productivity for the sake of productivity doesn’t
create value. Having stand-ups for the sake of stand-ups isn’t progress. If your Agile
teams aren’t removing ambiguity, rapidly iterating, and adapting attention to what
drives business impact and value, then your Agile processes are at risk of being
mostly ritualistic theater.
In organizations, responsibility is distributed among various stakeholders. Agile
stakeholders collaborate to ensure products, processes, and services are not only
reliable and functional, but that they also generate sustainable value. However, meas‐
uring success solely through velocity points—an Agile metric that quantifies the
amount of work a team can tackle during a single sprint—can cause unintended con‐
sequences. This approach can incentivize engineers to manipulate how they assign
and complete points, leading to a defensive, self-protective culture known as cover
your ass (CYA) that does not create business value.
Overemphasis on velocity metrics can encourage individuals to exploit the system,
focusing more on inflating their stats—such as how many story points they’ve com‐
pleted—rather than on genuine innovation. But the true goal of innovation isn’t
about completing the most story points. It’s about creating business value, positively
impacting colleagues, and serving customers effectively.

Agile, Waterfall, and Unifying
Unifying sets itself apart. Rather than thinking of it as just another methodology,
envision unifying as the “tuning fork” of project management. By introducing it
before design or construction begins, it ensures that whether you opt for Agile or
the waterfall methodology, your approach is fine-tuned for utmost efficiency and
alignment. Think of it as continually adjusting a musical instrument to hit the right
notes; unifying continuously calibrates direction and purpose to ensure harmony in
execution, as shown in Table 1-1.
Unifying aligns at the conceptual level to minimize the costly risks of misunderstand‐
ings and mistakes. It forefronts collaboration and innovation, acting as a tool to
determine the optimal methodology—be it Agile or waterfall—ensuring the highest
chances of project success.

What a Unifying Data Strategy Will Do for Agile | 13



Table 1-1. The benefits of Agile and waterfall methodologies, and how unifying compliments
them

Waterfall Agile With unifying
Defined milestones and measures of Flexible to changes and improving Continuously calibrates perspectives, with a
progress from a macro perspective. goal setting at the micro perspective. focus of quantifiably knowing the right

direction before taking action.
Segments of delivery, such as Faster delivery, incremental Focuses on removing and reducing
factory or supply-chain processes, development, and regular releases. unnecessary effort by minimizing ambiguity,
can be optimized independently. knowledge gaps, and blind spots.
Allows segments to Getting regular feedback by Serves as a translator between technical teams
move autonomously, creating engaging stakeholders and end users and business stakeholders using JSON Schema,
independence and integrity for throughout the development process, ensuring a unified vision across both parties,
individual unit responsibilities. Agile ensures that the product thereby preventing costly misalignments.

evolves based on actual user needs.

Defining a Unifying Data Strategy Approach
For analytics, data science, and machine learning to enable enhanced decision mak‐
ing, they require high-quality data that represents business situations and outcomes
accurately. Too often, data scientists are expected to comb through mountains of
poor-quality data and miraculously extract insights when they should have been
involved in decision making processes.
Business leaders don’t know what they don’t know and are reluctant to take time
that they don’t have to understand data science concepts. Meanwhile, data scientists
frequently lack an understanding of the collaboration and business dynamics of
finance, design, product management, and software development, isolated as they are
in enigmatic realms of mathematical jargon and data tools.
In this book we will explore the innovation challenges faced by organizations and
the importance of embracing a data-centric innovation strategy. The lack of a shared
understanding of goals, and the communication gaps between business leaders and
data scientists, contribute to difficulties and may lead to investments in inadequate
technology.
Overcoming obstacles around data requires fostering a data-centric culture that
emphasizes collaboration, gathering high-quality data, and the strategic involvement
of data and data science teams. Understanding the problems around data helps
organizations transform into truly data-driven powerhouses, ensuring that they can
unlock the full potential of their data and achieve their desired outcomes.
A unifying data strategy approach uses the three characteristics of being Agile—
removing ambiguity, rapidly iterating, and adapting attention to value—and adds a
few key factors:

14 | Chapter 1: The Need for a Unifying Data Strategy



Holistic connected perspective
For data to be valuable, it needs to be connected across the organization. Data
doesn’t belong to any one team; people come and go, teams are reorganized and
shifted, but the data belongs to the organization. A unifying data strategy maps
together the various functional business units, the data they collect, and why the
data is important.

Information flows
For data to be usable, it must be able to flow and be combined and transformed.
Understanding how data flows between teams with differing language, mental
models, and problems, is critical.

Minimal viable data (MVD)
Data-driven decision making requires the right quality and quantity of data. The
minimal viable data (MVD) is the smallest possible amount of high quality data
necessary to make reliable predictions and drive value. Simply having a large
volume of data isn’t sufficient if the quality is poor—as in GIGO. 

Understanding the Phrase Being Data Driven
Almost all organizations say they want to be data driven in order to maximize
efficiency and optimize decision making. But what does this actually entail?
Efficiency is the ability to achieve a desired outcome with minimal waste of time,
resources, or effort. When an organization is aligned, it naturally streamlines pro‐
cesses and eliminates redundant efforts. Teams can work seamlessly, reducing the
risk of miscommunications, mistakes, and misunderstandings. This results in faster
project completion times and more efficient use of resources.
Effectiveness is the ability to achieve desired outcomes with superior degrees of
accuracy or quality. Alignment can improve effectiveness by ensuring that everyone is
harmoniously working toward goals and objectives in a focused way.
An organization is data driven when it masters producing and analyzing data, ena‐
bling decision makers to learn faster, make higher-quality predictions, and identify
problems and opportunities sooner. In other words, they still are making decisions
and balancing information with experience, but decisions are made with the highest
quality data that they can use.
This is the opposite of HIPPO, which stands for highest paid person’s opinion. HIPPO
decisions are made when someone’s opinion is prioritized because of their senior‐
ity rather than because data has been collected to evaluate whether opinions and
assumptions are invalid. Because of reluctance to challenge HIPPO, HIPPO decisions
tend to be tested only after time and money have been spent and things have gone

Understanding the Phrase Being Data Driven | 15



wrong. Worse, poor HIPPO decisions often end up being blamed on the teams who
are supposed to defer to senior employees.

To Be Data Driven, Be Data Centric
Paradoxically, data is the most undervalued and deglamorized aspect of AI…We define,
identify, and present empirical evidence on data cascades—compounding events causing
negative, downstream effects from data issues—triggered by conventional AI/ML practices
that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible,
delayed, but often avoidable.

—Google Research, “‘Everyone Wants to Do the Model Work, Not the Data Work’:
Data Cascades in High-Stakes AI,” 2021.

Having higher-quality data can have a significantly greater impact on the value of
data when compared to higher volumes of lower-quality data. Good data is usually
better than big data because its quality, relevance, and accuracy minimizes irrelevant,
incorrect, or misleading information, and requires fewer resources to process.
Good data makes it easier to communicate insights to stakeholders and facilitate data-
driven decision making, reducing risks and creating true value that is reusable across
the organization. When an organization values data as a critical resource and invests
in the continuous production and maintenance of high-quality, feature-rich data in
order to maximize operational efficiencies and effectiveness, then the organization
can say that it is data driven.
Certainly, using poor-quality data and asking data teams to find value without includ‐
ing them in business decisions and innovation efforts is not being data driven.
Unfortunately, the norm for many teams is using poor-quality data and not including
data teams in decision making. Many organizations operate under the false assump‐
tion that they are data driven when they are not. Here is a real-world example.
A multibillion dollar global company invested a significant amount of money into
their Salesforce system. Sales teams were required to log potential customer interest
in a service. The analytics were showing that sales teams had a 30% closing rate,
meaning that one out of three conversations resulted in a sale. However, upon further
examination, that number was revealed to be completely meaningless!
Salespeople don’t want to document every time they lose a sale, so they were only
putting in sales data when they were confident they could make a sale. Some sales
people were only entering that data when they were confident they could make a
sale, and some simply avoided the task entirely. When sales were won or lost, few
if any sales members logged information as to why, other than selecting a generic
checkbox which was the default selection. From a business leadership perspective, the
organization had invested a ton of money in Salesforce and had a lot of data, but
after several years of collecting data, when they tried to predict who would buy what

16 | Chapter 1: The Need for a Unifying Data Strategy



service so they could know who to focus on or what products to develop, they ended
up with absolutely nothing that was usable from their hefty investments.
In order to be data driven, companies must start by being data centric: examining
everything in your operational business processes to make sure that data really does
live at the heart of your activities. In the previous example, this would entail having
a designer optimize the Salesforce UI for intuitiveness and ensure that information
could be leveraged effectively. In order to make sure that data capture is accurate,
managers need to be trained to incentivize teams to capture data and how to get
immediate value from accurate data.

Data must always be thought of as the center of the universe.

Here’s a checklist you can use to evaluate how data centric your organization is:

• How is data collection from employees managed?
• Is there someone responsible for owning data quality and a data-centric strategy?
• What investments have been made into training managers and teams about data?
• Are experiments and innovation efforts funded in order to learn how to create

value in exchange for data collection?
• How granular is the data collected, and how is data quality measured?

Bottlenecks Preventing Teams from Being Data Driven
100% of customers are people. 100% of employees are people. If you don’t understand people,
you don’t understand business.

—Simon Sinek, Start with Why (Portfolio, 2009)

Over the years, the authors of this book have researched the top pain points and
bottlenecks that data teams face. The hard truth is that business leaders usually have
little understanding of the complexities of managing data and little desire to feel like
they aren’t experts, and they don’t really appreciate technology. What leaders care
about is increasing profits, reducing costs, and growing as fast as possible. After
that, they care about reducing risks. If you are lucky, an organization may have a
philanthropic or employee development culture.
Business leaders want their reports, dashboards, and insights, but they have no idea
that the person they are asking to generate a report has to often dig through data
swamps to find a thousand CSV files, not knowing who created them or why, or even
what the table headers mean.

Understanding the Phrase Being Data Driven | 17



Let’s imagine a business intelligence analyst gets instructions to generate a quarterly
sales projection report. This report allows the finance team to make sure they are
operationally healthy, can pay salary and other expenses, and can prepare reports to
the SEC. Inaccurate projections can have legal consequences and huge impacts on
shareholder price.
First, the business intelligence analyst must meet with stakeholders, who almost never
know what data exists or how it is generated. The analyst gets access to sensitive
financial data stored in CSV, Excel, JSON, and other formats, digging through data‐
sets with commonly used terms such as Revenue unaware that different teams may
intend slightly different meanings for how revenue is calculated. For example, a sales
team might define revenue as the total sale to a client, while an accountant might
define it as the sale minus the salesperson’s commission.
After digging through the data swamp, the analyst needs to set up more meetings
to verify these meanings and ask which dataset is the source of truth. If the analyst
is unlucky, they will not uncover the different meanings—only for the finance team
to discover months later that something has gone wrong and that important and
expensive financial decisions were made on the basis of the analyst’s faulty report.
The finance team is now blaming the analyst. Welcome to the experience of people in
data.

This Book’s Project: Intelligence.AI Coffee Beans
To demonstrate the application of a unifying data strategy, we are going to work
with a sample problem and datasets and explore strategies for working effectively
with data from a technical and business perspective. Intelligence.AI is the company
the authors of this book founded. We will present it as a fictitious company selling
premium coffee beans from around the world with a humorous flair and inspiring
artwork on the coffee bags, as shown in Figure 1-3. The company is small, but it is
building an online presence and wants to be data driven.
You are the CEO of Intelligence.AI. You need to decide which marketing channels
are most effective in driving sales and acquiring new customers. Your marketing lead
has allocated budgets across social media, email campaigns, and in-store promotions,
but lacks insights into the return on investment (ROI) for each channel. You want
to understand the factors influencing customer acquisition in order to optimize
your budget spend and to make informed decisions on inventory management and
pricing. To address these challenges, Intelligence.AI decides to adopt a unifying data
strategy approach to analyze the available data and provide actionable insights.

18 | Chapter 1: The Need for a Unifying Data Strategy



Figure 1-3. Products from the fictitious Intelligence.AI virtual store. This store will
provide examples of applying a unifying data strategy to a small ecommerce startup,
including inventory management, pricing, customer acquisition, design, and copywrit‐
ing. Images prompted using Midjourney (5/11/2023). Left image: “a teddy bear passion‐
ately singing into a microphone hyperrealistic precision 8K, and 8K hyperrealistic.” Right
image: “a dog and cat cuddling together cute, gorgeous, loveable.”

The datasets at your disposal encompass sales, marketing, and customer service. You
plan to A/B test coffee bag designs using annotated labels to describe the concepts
featured in the designs (e.g., teddy bear, dog, cat). One effective way to work with data
is by using JSON, the most popular data format in use today. JSON is a universal lan‐
guage that is both easy to read and incredibly powerful for data-driven applications.
You will learn more about JSON in Chapter 2, and throughout the book you’ll be
unifying your coffee business’ datasets.

Summary
Across most organizations, there are conflicting perspectives on the North Star, or
guiding principle, especially regarding being truly data centric. While leaders may
emphasize a data-driven approach, data problems abound, often resulting in costly
investments that don’t address the fundamental problems: how to think of data from
a holistic perspective, and to identify the right problems to solve. Being data driven
requires a real commitment to creating, curating, and disseminating high-quality data
and supporting a data-centric culture.
Concept-first design involves translating business logic into simple pseudocode struc‐
tures, which clarifies key concepts and aligns stakeholders, preventing miscommuni‐
cations and costly rework.

Summary | 19



A unifying data strategy enables you to quickly identify, address, and learn from
failures. To implement a unifying data strategy, focus on removing ambiguity, rapidly
iterating, and adapting attention to value creation. The costs of not having a unifying
data strategy are bottlenecks, data swamps, and inconsistent use of language, which
hinders data-driven decision making.
In Chapter 2, we will delve into the world of JSON, a popular data format that is easy
to read and powerful for data-driven applications. Understanding JSON is crucial
for implementing a unifying data strategy, as it provides a universal language for
structuring and exchanging data. In Chapters 3 and 4, we’ll explore how to connect
your unifying data strategy to your code.

20 | Chapter 1: The Need for a Unifying Data Strategy



CHAPTER 2
The Lingua Franca of Data: JSON

No matter what industry you work in, JSON is likely already used by your organiza‐
tion and that you have at least heard about it. JSON (JavaScript Object Notation) is
a standard and language-agnostic data interchange format inspired by a subset of the
JavaScript programming language.
JSON is easy to learn. Its simplicity, both in terms of data types and grammar, was a
significant contributing factor to its popularity. The specification that defines JSON
states: “Because it is so simple, it is not expected that the JSON grammar will ever
change. This gives JSON, as a foundational notation, tremendous stability.”
If you are working with data in some way, familiarity with JSON and its ecosystem is
a powerful skill.

This chapter is aimed at the more technical audience interested in
implementing the methodology proposed in this book. See Chap‐
ter 3 to continue learning about these ideas in a business sense.

Introducing JSON
The origin of JSON dates back to the now-defunct Netscape, the first company ever
founded to capitalize on the World Wide Web. Netscape is known for maintaining the
most popular web browser in the mid 1990s, Netscape Navigator, and for inventing
JavaScript, a programming language specifically designed for Netscape Navigator.
By around 1996, Netscape needed a way to exchange data in a lightweight and easy-
to-read format. Netscape engineers started using an ad hoc, serializable version of the
JavaScript syntax to represent data structures. Douglas Crockford, an early adopter of
JavaScript, recognized the potential of this data interchange format, named it JSON,

21



and began promoting its use as an alternative to XML, which was the dominant
interchange format at the time. He described the JSON serialization specification
online in 2002 and published the first draft of it in 2006.
Though Netscape as a company is no longer active, the technologies it created, such
as JavaScript and JSON, have stood the test of time. In fact, JavaScript is the world’s
most popular programming language, and Netscape’s rendering engine provided the
foundation for the popular Mozilla Firefox web browser that is still actively developed
today.
At the time of writing, JSON is the lingua franca, or common tongue, of data.
According to content delivery network (CDN) providers such as Akamai, a majority
of HTTP requests on the internet consist of JSON documents. The Postman API
public network is the world’s largest registry of APIs, serving 25 million users and
more than 500,000 organizations worldwide as of 2023. Most of its API listings—over
18,000 of them at the time of writing—are JSON based. The list includes Salesforce,
Microsoft, Notion, Meta, Slack, PayPal, Stripe, Google, MongoDB, Zoho, Datadog,
Twilio, Amplitude, DocuSign, Booking, X (Twitter), Cisco, and many more.
Popular NoSQL and SQL databases, including PostgreSQL, MySQL, Oracle, Mon‐
goDB, and others, have first-class JSON support. Of course, JSON is natively sup‐
ported by popular data processing software such as Apache Hive, Apache Spark,
and GoogleSQL. Due to the dominance of JSON as a data format, JSON parsers
exist in virtually every popular programming language. According to research from
Stanford University, JSON is so prevalent in the world of big data that big data
applications often “spend 80%–90% of their time parsing JSON documents.” Stack
Overflow found that questions about JSON are being asked at least three times more
often than about other data formats used in the world of data science.
You may be familiar with XML and wonder how JSON compares to it. In 2020,
ProgrammableWeb, then the world’s leading source of news and information about
internet-based APIs, stated that JSON was the most popular response data format in
use, with JSON being used five times more than its closest competitor, XML.

22 | Chapter 2: The Lingua Franca of Data: JSON



A Simple JSON Example
JSON is designed to be human readable even if you are not familiar with its syntax.
Take a look at the following code, for example:
[
  {
     "precision": "zip",
     "Latitude": 37.7668,
     "Longitude": -122.3959,
     "Address": "",
     "City": "SAN FRANCISCO",
     "State": "CA",
     "Zip": "94107",
     "Country": "US"
  },
  {
     "precision": "zip",
     "Latitude": 37.371991,
     "Longitude": -122.026020,
     "Address": "",
     "City": "SUNNYVALE",
     "State": "CA",
     "Zip": "94085",
     "Country": "US"
  }
]

A piece of data represented using JSON is called a JSON document. The preceding
document consists of a sequence of two elements, where each is a set of key-value
pairs defining numeric and textual properties. In the first case, the "City" property is
set to "SAN FRANCISCO", and in the second case the "Latitude" property is set to the
number 37.371991.
Don’t worry about the details yet. We will explore the JSON grammar in “Overview of
JSON Grammar” on page 26.

Introducing JSON | 23



JSON Viewing and Authoring Tools
Viewing and creating JSON documents does not require any tools other than a text
editor, which you likely already have on your operating system of choice. However,
there is plenty of free and proprietary software to aid in creating or visualizing JSON
documents. These tools can be useful if you are not comfortable with the JSON
grammar. We encourage you to explore the JSON documents in this book in your
tools or editors of choice. Here are a few examples.

JSON Hero
JSON Hero (see Figure 2-1) is a popular free, web-based JSON viewer offering rich
visualization modes.

Figure 2-1. The JSON Hero online visualizer being used to explore our example JSON
document

24 | Chapter 2: The Lingua Franca of Data: JSON



OK JSON
If you are looking for a native and offline-first JSON editor, OK JSON (see Fig‐
ure 2-2) is a powerful and pretty JSON editor for macOS that comes with a free trial.
There are lots of similar options out there if your operating system of choice is not
macOS.

Figure 2-2. The OK JSON editor displaying our example JSON document

Visual Studio Code
If you are a Visual Studio Code user, the Outline view at the bottom of the File
Explorer automatically generates a tree-view explorer for the current JSON document
(see Figure 2-3).

Introducing JSON | 25



Figure 2-3. The JSON Editor Visual Studio Code extension displaying our example JSON
document

Overview of JSON Grammar
JSON is built around a small set of data types:
Booleans

The constants false and true
Numbers

Arbitrary-precision real numbers or integers, in both decimal and exponential
notation

Strings
Sequences of characters enclosed in double-quotes

Arrays
Heterogeneous ordered sequences of JSON values (referred to as elements)
enclosed in square brackets

Objects
Heterogeneous sets of key-value pairs (referred to as properties) enclosed in curly
braces, where keys are strings

Null
The constant null

26 | Chapter 2: The Lingua Franca of Data: JSON



Booleans
JSON provides two constants to represent Boolean values: false and true. Note that
these constants are not enclosed in quotes. Also, keep in mind that JSON constants
are case sensitive, so the value False is a syntax error.
An example of the true JSON constant:
true

An example of the false JSON constant:
false

Numbers
In comparison to many other data formats, JSON provides a single number data type
to represent both integers and real numbers. Both decimal and scientific notation is
supported.
An example of a JSON number that represents a positive integer:
5

An example of a JSON number that represents a negative real number:
-47.3265504058

An example of a JSON number that represents a negative positive integer using
scientific notation:
-5.3E4

Note that JSON does not permit numeric values that cannot be represented as
sequences of numbers, such as the concept of infinity or imaginary numbers.

Strings
A string is a textual value that consists of a sequence of characters. A JSON string
starts with the double quote character ", followed by zero or more Unicode charac‐
ters, followed by the double quote character ".
An example empty JSON string that defines no characters:
""

An example JSON string that corresponds to “Hello World”:
"Hello World"

Overview of JSON Grammar | 27



If a JSON string contains a double quote or a backslash, the offending character must
be escaped by inserting a backslash character before it. Here is an example JSON
string that utilizes double quotes:
"My name is \"Juan\""

Remember that enclosing a JSON string in double quotes is not
optional. Without them, your JSON string will not be considered
valid JSON. Forgetting the quotes or making use of single quotes
instead of double quotes, are very common pitfalls when starting to
work with JSON data.

Arrays
A JSON array is an ordered sequence of other JSON values referred to as elements.
An array starts with the open square bracket character [, followed by a sequence of
zero or more elements delimited by the comma character ,, followed by the closed
square bracket character ].
An example empty JSON array that defines no elements:
[]

An example JSON array that defines a single number element:
[ 5 ]

An example JSON array that defines multiple string elements:
[ "foo", "bar", "baz" ]

Note that the comma separator is not included after the last element of the array. In
fact, doing so results in a syntax error. See Figure 2-4 for an example of this error.

Figure 2-4. The free online JSON validator at jsonchecker.com checking a JSON array
with a trailing comma after the last element. An error is reported, as an ending comma
indicates that there is another element in the array.

28 | Chapter 2: The Lingua Franca of Data: JSON



JSON arrays are heterogeneous containers. In other words, the elements of a JSON
array do not need to be of the same type. For example, you may define an array that
mixes Boolean, number, and string elements:
[ false, 4, 3.4, "Hello World" ]

We defined JSON arrays as ordered sequences of other JSON values. A JSON array is
also a valid JSON value, which means that defining arrays of arrays is valid.
Consider the following example that defines a JSON array of arrays. The first element
is an array with a single string element. The second element is an array where its first
and only element is also an array whose first and only element is a number. The third
element is an empty array:
[ [ "foo" ], [ [ 7 ] ], [] ]

Objects
A JSON object is a set of key-value pairs where keys are JSON strings. This type
of data structure is also commonly referred to as a map, an associative array, or a
dictionary.
JSON objects are the most complicated JSON structure, grammar-wise. A JSON
object starts with the open curly brace character {, followed by a sequence of zero or
more key-value pairs referred to as properties, delimited by the comma character ,,
followed by the closed curly brace character }. A JSON object property is a JSON
string that represents the property key, followed by the colon character :, followed by
the JSON value that represents the property value.
An example empty JSON object that defines no properties:
{}

An example JSON object that defines a single number property referred to as foo:
{ "foo": 4 }

Note that object property keys must be valid JSON strings. The following example
JSON documents are invalid:
// Invalid! Property keys cannot be numbers
{ 3: false }
// Invalid! JSON strings must be enclosed in double quotes
{ foo: 4 }

Here’s an example JSON object that defines multiple Boolean properties. As with
arrays, note that the comma separator is not included after the last property of the
object:
{ "foo": true, "bar": false, "baz": true }

Overview of JSON Grammar | 29



An example JSON object defining properties of different types, including objects
themselves:
{ 
  "foo": [ 1, 2, 3 ],
  "bar": {
    "baz": "nested"
  }
}

This JSON object consists of two properties: foo and bar. The foo property is set to
an array of three number elements. The bar property is set to an object that consists
of a single property: a string property called baz.

Avoid Duplicate Properties
The JSON specification states that object properties should be unique. If you define
the same property twice, that’s considered undefined behavior, and different JSON
parsers might react to it in unexpected ways. Consider the following object that
defines the same property twice:
{ "foo": 1, "foo": 2 }

As you can see in Figure 2-5, the free online JSON validator at jsonchecker.com we
used in Figure 2-4 reports this JSON document as invalid.
However, the JSON Hero online visualizer from Figure 2-1 will silently drop any
duplicate property and only consider its last definition. Because you don’t know how
different JSON parsers react to this case, we strongly recommend avoiding duplicate
properties altogether.

Figure 2-5. The free online JSON validator at jsonchecker.com considers objects with
duplicate properties as invalid.

30 | Chapter 2: The Lingua Franca of Data: JSON



Null
JSON also provides the constant null. As with JSON Booleans, null is case sensitive,
so the values Null or NULL are syntax errors.
An example of the null JSON constant:
null

While null is given diverse meanings in different contexts, its origin comes from
ternary logic, which defines null as an indeterminate value. Null values come in handy
with objects in particular. Taking the ternary logic definition of null, setting an object
property to null is equivalent to stating, “I don’t know what the value is.” This is
different from omitting the property, which means, “the property does not exist.”

Some programming languages, like C#, do not make a distinc‐
tion between a null value and a nonexistent value. Therefore, the
semantics of JSON might not directly translate to your program‐
ming language’s data model. Greg Dennis, core contributor to
JSON Schema, wrote an article covering this problem in detail.
We recommend you consult your programming language and
JSON parser documentation to avoid conflicts.

Consider the following JSON object that defines a property as null:
{ "foo": null }

We can interpret this JSON document as stating that the foo property exists, but its
value is not known. In comparison, the bar property does not exist at all in the object.

Learning More
This section was a gentle overview of the JSON grammar. As with every data
interchange format, things get tricky once you start considering edge cases and
interoperability concerns. As an end user of JSON, you rarely need to worry about
these, but advanced knowledge of JSON can come in handy when implementing your
own JSON tooling.
To give you a taste of these concerns, JSON grammar defines a single number type of
arbitrary limit and precision for modeling both integers and real numbers. However,
many programming languages have limits on the numbers they can accurately repre‐
sent. On text encoding, JSON is defined to be valid on any Unicode encoding (such as
UTF-8, UTF-16, or UTF-32), but most applications assume UTF-8.
If you want to learn more about these topics, we recommend reading the free
ECMA-404 (from ECMAScript) and RFC 8259 (from the Internet Engineering Task

Overview of JSON Grammar | 31



Force) specifications. The json.org website also publishes easy-to-follow visual JSON
grammar definitions (see Figure 2-6 for an example) and formal grammar in McKee‐
man Form. We also recommend Introduction to JavaScript Object Notation by Lindsay
Bassett (O’Reilly, 2015), an excellent and concise guide to every detail of the JSON
grammar.

Figure 2-6. An example visual state machine for the grammar of JSON arrays taken
from the json.org official website. These diagrams are handy, concise references to the
JSON grammar.

Minification
In JSON, whitespace present outside the declaration of values carries no meaning.
As a consequence, you can write the same JSON document in a variety of ways. See
Figure 2-7 for an example.

Figure 2-7. Whitespace outside of the declaration of JSON values is meaningless. The
four JSON documents in this figure represent the exact same JSON document, despite
variations of whitespace and new lines.

32 | Chapter 2: The Lingua Franca of Data: JSON



This aspect of the grammar is commonly exploited to reduce the byte size of JSON
documents by deleting unnecessary white space. The process of deleting unnecessary
whitespace in a JSON document is called minification. See Figure 2-8 for an example.

Figure 2-8. On the left, a JSON document consisting of objects, strings, arrays, and
numbers. The size of this JSON document is 62 bytes. On the right, a minified version of
the JSON document on the left. The size of this JSON document is 38 bytes.

Minification is common practice (mainly in conjunction with data compression) to
improve network performance when transmitting JSON documents and reduce the
space required to store them.
You will likely encounter minified JSON documents at some point or another.
Because minified documents are often hard to read (mainly if they are big), we
recommend using visualization tools like JSON Hero or OK JSON that always present
the structure of a JSON document in a readable manner or using specific tools like
JSON Beautify (see Figure 2-9) to add back the deleted meaningless white space. The
process of adding meaningless whitespace for readability purposes is often referred to
as prettifying or beautifying the document.

Overview of JSON Grammar | 33



Figure 2-9. An example of JSON Beautify, one of many online tools to prettify a minified
JSON document to improve its readability

Alternative Representations
Though the JSON grammar is simple, and that is one of the key reasons why JSON
is popular, its simplicity comes with drawbacks. For example, JSON does not support
adding comments. This fact resulted in the creation of many data formats that follow
the JSON data model and are meant to be transformed to JSON without loss of
information, but that provide either a relaxed or alternative syntax.

Textual alternatives
Popular examples of these kinds of JSON alternative formats are JSON5 (JSON with
a relaxed syntax and comments support), Jsonnet (JSON with additional templating
constructs), and YAML (an alternative syntax with many power features).
Here is an example of the JSON5 format taken from its official website. As you can
see, JSON5 is a superset of the JSON grammar that permits comments, single quotes
for strings, the omission of quotes for object properties, and more:

34 | Chapter 2: The Lingua Franca of Data: JSON



{
  // comments
  unquoted: 'and you can quote me on that',
  singleQuotes: 'I can use "double quotes" here',
  lineBreaks: "Look, Mom! \
No \\n's!",
  hexadecimal: 0xdecaf,
  leadingDecimalPoint: .8675309, andTrailing: 8675309.,
  positiveSign: +1,
  trailingComma: 'in objects', andIn: ['arrays',],
  "backwardsCompatible": "with JSON",
}

Here is an example of the YAML data format. As you can see, YAML looks like JSON,
but uses a more minimalistic and indentation-based grammar:
title: "YAML Example"
data:
  name: "John Smith"
  age: 34
  gender: "male"
  address:
    street: "123 Main St"
    city: "Anytown"
    state: "NY"
    zip: "12345"

Binary alternatives
Alternate JSON representations do not need to be textual. A popular binary represen‐
tation of JSON is BSON (Binary JSON). The BSON binary format (see Figure 2-10)
was designed by MongoDB with traversability in mind, and it is the way in which the
MongoDB database internally represents the JSON documents you insert.

Figure 2-10. A hexadecimal example of the BSON version of a simple JSON document.
BSON is not meant to be human readable, but faster for a machine to traverse.

Other binary formats that adopt the JSON data model include JSON BinPack, Messa‐
gePack, CBOR, FlatBuffers, and Apache Avro.

Overview of JSON Grammar | 35



JSON versus the JSON data model
These alternative JSON representations are not JSON, despite their somewhat confus‐
ing names. For example, Jsonnet is not JSON; it is a different serialization format
inspired by the constructs of JSON.
To shed light on these cases, the community makes an explicit distinction between
JSON (the data interchange format) and the JSON data model (the abstract modeling
capabilities of JSON). In other words, you are only using JSON if you follow the
precise textual grammar introduced earlier in this chapter. However, if you define
a format that can be losslessly transformed to JSON, then we say that this format
follows the JSON data model. Putting these definitions into practice, we can say that
while JSON5, Jsonnet, YAML, BSON, and other formats are not JSON, they are based
on the JSON data model.
This distinction is important in the context of this book. While we will be using JSON
examples throughout this book because of its human-readability, what we are truly
advocating for is the adoption of the JSON data model. Not all data you will handle in
your organization will be JSON. However, if such data follows the JSON data model,
you can treat it as JSON, and make use of a humongous ecosystem of powerful tools
designed to work on the JSON data model, such as JSON Schema.
For example, if you’ve used the MongoDB database, you’ve already used tooling that
operates on the JSON data model instead of on JSON directly.

While JSON is an incredibly popular format, the true lingua franca
of data is the JSON data model.

Creating a JSON Document
Chapter 1 introduced Intelligence.AI, an online coffee company selling premium
coffee beans from around the world with humor and inspiring artwork on the bags.
Armed with our new knowledge, let’s use JSON to define example products and
orders.

A Product Entry
A product in our catalog consists of a stock-keeping unit (SKU) that uniquely identi‐
fies the product, a timestamp for when the product was put on sale, a URL to the
product page, the human-readable title and one-sentence description of the product,
the URL of the product image, the price of the product, and the currency on which
the price is represented.

36 | Chapter 2: The Lingua Franca of Data: JSON



A sample product entry, declaring a set of string and integer properties, might look
like this:
{
  "sku": "SHBH00001",
  "timestamp": "2023-04-07T06:32:54-04:00",
  "url": "https://intelligence.ai/products/espresso-yourself-whole-latte-love",
  "title": "Espresso Yourself - Whole Latte Love!",
  "price": 30,
  "currency": "USD",
  "description": "Introducing \"Espresso Yourself\" coffee - the ultimate bean 
potion that's here to add a splash of silliness, a dollop of delight, and a whole 
latte love to your daily grind!",
  "image_url": "https://cdn.shopify.com/s/files/1/0749/8389/9441/products/
Frame7.png"
}

A Store Order
After putting the store online, orders start to come in. What people ordered, when,
and to which delivery locations, is important information we can also model using
JSON. In our store, orders include the email address of the customer, the delivery
address, the items the customer purchased referenced by their respective SKUs, the
total price of the sale, and the date and time when the order took place.
A sample order for two bags of the beans with SKU SHBH00005 and one bag of the
beans with SKU SHBH00006 might look like this:
{
  "timestamp": "2023-04-13T11:30:00-04:00",
  "email": "jsmith@gmail.com",
  "delivery_address": {
    "street": "123 Main St",
    "city": "New York",
    "state": "NY",
    "zipcode": "10001",
    "country": "US"
  },
  "total_price": 110,
  "items": {
    "SHBH00006": 1,
    "SHBH00005": 2
  }
}

Creating a JSON Document | 37



Summary
This chapter covered the JSON data interchange format and its relevance in the data
industry. You now know the distinction between JSON and the JSON data model.
You have examined existing JSON documents and can write your own, keeping best
practices and common pitfalls in mind.
Data for the sake of data is meaningless unless you have clarity on the problems you
are trying to solve and a sound strategy for how data comes into play. In Chapter 3,
you will learn how to think about data-centric products and innovation. In future
chapters, you will optimize how your organization works with data by putting into
practice your knowledge of JSON and data strategy.

38 | Chapter 2: The Lingua Franca of Data: JSON



CHAPTER 3
Data-Centric Innovation:

A Guide for Data Champions

Chapter 2 laid the groundwork for understanding the importance of data and how
it can be structured and utilized effectively within an organization. We’ve introduced
JSON as the lingua franca of data and described how it is an excellent choice to
facilitate the management and exchange of data across different teams and systems.
In the context of our example company, Intelligence.AI, we want to see how data
can be used to drive decision making and optimize business operations. Whether it’s
understanding customer behavior, predicting sales, or evaluating the effectiveness of
marketing channels, data plays a crucial role. However, data alone is not enough. It’s
essential to have a clear understanding of the problems you’re trying to solve and
develop a strategic approach to how data can help solve those problems. This is where
tools like Agile, objectives and key results (OKRs), and key performance indicators
(KPIs) from the fields of product management and data strategy can be valuable in
ensuring that your data efforts are focused and effective.
This chapter delves deeper into these topics. We’ll explore how to think about data-
centric products and innovation and how to align your data strategy with your
business objectives. We’ll also discuss the importance of schemas in managing and
understanding your data. Additionally, we’ll look at how JSON Schema can be used
as part of a unifying data strategy, providing a structured and standardized way to
describe your data.

39



Data Transformations Require Data Champions
Champions are not the ones who always win races—champions are the ones who get out
there and try. And try harder the next time. And even harder the next time. “Champion” is
a state of mind. They are devoted. They compete to best themselves as much if not more than
they compete to best others.

—Simon Sinek, author and motivational speaker

As you strive to become a transformative force for innovation in your organization,
you’ll need to step into the pivotal role of data champion. As a data champion,
you serve as the crucial link between the technical and business aspects of your
organization, ensuring that data is not just collected and managed effectively, but
harnessed to its maximum potential to drive business value.
There are several career paths in the world of data. For example, you may be a data
scientist, data strategist, data product manager, or even a chief data officer (among
many others). Each of these roles requires a unique blend of skills and offers different
opportunities to influence your organization’s utilization of data. A data champion
role could be in any department (data, code, business, or experience design) or held
by any title. Let’s examine more closely what being a data champion means, and how
it relates to the roles in Figure 3-1.

Figure 3-1. Various roles in an organization, the different domains they can be involved
in, and how they relate to being a data champion. A unifying data strategy focuses on a
blend of skills and perspectives from data strategy and product management to develop
data champion skills. XD stands for experience design.

While every company differs in terms of how they design their roles and responsibili‐
ties, the following is a broad, noncomprehensive, general way to group these roles in
the context of this book:
Chief data officer (CDO)

Develops and implements an organization-wide data strategy and oversees data
governance and data management initiatives. A CDO advocates for data teams at

40 | Chapter 3: Data-Centric Innovation: A Guide for Data Champions



the executive level and aligns with other executives on business goals. Typically,
CDO is a strategic role, focusing on the big picture rather than the technical
details of data management.

Data strategist
Identifies opportunities to leverage data for business decision making, advocates
for data quality, helps design data governance policies, develops strategic plans
in collaboration with other teams to understand needs and challenges. While
data strategists primarily operate in the realms of business and data, they are con‐
cerned with the big picture, including how efficient and effective collaboration
and communication are.

Product manager
Responsible for guiding the success of a product by leading cross-functional
teams, defining product vision and strategy, understanding customer needs,
setting and prioritizing product goals, monitoring product performance, and
communicating product plans and progress to stakeholders. Please note: this role
is different from that of a data product manager, which will be described in “The
Rise of the Data Product Manager” on page 42.

Data manager
Oversees the collection, storage, and organization of data; implements data qual‐
ity assurance processes, data security, and data privacy policies; develops and
maintains data documentation and metadata; and promotes best practices. A
data manager may overlap with other teams, but their primary responsibility is
managing data-specific processes, tools, and deliverables.

A data champion is someone who is passionate about helping the
organization innovate and transform into a truly data-driven orga‐
nization. They care about how data is connected across different
domains of knowledge and across different layers, from concrete
and complex details all the way up to strategy.

Being a data champion can be challenging. As with many superhero roles, it can feel
like thankless work, without official recognition or support. Teams are supposed to
align at a high level, and everything is supposed to “fit” together, but this “fitting”
rarely happens without people going out of their way to work collaboratively across
the organization. Aligning and fitting usually come about due to the strength of
personal relationships rather than a well-thought-out and structured process. The
more complex the organization, the more demands it places on data champions, who
risk losing focus on their assigned tasks by spending time focusing on data from a
holistic perspective.

Data Transformations Require Data Champions | 41



Your job title doesn’t matter. You can become a data champion. If your organization
doesn’t have a formal way to empower or support data champions, don’t worry. This
book will provide you with concrete tools and ways to communicate core concepts
to your leadership. The goal of a data champion is to create value, drive innovation,
and lead the organization toward becoming truly data driven. This journey may
be challenging, but the rewards—in terms of improved efficiency, innovation, and
profitability—are well worth it.

The Rise of the Data Product Manager
In the world of data-centric innovation, the data product manager emerges as a
crucial link between the technical and business aspects of an organization, ensuring
that data is not just collected and managed effectively, but also harnessed to its maxi‐
mum potential to drive business value. A data product manager is the closest formal
equivalent to a data champion. Many organizations don’t yet have data product
managers defined as formal roles, especially in smaller organizations where a product
manager, data engineer, program manager, or business analyst might be taking on
data champion responsibilities.
A data product manager is instrumental in steering the creation of data-centric prod‐
ucts, defining product requirements and aligning these with overarching business
goals. They work in concert with data scientists, stakeholders, and other relevant
teams, measuring product performance and making value-based decisions for con‐
tinual improvements. While a data product manager may work solely with the data
team on data products, they may also engage closely with software and experience
design teams. Throughout this book, we will use the terms data champion and data
product manager interchangeably, as our research has highlighted a significant gap in
many organizations’ understanding and allocation of resources for these roles.
As a data champion, you bring to the table a profound understanding of data and
its potential to drive strategic and operational transformation. Your expertise isn’t
confined to the technical aspects of data collection and analysis; you are skilled
at translating these intricate concepts into the language of business, enabling their
practical application to drive outcomes. You are equipped to demonstrate to business
leaders how to unlock value from data, guiding them toward data-informed decisions
that can amplify efficiency, spur innovation, and bolster profitability.
The rest of this chapter dives into how the disciplines of data strategy and product
management can be synergistically applied within an Agile data strategy, which is
precisely how a data product manager can catalyze your organization’s evolution.

42 | Chapter 3: Data-Centric Innovation: A Guide for Data Champions



Alignment Is a Journey, Not a Destination
A data strategist plays a vital role in aligning data initiatives with the overarching
business goals and objectives. They understand the strategic direction of the business
and ensure that all data-related activities support these goals. By ensuring that data
efforts are focused and relevant, the data strategist helps to maximize the value that
data brings to the organization. A data strategist is able to zoom in and out to various
levels of scales and complexity, understanding how processes, people, and technology
connect and impact each other from a systems perspective.
A product manager typically identifies and sets KPIs for an externally facing product
or service, whereas a data strategist sets KPIs for internal business processes and
outcomes, such as implementing a new data governance framework or guiding the
use of data in supporting strong business measures and metrics.
Promoting and enhancing data literacy within the organization is another key
responsibility of the data strategist. They work to ensure that all stakeholders,
from executives to frontline employees, understand the value of data, know how
to interpret and use data effectively and recognize the importance of data quality and
integrity. This might involve creating training programs, developing user-friendly
data tools and dashboards, and promoting a culture of data-driven decision making.
The data strategist also plays a crucial role in facilitating cross-functional collabora‐
tion. They work closely with various teams within the organization, including IT,
business units, and legal and compliance teams. For example, they might collaborate
with IT to ensure the right data infrastructure is in place, with business units to
understand their data needs, and with legal and compliance teams to ensure data
practices meet regulatory requirements. This cross-functional collaboration is key to
ensuring that data is effectively integrated and utilized across the organization.
Finally, a data strategist must embrace continuous improvement and adaptability. The
world of data is constantly evolving, with new technologies, trends, and challenges
emerging all the time. The data strategist needs to stay up-to-date with these changes,
continuously improving data processes and systems, and being adaptable to changing
business needs and environments.

Evaluating Alignment from a Holistic Perspective
Imagine an orchestra. Each section—the strings, the brass, the woodwinds, the per‐
cussion—has its own unique role to play, but for the orchestra to create harmonious
music, all the sections must be in sync, following the same score and the conductor’s
direction. If one section is out of tune or playing a different piece, the entire perfor‐
mance suffers.

Alignment Is a Journey, Not a Destination | 43



In a business aiming to adopt a data-centric approach, each department—marketing,
sales, operations, finance, and so on—is like a section of the orchestra. Each has its
own unique role and its own data needs. But for the business to truly leverage the
power of data, all departments must be aligned in their approach to data. They must
follow the same “score”—the organization’s overall data strategy—and work together
to create a harmonious performance, which in this case means a data-driven business
that operates efficiently and effectively.
This is what we mean by holistically evaluating how an organization aligns around
producing and consuming data. It’s about ensuring that all parts of the organization
are working together toward the same data goals using the same methodologies and
speaking the same data language. It’s about recognizing that data is a shared resource
that belongs to the entire organization, not just one department. It’s about breaking
down silos and fostering collaboration and communication across departments.
Without this alignment, efforts to become data-centric can easily become disjointed
and ineffective. You might have one department collecting data in one way and using
it for one purpose, and another department collecting similar data in a different way
and using it for a different purpose. Such a situation can lead to inconsistencies,
inefficiencies, and missed opportunities.
Initiate your role as a data champion as if you were embarking on a tactical sea
voyage to hunt for treasure (the business outcomes and value that will be unlocked
upon successful execution). The process of planning the alignment of an organization
around data, viewed from a macroscopic perspective, can be compartmentalized into
the following stages:

1. Form your team. Much like assembling a dedicated crew for a challenging voyage,
the first step is to identify the key stakeholders in your organization who interact
with data. This includes not only those who directly handle the data trove, such
as data scientists and analysts, but also those who rely on data for steering their
strategic course, like executives and managers.

2. Chart your course. Next, you need to figure out how these crew members cur‐
rently operate with data. What objective serves as their North Star? What data
serves as their guiding lighthouse? How do they gather, analyze, and interpret
it? What decisions are made based on this data? This stage involves both formal
leadership meetings and informal team discussions, as well as direct observation
of operational practices.

3. Check your compass. Once you comprehend the current data navigation methods,
you need to assess how well these practices align with your organization’s overall
business goals. Are stakeholders utilizing data in ways that facilitate reaching
these goals? Are there obstacles or rough waters between data practices and
business targets?

44 | Chapter 3: Data-Centric Innovation: A Guide for Data Champions



4. Spot the sea monsters. If trouble is brewing, the next step is to spot the monsters
causing the trouble. These might be technical beasts, such as lack of access
to necessary data or obsolete data analysis tools. They might also be cultural
krakens, such as resistance to change or lack of understanding about the value of
data.

5. Prepare a treasure extraction plan. This plan might involve technical solutions,
like implementing new data tools or enhancing data accessibility. It may also
necessitate cultural adjustments, like training programs to boost data literacy or
initiatives to cultivate a more data-centric culture. This strategy should not only
guarantee preparedness but also be robust enough to anticipate potential hazards
and enable efficient execution when you strike gold.

Remember, alignment is not a one-time task but a continuous
process. Regularly revisit these steps to ensure your organization
remains aligned as it evolves and as its data needs change.

The Goal Isn’t Alignment, It’s Effective Alignment
No matter how brilliant your mind or strategy, if you’re playing a solo game, you’ll always
lose out to a team.

—Reid Hoffman, LinkedIn cofounder

Effective alignment in an organization can be likened to a jazz ensemble. Each
musician in a jazz band has their own instrument and unique style, but they all
play in harmony, following the same rhythm and working toward the same musical
piece. They have the freedom to improvise and express their individuality within the
framework of the song, but they also grasp the importance of staying in sync with the
rest of the band. This balance between individual creativity and group coordination is
what makes the performance captivating and successful.
Similarly, in a business aiming for a data-centric transformation, each department has
its own unique role and data needs. But for the transformation to be successful, all
departments must work in harmony, following the same data strategy, and working
toward the same business goals. They must have the freedom to use data in ways
that best support their specific needs, but they also need to adhere to the overall data
governance and standards of the organization. This delicate balance between creative
freedom and systems control is what drives business value and innovation.
The essence of effective alignment is not static; it’s a dynamic process that requires
continual reassessment and adjustment. It’s not enough to simply establish alignment
and then assume the job is done. Just as the business environment is constantly

Alignment Is a Journey, Not a Destination | 45



changing, so too must the alignment within an organization continually evolve and
adapt.
This is where the concept of continual improvement comes into play. It’s not just
about maintaining alignment, but about constantly seeking ways to improve what
people are aligned for and how they are aligned. It’s about continually refining the
shared vision and goals, and constantly seeking better ways to achieve them.
Continual improvement also applies to the methods and metrics used to measure
alignment. As our understanding of the business and its environment evolves, so
too should our measures of alignment. We must continually reassess our metrics to
ensure they are still relevant and effective and adjust them as necessary to reflect
changes in our goals or circumstances.
In this way, as we’ve pointed out, alignment becomes a journey rather than a destina‐
tion. It’s a continual process of seeking, assessing, adjusting, and improving. It’s about
ensuring that everyone in the organization is not just moving in the same direction,
but constantly moving toward a better version of what the organization can be. This
continual improvement mindset, underpinned by data, is what drives an organization
forward and ensures its long-term success.

Strategies for Setting Up Teams for Success
One important responsibility for a data strategist is designing systems and processes
for people to effectively achieve goals together. Innovation is going to inherently
invite mistakes. When people are focused on protecting themselves from blame, they
aren’t going to take risks or be open to any new ideas. The blame game can go all the
way up. For example, if an employee made a mistake, you could blame the manager
for why the employee made the mistake. And you could blame the CEO for why the
manager made the mistake that enabled the employee to make the mistake.
A company’s most expensive costs are usually its investments in hiring, training,
retaining, and managing people, including security, hardware, software licenses,
office expenses, and so forth. Getting teams to work effectively together is incredibly
valuable to the bottom line.
Google spent two years studying 180 teams with more than 200 double-blind inter‐
views, and they evaluated the teams on over 250 attributes in a project named
Aristotle.
Here are a few factors they found that are not associated with high-performing teams:
Colocation of teammates (sitting together in the same office)

Where people work isn’t statistically significant to how well the team operates.
Consensus-driven decision making

Getting everyone to agree isn’t associated with better performance.

46 | Chapter 3: Data-Centric Innovation: A Guide for Data Champions



Seniority
The number of team members with higher-ranking titles isn’t a reliable predictor
of better productivity and outcomes.

Here are the top three factors the project identified that did characterize high-
performing teams:
Psychological safety

Team members don’t have to fear punishment for admitting mistakes, asking
questions, offering ideas, or taking risks.

Dependability
Members can rely on each other to complete tasks they said they would.

Structure and clarity
Teams have clear goals (Google uses OKRs), processes for meeting those goals,
and clearly defined performance indicators and outcomes.

It bears repeating that data belongs to the whole organization, not
one person, one team, or even one division. Everyone is collectively
responsible for the quality and value of data, so leaders need to edu‐
cate themselves and their employees about data quality standards.

Here’s what teams and organizations must do in order to enable effective alignment to
thrive:
Establish clear roles and accountability

Clearly define team members’ roles and expectations. Responsibility fosters a
sense of ownership and accountability, as does ensuring everyone understands
how their work contributes to the organization’s data-centric innovation efforts.

Encourage open communication
Open communication that is transparent, candid, and kind is vital for effective
alignment. Facilitate regular interaction between teams through meetings and
workshops, and encourage bottom-up communication that is outside of hier‐
archical lines. This will enable more accurate knowledge sharing about what
works and what doesn’t in how teams approach collaboration.

Develop shared goals and objectives
Formulate common goals and objectives in detail, and question any investments
that aren’t aligned with the organization’s strategic direction. Develop a collective
framework for evaluating the success of data initiatives and how conflicts can be
mitigated.

Alignment Is a Journey, Not a Destination | 47



Provide training and support
Invest in training and support to help team members acquire the skills and
knowledge necessary to contribute meaningfully to data-centric innovation
efforts. This includes technical training in data analysis and visualization tools
and soft skills training in areas like communication, collaboration, and problem
solving.

Incorporating a Product Management Mindset
Product management is a multifaceted discipline that sits at the intersection of busi‐
ness, technology, and user experience. A product manager is responsible for guiding
the development, production, marketing, and overall success of a product or product
line. They act as the nexus between various cross-functional teams, including engi‐
neering, design, marketing, and sales.
A good product manager possesses a unique blend of skills. They have a deep under‐
standing of the market and the customer, allowing them to identify opportunities
and define product requirements that meet customer needs and align with business
objectives. They are skilled communicators, able to articulate the product vision and
strategy to different stakeholders, from engineers to executives. They are also adept at
prioritizing, making tough decisions about what features to build, and how to allocate
resources based on a deep understanding of customer needs, business constraints,
and competitive landscape.
Let’s review key topics from product management that are especially important to
develop and integrate into the role of a data product manager:
Defining data users’ needs

Just as product managers must understand their end users to build successful
products, data product managers must understand the needs of the stakeholders
who will use the data products. This includes understanding how the data will be
used, what insights are expected from it, and what business questions it should
answer.

Defining product features
A data product manager is responsible for defining the features of the data
product, such as reports, visualizations, or predictive models, and prioritizing
their development based on business value, feasibility, and user needs. They need
to make strategic decisions about what features to build next, which requires a
strong understanding of both the data and the business context.

Defining and measuring success
Product managers use OKRs, KPIs, and other metrics to measure the success of
their products and make informed decisions. Likewise, data product managers
must define clear success metrics for their data products. These metrics can

48 | Chapter 3: Data-Centric Innovation: A Guide for Data Champions



be related to usage (for example, how often reports are accessed), impact (how
decisions influenced by the data product led to business outcomes), or user
satisfaction.

Defining Data Users’ Needs
Consider this scenario. You’re a dedicated data scientist, poised to demonstrate
how effectively harnessing data can provide tangible value to your company. Your
objective is to uncover data-driven insights that improve the allocation of marketing
resources and lower the churn rates of customers who subscribe to a monthly service.
Achieving this could mean the difference between a spate of layoffs and a return to
profitability, between an uncertain future and a renewed trust in your data science
team’s capability to drive genuine business value. You are given two weeks to devise
a data-centric strategy that could turn the tide and to present it to the CFO. The
integrity and accessibility of your data can either help or hinder you.
Your initial stop is the data lake, the expansive repository housing your organiza‐
tion’s data. You’re in search of key marketing data, including campaign specifics,
user interactions, sign-ups, drop-offs, and expenditure. Instead of a comprehensive
guide to assist your search, you are faced with a labyrinth of folders and generically
labeled .csv files. It’s the proverbial search for a needle in a haystack.
You engage your colleagues in the marketing department, hoping they can provide
some guidance. The most knowledgeable person about the data has left the company,
but after several meetings and considerable time waiting for responses, you finally
get a lead. You return to the data lake equipped with slightly more information. You
locate some relevant files, sanitize the data, and confirm with the marketing team
that you’re heading in the right direction. A week has elapsed and you’re just getting
started.
As you dig into the data, you encounter another hurdle: inconsistent terminology.
One dataset uses “GTM,” “TOFU,” “CLTV,” “CAC,” “PPC,” “CTA,” while another uses
“CPA,” “CPC,” “CTR,” and “PPR.” You’re back to the marketing team for explanations,
but they can only provide a conceptual understanding. The exact calculations remain
a mystery.
Then you discover data quality issues. There’s no standardization in data collection
or reporting methods, you don’t have access to all of the financial data you need, and
you’re unable to trace the data lineage to back up your findings.
The deadline is pushed back. A month later, you finally have everything you need to
start working on your data science insights.
This exemplifies the critical importance of breaking down silos and fostering cross-
functional collaboration for data-driven decision making in marketing. If your organ‐
ization’s data were a product, it would be difficult to sell. The complexity, inefficiency,

Incorporating a Product Management Mindset | 49



and bureaucratic challenges of getting and using it would deter any customer. This
fictitious example of a poor data experience illustrates that organizational systems
and decisions around managing data are not designed with the user in mind. They’re
inconsistent, difficult, and time-consuming to navigate. Instead of enabling you to do
your job, they create barriers that you have to overcome.

While organizational leaders unfortunately seek quick technology
solutions, they fail to recognize that most problems created by
poor data experiences are caused by decisions, and that expensive
solutions that promise quick fixes rarely live up to those promises.
The fault isn’t in the technological solutions out there, it is that
technology cannot address operational issues caused by a lack of
policy that supports a data-centric approach to achieving business
goals. If there are core problems around data experiences, the root
cause of those problems need to be addressed, and they are rarely
related to technology.

A data experience begins the moment a data consumer requests data, and it continues
through the process of using the data to complete tasks, including the collaborative
and social experience of sending data downstream. Your data team should have
seamless, efficient, and effortless user experiences with the data itself and with data
management so that they can focus on creating real value.
In summary, data experiences can make or break your ability to generate insights and
create value. A poor data experience can lead to delays, frustration, and suboptimal
results. A positive data experience, on the other hand, can empower you to do your
best work, leading to better outcomes for your organization and the people it serves.
It’s time to prioritize data experiences and recognize them for the critical role they
play in data-driven decision making.

Defining Product Features
When considering a commercial product—whether it’s an athletic shoe, a Photoshop
subscription, or a cup of coffee—we often picture a simple transaction: money
exchanged for a tangible or intangible item of value. Even in scenarios where a pur‐
chase is made on behalf of an organization, it’s ultimately a person who is responsible
for a transaction.
In the context of data, we introduce the terms data consumers and data producers.
Throughout this book, we’ll be focusing on data products intended for internal
customers and consumers (employees, leaders, and the systems managing internal
data) unless explicitly stated otherwise.
A data consumer is an individual within an organization who uses data to perform
a specific task. A data customer, on the other hand, is the team or leader who funds

50 | Chapter 3: Data-Centric Innovation: A Guide for Data Champions



the data service as part of their budget. Typically, data customers are strategic deci‐
sion makers, while data consumers are data scientists, data engineers, and business
analysts who utilize data in their various roles.
Interestingly, data consumers often transform into data producers. For instance, a
business analyst who collects data and transforms it into a report for executives to
consume is, in fact, a data producer. Similarly, software engineers who implement a
button on a web page that captures interaction analytics when a customer clicks it are
also data producers.
In this context, we also introduce the concept of data distributors—those responsible
for distributing data across the organization to the right people at the right time.
Roles in data governance and data engineering often fall under this category.
The journey of data from its point of origin to its point of consumption is often
referred to as moving upstream and downstream. For instance, when a software
engineering team implements a feature that generates data (like the button-click
event), they are considered to be upstream from the business analyst who uses that
data. When the data is transformed into a report by the business analyst and passed
along to an executive, the data is said to move downstream, as shown in Figure 3-2.

Figure 3-2. A data producer (person or system) is upstream from a data analyst, who
consumes the data and becomes a data producer, sending the data downstream to
executives, who are the new data consumers. Therefore, defining product features should
consider both scenarios of producing and consuming data and the impact on others
along the flow of information.

Upstream and downstream are dependent on the perspective of
who is consuming and producing data. For example, from a data
engineering perspective, they might be consuming data upstream
from a nightly batch process and producing datasets via data pipe‐
lines for an analyst who is downstream from them.

One of the most significant challenges in managing data is navigating the complexi‐
ties of this process. There is a cost every time data is communicated and transformed,
and miscommunication and misunderstandings caused by poor data quality or
poorly managed data can be incredibly wasteful.

Incorporating a Product Management Mindset | 51



Therefore, product features defined by a data project manager impact the entire flow
of how data is produced, consumed, distributed, and transformed. User experience is
important not only to data consumers, but also the business value derived from the
data and the operational efficiency of distributing the data. Understanding product
features depends fundamentally on perceiving how data flows between people and
processes.

Defining and Measuring Success
One of the primary roles a product manager holds—irrespective of whether it is
stipulated in their job description—is the ability to define and measure success. This
is an absolute necessity, a cornerstone in the architecture of their responsibilities.
While a conventional product manager caters to external clients, a data product man‐
ager usually focuses on serving an internal clientele—the employees who generate
and utilize data. OKRs and KPIs are potent tools for setting targets and assessing
performance. OKRs comprise an objective—a clear statement of what you intend
to accomplish—and key results, which are quantifiable means to gauge the progress
toward the objective. KPIs are specific metrics that reflect the performance of an
organization, a team, or an individual toward realizing their business objectives.
Consider a scenario where a business needs to achieve $1M in sales by a certain
season to meet its payroll requirements. In such a case, a KPI can be established
to scrutinize the vitality of the sales. An executive can view a report and instantly
recognize any impending issue. An OKR symbolizes an aspirational goal, such as,
“Double the sales by improving the attention to personal preferences of clients in
communication and execution.” Key results are assessed in terms of the percentage of
the goal achieved.

Bear in mind that data can potentially be misused as a punitive
tool, which in turn encourages individuals to set the lowest plausi‐
ble OKRs and KPIs to avoid the risk of unfavorable in outcomes.
Instead, you should consistently strive to establish goals that have a
meaningful impact and generate substantial value.

Unifying Versus Aligning
There’s a subtle yet significant difference between the terms aligning and unifying.
Alignment refers to the synchronization of people within an organization, ensuring
they work toward a common objective. This encompasses shared understanding,
mutual goals, and coherent strategies, akin to a band playing in harmony. However,
alignment is not a static state; it’s a dynamic, ongoing process necessitating continual
reassessment and adjustment.

52 | Chapter 3: Data-Centric Innovation: A Guide for Data Champions



On the other hand, unifying refers to a deeper level of integration, encompassing not
just people, but also processes and technology. It’s the process of fine-tuning these
three elements to interconnect and work together efficiently and effectively. Unifying
necessitates a continual commitment to improvement, a willingness to be self-critical,
and the use of data to evaluate and drive change. It requires awareness of one’s biases
and the humility to correct them.

The essence of unifying lies in its ability to break down silos, con‐
nect disparate elements, and foster an environment where everyone
and everything works in unison toward a common objective.

A data champion or data product manager is able to see how data flows and connects
across various silos, diving into low-level technical conversations—such as how to
use JSON and JSON Schemas to model business processes—and then zooming out to
high-level strategic conversations, such as setting KPIs on financial performance and
designing ways for teams to better collaborate and remove barriers and inefficiencies.
The example in “Defining Data Users’ Needs” on page 49 about leadership needing
a machine learning team to optimize marketing budgets at the risk of layoffs comes
from a real-life consulting experience. Because the company currently had teams with
two separate objectives, one team was incentivized by one KPI—how many custom‐
ers signed up—but they didn’t care whether those customers would quit shortly after.
This is an example of how KPIs can skew incentives. Meanwhile, the data science
team responsible for analyzing churn and providing insights on how to reduce it
had absolutely no impact or influence on the root cause of the problem. The advice
we gave was that rather than having one team using data science to optimize ads to
get customers to sign up for subscriptions and another team working on reducing
churn, leadership should think about the problem holistically. The goal is to spend
the minimal amount of money on finding the customers with the minimal chance of
canceling their subscriptions.
The resolution necessitated a fusion of data strategy and product management. The
individual who sought our consulting expertise had to step into the role of a data
champion, which required approaching executives with a bold claim: “The existing
systems, processes, and experts you’ve put in place do not optimally support the
data team’s ability to generate business value.” Making such a statement can indeed
be daunting, and it can potentially disrupt established norms. The imperative was
the essence of the unification we advocate: to align the business and data teams’
objectives, and subsequently delve deeper technically to harmonize their processes,
data collection, and analysis methodologies. By optimizing their surroundings, they
enhanced the utility of their data, bolstered their teams’ performance, and ultimately
improved their business outcomes.

Unifying Versus Aligning | 53



As a data champion, you are tasked with catalyzing changes in business processes
and breaking down silos to align business goals with data perspectives. Instead of
approaching problems based on the organization’s current state, rules, processes,
and silos, you must have the goal of unifying business, data, and code into a single
coherent strategy. This approach underscores your willingness to take risks for the
betterment of the business and the necessity of deriving value from data.
The ability to zoom in and out between various levels of abstraction and complexity
in the worlds of experience design, data, and code while synchronizing with your
organization’s communication channels is what will make you most effective as a data
champion.

As a data champion, your greatest challenge, skill, and source of
reward lies in the perpetual commitment to unifying at various
levels and domains, persistently refining how all the pieces connect
and influence each other. Unifying is about creating an intercon‐
nected system where people, processes, and technology are not just
aligned but are deeply interwoven, creating a robust, efficient, and
adaptable organization. This level of organizational unity can drive
significant efficiencies, foster innovation, and ultimately contribute
to sustainable business success.

Summary
Data product managers, much like skilled chess players, must strategically maneuver
various elements within an organization to achieve a desired outcome. They need
to understand the entire board—or the entire organization—and how each piece, or
department, can best contribute to the overall strategy.
Each chess piece has a unique role and movement pattern, and the same can be
said for the different departments within an organization. For instance, consider the
design of a website. A data product manager, like a chess player thinking several
moves ahead, must consider how the design needs to be altered not only to utilize
insights gained from data but also how to collect the necessary data for future
predictions. They must work closely with the design team to ensure that the site is not
only aesthetically pleasing and user friendly, but also optimized for data collection
and analysis.
Similarly, a data product manager must determine how software engineering can
create high-quality data upstream at the source. This might involve implementing
specific data collection features or standards in the software development process,
or it might mean working with engineers to ensure that the data generated by the
software is accurate, reliable, and useful for downstream analysis.

54 | Chapter 3: Data-Centric Innovation: A Guide for Data Champions



In the chessboard analogy, if cash is an organization’s king—the lifeblood of the
organization—then data is the queen, a powerful piece that can move in any direction
and has the potential to change the course of the game. Just as a chess player protects
and strategically uses the queen, a data product manager must safeguard the quality
of the data and leverage it strategically to drive business value.
Data product managers play a key role in steering organizations toward becoming
more data-centric. They advocate for the strategic use of data across all levels of
the organization, from daily operations to long-term strategic planning. Above all,
because data belongs to the entire org and not any one team or functional depart‐
ment, they work to embed data thinking into the organization’s culture, breaking
down silos and fostering cross-functional collaboration around data.
The data product manager is not just a role, but a mindset. It’s about seeing the
potential of data to transform the organization and having the skills, knowledge, and
influence to turn that potential into reality. It’s about leading the organization on
its journey toward becoming truly data driven, where data informs decisions, drives
innovation, and creates value at every level.
Now that we’ve had a thorough review of what a data champion is, and how a data
product manager incorporates parts of roles and responsibilities from data strategy
and product management, Chapter 4 will cover what a data product is and discusses
schema-first design in detail. Knowing how to design data products and use schema-
first design will provide you with effective tools that a data product manager can use
to implement an Agile data strategy.

Summary | 55



CHAPTER 4
Concept-First Design for Data Products

In this chapter, you will learn a new and straightforward definition of a data product,
described across four facets. You’ll also discover a method called concept-first design
for crafting data products and understand their relationship to schemas.
Concept-first design is an effective strategy that seamlessly bridges the communi‐
cation gap between nontechnical business teams and technical teams. It employs
universally understood tools, such as diagrams and spreadsheets, to represent ideas
and challenges.
The beauty of concept-first design lies in its simplicity. One translates business
language or logic into a hierarchical format, akin to how JSON and JSON Schema
operate—these are digital languages that machines comprehend. By using concept-
first design, the effort to convert business logic into code is reduced. This minimizes
the errors and inefficiencies often arising when developers and data stewards make
assumptions during concept modeling.
Imagine a data product as a box containing a single tool, complete with everything
a user needs to accomplish a task. Think of buying a friend an electronic gift. If the
gift lacks a battery or instructions, it leaves your friend scrambling to find missing
pieces—a frustrating experience. This is precisely how you must think about data
products. A data product should be a comprehensive package, providing users with
everything they need to utilize the data effectively. The goal is to avoid making users
chase down additional components or “fix” elements to make the data usable.
The concept-first design approach is the key to creating these all-inclusive data
products. It’s a method of translating the concepts you’ve derived from stakeholder
conversations into a pseudocode format. This format acts as a relay baton, allowing
you to pass the concepts to data practitioners to implement in line with the business
needs of your data product. Concept-first design provides a high-level representation

57



of concepts (either visually or textually), whereas a schema deep dives, formalizing
these concepts with meaning and hierarchical structures.

Consider this: if you interview five stakeholders and ask them to
describe a problem and the concepts involved, you’re likely to get
five different perspectives. However, if you ask them to formalize
their definitions into a pseudocode data structure, you’ll see more
concrete alignments and misalignments and expose the gaps in the
shared understanding of problem concepts and objectives.

Being data driven refers to an approach where data is at the core of the decision-
making process. The idea is that all insights, strategies, and decisions are based
on data analysis and interpretation, rather than solely on intuition or personal
experience. In a data-driven approach, data is considered a critical asset, and the
systems and procedures are designed around it to ensure its accuracy, consistency,
and availability.
In order to become data driven, it is crucial to first align people with the concepts
that the data is going to represent. This is where concept-first design comes into play.
Concept-first design is about defining the concepts and their relationships before
they are transformed into a data model. It starts with understanding the business
requirements, defining the key concepts, and making sure everyone involved has a
shared understanding of what these concepts represent, for whom, and why.
These concepts form the foundation for the data model and help to ensure that it
accurately reflects the real-world scenario it is intended to represent. To implement
a concept-first design, stakeholders must define these concepts, structure them for‐
mally into schemas, assign meaning to the used vocabulary, identify any gaps, and
work collaboratively to align everyone’s understanding. This leads to the creation
of a shared understanding that adequately meets the stakeholder needs. Once this
shared understanding is achieved, the data product is well designed and ready for
implementation.
The transformation into a data-centric organization begins with a concept-first
design approach, fostering a culture where decisions are made based on data, leading
to increased efficiency, better insights, and more informed decision making. This
chapter is your operating manual. As the data champion, you’re the crucial link
between business stakeholders, data, and coding teams. Chapter 5 will delve into the
nuts and bolts of implementation. For now, let’s focus on your role in creating a
compelling and effective data product.

58 | Chapter 4: Concept-First Design for Data Products



Packaging and Products: An Example Using Coffee
As we’ve established, data should be considered a product, and to hammer that home
we’re going to use an actual product, coffee, as our running example as we talk about
data. Picture yourself as an artisanal coffee producer. Your craft revolves around
having detailed knowledge of coffee varieties and the process of transforming raw
beans into a tantalizing brew. You journey to Colombia, one of the world’s premier
coffee-producing nations, to source the finest beans and partner with local farmers to
ensure optimal harvesting and processing methods.
You opt for small-batch roasting to maintain meticulous control over the process,
fine-tuning the roasting conditions to achieve the perfect flavor profile. Unlike
large-scale commercial roasters, your intimate approach allows for a more distinct
and refined taste. Each batch, even those from the same source, demands unique
adjustments based on factors like the age of the beans and the ambient roasting
conditions.
Upon completing the roasting, you package your beans to shield them from oxygen
and light, appending a label indicating the brand name, coffee type, roast level, and
other pertinent information. Your hope is to catch the eye of potential buyers with
your enticing blend and attractive packaging.
In a local coffee shop, a woman—a business leader by day and a mother by night—
spots your coffee beans. Intrigued by the artisanal packaging and anticipating a long
day of tackling complex financial problems, she decides your coffee would be an
excellent way to start her workday. So, she purchases your product.
She wouldn’t buy the coffee if it lacked proper labeling. Nor would the store sell it if it
didn’t meet FDA regulations, including clear source labeling. So, before the coffee can
be enjoyed, the customer interacts with the packaging.
The bag provides context—indicating the coffee’s origin (Colombia), the manufac‐
turer (you, the artisanal coffee producer), and the product’s purpose (to deliver artisa‐
nal organic medium roast coffee directly from farmers). It also contains nutritional
information and an ingredient list, a standardized structure explaining the product’s
composition. The bag’s barcode enables the store to identify the specific product
being purchased, eliminating any ambiguity about the transaction. Only after these
interactions with the packaging and store can the customer use the product inside the
bag—your delicious coffee beans.
In this analogy, the coffee beans symbolize data. The difference between raw data
and a data product lies in the additional contextual elements—the who, what, which,
why, how, and where. Just as a bag of coffee beans must meet certain requirements
before being stocked on the shelf and purchased by a customer, a data product of
quality needs to meet similar standards for an organization to safely distribute and

Packaging and Products: An Example Using Coffee | 59



use it. Leaving gaps between data and data products often results in wastage and
inefficiencies in how organizations manage data.

The Four Facets of a Data Product
A data product encapsulates both the data and its packaging into a single, self-
contained object. As a data product manager, your role is to maximize the data pro‐
duct’s value, which is measured by the utility and quality of user experience for the
person working with the data and its financial worth. A well-designed data product
has clear measures and metrics, akin to commercial, customer-facing products like
subscription services or physical goods.
If a company is already monetizing its data by selling it to other businesses, it is
inherently treating its data as a product. However, viewing data as a product is
even more crucial when considering internal clients, namely the company’s employ‐
ees. The same principles and best practices that are applied to create valuable, user-
friendly, and business-aligned data products for external customers should also be
used for data products intended for internal use. Whether the data product is facing
outward or inward, it is still a product of the company and should be treated with the
same level of care, precision, and quality. The value of treating data as a product is not
limited to external transactions; it’s equally crucial to the internal data handling and
decision-making processes.
The user experience with a data product begins the moment a user encounters it.
If a user opens a CSV file and struggles to understand the column names, that’s a
poor user experience. We can gauge user experience with data much like a product
manager would measure a web form’s usability, including time taken, number of
clicks, and any issues that lead to drop-offs or lost opportunities.
As a data product manager, your role is to create fantastic data product experiences.
High-quality, user-friendly, and business-aligned data products save time, instill trust,
and are optimized for business objectives. They are, without a doubt, a valuable asset
to your organization.
Yet, if your data product isn’t one self-contained object, it’s incomplete and of inferior
quality. Figure 4-1 shows the four facets of a well-crafted data product, which like
a physical product, encapsulates the dimensions needed to minimize ambiguity and
reduce knowledge gaps and blind spots in our data.

60 | Chapter 4: Concept-First Design for Data Products



Figure 4-1. The four facets of a data product are meant to ensure data hygiene, as
described in the Preface, so that anyone using data has everything they need to work
with data efficiently and effectively in a self-contained object.

A well-designed data product, much like our artisanal coffee, boasts the following
characteristics. Think of them as four facets of a gemstone:
Data

This is the what: the actual information contained within the data product.
Structure

This facet deals with the how. How is the information expected to be formatted
when communicated?

Meaning
This facet addresses the which. Which definition of a word is intended, given the
language and concepts conveyed?

Context
This answers the why, where, and who. Why was the product created, and what is
the problem it solves? Where can it be found (API, database, table, lineage)? Who
is responsible for governance, service-level agreements (SLAs), roles-based access
controls (RBAC), and who created it?

These four facets help transform mere data into a meaningful data product. Just as
the coffee beans alone wouldn’t result in the same experience without their carefully
designed packaging, data alone is not as impactful without the proper structure,
meaning, and context.
From a data product perspective, if you have data but not the other facets in one
contained object, you have an incomplete and poor quality data product. Figure 4-2
illustrates this further as a comparison to a coffee bean product.

The Four Facets of a Data Product | 61



Figure 4-2. A data product has four main facets: data, structure, meaning, and context.
This completely encapsulates the key elements a data user needs in order to be effective
at working with data into a single object (product).

A data product approach also makes searching for data and searching in data more
efficient because you are standardizing how information about the data is organized,
which can save a tremendous amount of time.

For more information on the difference between searching in data
and searching for data, take a look at The Enterprise Data Catalog,
by Ole Olesen-Bagneux (O’Reilly, 2023):

• Searching for data. It is critical that employees can easily and
quickly find data.

• Searching in data. It is equally vital that employees can under‐
stand what concepts in the data mean, such as column names.
Remove ambiguity.

A unifying data strategy means reducing waste and inefficiency. Having the data,
structure, meaning, and context located in separate systems and places—or lacking
entirely—adds incredible amounts of complexity and chaos into your data manage‐
ment practices.
Bring the dataset, the metadata, the semantic management, the governance, and so on
into a single, complete unit: the data product. Doing so will produce a far better data
experience, one that is ultimately more effective and efficient from an operational
business perspective because your data users won’t be wasting time searching or using
multiple systems to manually combine them together. And this is what a good data
product design will do for your organization.

62 | Chapter 4: Concept-First Design for Data Products



Getting Started with Concept-First Design
One of the authors of this book, Ron, has had several experiences with companies
that were groundbreaking for him as a consultant. To illustrate these experiences,
consider the following real-world examples (which have been merged and altered to
keep the companies’ identities private).
Imagine working with Company A that makes a billion dollars in revenue a year from
a single flagship product, a company that has been historically and reliably dominant
in its category across the entire global market. Its fiercest competitor, Company B,
has just launched an AI version of its similar product, putting immense pressure
on Company A’s C-level executives to show their board that the company is staying
innovative and has a response to the new threat. Company A is currently leading
market share, but isn’t particularly known for being innovative, and if an AI solution
is not offered soon, there is a very real possibility that customers could switch to
Company B, which could have dire negative consequences for Company A.
To respond to the threat, a team of top-tier, world-class data scientists and machine
learning engineers are hired. They are paid fantastical salaries—money is no object
and expenses are not spared to hire the team that the C-level executives can point to
as “the best in the world.”
However, after several years, the team is still unable to create an AI version of the
product. Deadlines keep getting pushed back, and teams are blaming each other.
So, the C-level executives do what most executives do: hire expensive consultants to
solve the problem for them, believing that their teams internally couldn’t do it. The
expensive consulting firm proceeds to charge tens of millions in consulting fees to
bring in entire teams of expensive experts and the most powerful cloud computing
services.
Almost a hundred people are either hired or pulled off other teams, so the renewed
effort succeeds, and the building of the AI product begins! But then one year later, the
AI product is still terrible, users hate the experience, the AI doesn’t work very well,
and now the C-level executives have to tell their board that they’ve wasted years and
many tens of millions of dollars with absolutely nothing to show for it.
This is the point in the story where Ron was brought in to see what he could find,
and the result was that in under a year, the AI engine was built with a fraction of the
team, time, and cost of the previous effort. The technique that was used and refined
over many problems, across many industries and years, eventually came to be called
concept-first design.
Our emphasis in concept-first design is on the word first. It underscores the
importance of not rushing into development, design, or business decisions until

Getting Started with Concept-First Design | 63



all concepts are meticulously designed and agreed upon by all stakeholders. For
more technical teams, you could describe this process as schema-first design, although
concept-first design is a far more approachable term for nontechnical people.

A Blueprint for Unifying
To help leaders grasp the transformative power of unification within an organization,
let’s break down the journey into three pivotal phases: assess, align, and accelerate.
Each phase serves as a building block, and together they offer a structured pathway to
drive change effectively, as illustrated in Figure 4-3:

Figure 4-3. The three phases of unifying can be summarized using this flowchart to
explain the activities of unifying in a high-level summary. Whether you are an internal
or external data champion, giving a clear description of what transformation will look
like can help gather support.

1. Assess. The first step is a thorough evaluation of your organization’s current state.
Meet with key stakeholders to listen, gather insights, and assess different view‐
points. The assess phase begins with concept-first design to translate stakeholder
perspectives into organized, hierarchical concepts. This not only aids in data
collection but also in establishing a common way to capture perspectives and
language across departments. With the help of a well-designed spreadsheet or
whiteboard, this phase enables everyone to better understand existing alignments
or disparities.

2. Align. Armed with a comprehensive assessment, the next step is to bring the
disparate perspectives into alignment. Chapter 6 introduces the concept compass,
a tool designed to precisely illuminate variances among stakeholder perspectives.
However, this also requires a tool to connect all the threads across organizational
networks that use those concepts, and data champions will utilize the CLEAN
data governance framework, covered in Chapter 7, to follow a methodical and

64 | Chapter 4: Concept-First Design for Data Products



structured approach. This phase ensures that everyone is on the same page,
making the organization ready for targeted action.

3. Accelerate. Once concepts are defined, aligned, and connected with proper data
hygiene, they are ready to be described in the context of success measures and
metrics that gauge progress. The primary tools for the accelerate phase are
annotated process maps, covered in Chapter 9, and success spectrums and UX
design strategies in Chapter 10. The accelerate phase tools enable teams across
various domains—business, data, code, design, and more—to align with a clear
execution road map and measurement criteria.

Crafting impeccable data products necessitates a rigorous commitment to the
principle of data hygiene, as detailed in “What You Can’t See Can Kill You,
and the Same Is True for Data” on page xii. The systematic progression through the
assess, align, and accelerate phases serves to eliminate subtle threats such as ambigu‐
ity and knowledge disparities, laying the foundation for enhanced collaboration, pre‐
cise decision making, and optimized productivity. When we adhere to data hygiene
and concurrently maintain overarching principles of alignment across processes and
organizational networks, we pave the way for breakthroughs in innovation, data
science initiatives, and operational streamlining.
Echoing the insights of Dr. Semmelweis on the indispensability of hand hygiene in
healthcare (see “What You Can’t See Can Kill You, and the Same Is True for Data” on
page xii), the value of data hygiene becomes crystal clear. Much like the foundational
role of handwashing in medicine, impeccable data hygiene stands as the bedrock,
ensuring organizations not only remain protected but also thrive. Ignoring data
hygiene can result in disastrous consequences.
Assess, align, and accelerate isn’t just a mantra; it’s our map.

Mapping the Conceptual Terrain: Assessing Concepts
The assess phase is a structured exploration of the collective mind of your team. It’s
a series of one-on-one interviews with key stakeholders and contributors across the
board. This includes developers, designers, product managers, data scientists, and
more. This phase is about asking the people you interview to share their perspective
about the work they do:

1. Describe the important problems to solve. Ask people to describe what problems
they are working on, toward which objectives, what actions they take, and how
success is measured by the outcomes. Their descriptions can be as a visual on a
whiteboard or in a document such as PowerPoint, Excel, or Google Docs.

2. Clarify the important concepts they contain. With the map of the most important
problems, objectives, actions, and success metrics in hand, explore these with

Mapping the Conceptual Terrain: Assessing Concepts | 65



your interviewee. Simply capture the words and their meanings in plain language,
and if there are any calculations (such as profit or customer engagement), ask
them to specify exactly how they are measured. This is best done in a spread‐
sheet, an example of which is shown in Figure 4-4.

3. Arrange the concepts into hierarchical relationships. Once you have some mean‐
ings, ask people to break down the concepts into more granular components in
a hierarchical structure of parent-to-child or child-to-sibling relationship. This is
the same format a schema uses. This is also best done in the spreadsheet format
shown in Figure 4-4.

Figure 4-4. Using a spreadsheet for concept-first design. Three columns are used for any
concept: Level #, Type (object, number, text, list, and so on), and Definition. These are
what are used to create meaning for the concepts. When you want to add a subconcept
(also known as a child concept), you add those child concepts in the column representing
the next level. For example, for the Product concept (level 1), the Title child concept
is added at level 2. This layering of levels of concepts becomes a hierarchical structure,
which is another way to say a schema.

Your assessment will be as follows. Compile all of the stakeholder and contributor
perspectives and show how aligned or unaligned they are. You now have a quantita‐
tive way to provide feedback to leaders and the team, showing that, for example,
out of five people, only two agreed exactly on what certain concepts mean and how
they are structured. How can any team work effectively together if they aren’t unified
around the concepts that data and code are supposed to capture? They usually can’t.
And if you ask a designer, data engineer, marketer, business leader, and software
engineer to come up with the meaning and structure of concepts, they will all have
their own particular perspectives.

The fundamental step in concept-first design is always to put
understanding the constellation of concepts first before making deci‐
sions to invest (business), analyze (data), and build (code).

66 | Chapter 4: Concept-First Design for Data Products



Figure 4-4 showcases a spreadsheet that acts as a schema. It visually represents
the hierarchy of concepts in a product, illustrating the arrangement levels and the
expected entities at each level. This approach brings schema design to everyone, not
just to developers. It offers a unique and accessible way to craft concept structures
similar to how they would be created in JSON.
The organization of concepts in this CSV format diverges from a traditional schema.
A traditional schema primarily focuses on the hierarchical structure of concepts,
whereas this CSV-based schema integrates both structure and meaning. Therefore, it
is not just a spreadsheet; it’s a comprehensive representation that includes semantic
information alongside the conceptual hierarchy. This integrated approach helps pre‐
vent and reduce ambiguity, thereby enhancing efficiency in data management and
system development.

Facilitating Assessments of Conceptual Alignment Across
Technical and Nontechnical Teams
Figure 4-5 shows a real-life example, or an instance, of a coffee product, constructed
using the blueprint provided by the schema in Figure 4-4. Essentially, Figure 4-4
outlines the expected structure of a product object, and Figure 4-5 demonstrates how
to populate that structure with specific data about an actual product.
The real power of conceptual schemas, as demonstrated in Figure 4-5, lies in their
accessibility and the simplicity they bring to expressing business logic. By using a
straightforward spreadsheet, business logic can be seamlessly and directly translated
into JSON objects. This approach greatly enhances efficiency and eliminates a signif‐
icant pain point—the often complex and error-prone task of translating business
stakeholder language into functional technical requirements.
Developers frequently grapple with the challenge of translating business requirements
into functional code. Business leaders often articulate their needs based on their per‐
spective, which is rooted in business strategy, customer demands, or market trends.
However, these needs are not always communicated in terms that developers can
easily translate into a technical design. This communication gap is not because of a
lack of effort on either side, but rather because business and development are two
distinct domains with their own jargon, perspectives, and priorities.
The challenge lies in the fact that developers are not mind readers. They can’t intuit
the intricate nuances of a business requirement unless it’s spelled out explicitly. When
expectations are not met, it often leads to frustration and blame, with developers
bearing the brunt for not understanding the “obvious.” This situation results in was‐
ted time, resources, and strained relationships between teams. Moreover, it creates an
environment of constant back-and-forth revisions, delayed projects, and subpar end
products.

Facilitating Assessments of Conceptual Alignment Across Technical and Nontechnical Teams | 67



Figure 4-5. A conceptual schema. The concept-first design schema from Figure 4-4 can
be mapped one to one to a JSON representation. This mapping makes it easier for
technical and nontechnical teams to understand where they are aligned or misaligned in
how to model and communicate concepts used across teams and technology systems and
to integrate into functional business processes.

Conceptual schemas can help bridge this communication gap. They provide a com‐
mon language for both business leaders and developers to articulate and understand
requirements. By organizing concepts in a structured, hierarchical manner that
includes both meaning and structure, conceptual schemas offer a clear road map
for developers. They enable developers to see what business leaders envision, in a
format that can be directly translated into code. This approach reduces ambiguity,
ensures alignment of expectations, and streamlines the process of turning business
logic into functional requirements, leading to more successful projects and a better
working relationship between teams.
JSON and JSON Schemas, being both human- and machine-readable languages, are
crucial elements of concept-first design. During the assess phase of unifying, it’s
essential to include both technical and nontechnical teams in your evaluation. The
aim is to assess the organization’s overall alignment concerning data vertically and
horizontally across various domains.

68 | Chapter 4: Concept-First Design for Data Products



If the technical aspects seem daunting, don’t be concerned. You
don’t have to scrutinize the code personally. Instead, collaborate
closely with a developer who can examine the data structures in
the code and provide insight into the alignment or misalignment
between technical and nontechnical teams.

If the technical team requires more data than what the business team has defined in
their representation, that’s perfectly fine. There might be a legitimate technical need
for additional information. Nevertheless, it’s crucial for the business, data, and code
teams to reach a consensus on the fundamental concept representations, aligning
them with the first step of our assessment: define the critical problems to solve and
their context, including objectives, actions, and success metrics.
The assess phase isn’t solely about measuring conceptual differences among team
members or between code representations of concepts. It also involves gauging the
disparities between concepts and the defined problems, objectives, actions, and suc‐
cess metrics.
In the consulting adventure outlined earlier, Ron discovered that a single word—
just one—that was being used in the success metrics had different interpretations
across teams. For the business team, the word adaptive signified that success was
tied to their current product’s ability to adapt the user experience to their current
business model, which was, coincidentally, tied to the KPIs that measured their
success, bonuses, and so on. In contrast, the data science team evaluated the success
of being adaptive based on how accurately their predictive models could adapt the
user experience—not optimized for the business team’s KPIs, but for what was best
for the user experience.
You can apply concept-first design to literally anything that can be conceptualized,
including success metrics. Once you have made your assessments, showing the gaps
of alignment, you can begin working on aligning teams to a shared concept-first
design schema, which we will explore later in the book. Chapter 5 explains how a data
product manager or data champion should become more familiar with the technical
aspects of implementing and using schemas with JSON Schema, which will enable
technical teams to begin designing schemas for technical purposes.

Smooth Is Slow, Slow Is Fast
The phrase “go slow to go smooth, go smooth to go fast” emphasizes the importance
of taking the time to learn how to do something correctly before focusing on speed.
This approach applies equally to learning a musical instrument and learning how to
align innovation efforts.
Assessing concepts first, as shown in Figure 4-4, is a simple way to ask basic questions
of stakeholders, learn about their problems, and find where they differ in opinions

Smooth Is Slow, Slow Is Fast | 69



from other stakeholders. This assessment is critically important, even if it feels like it
slows down your team significantly. Some of the questions worth asking may seem
silly, obvious, or even insulting. However, over and over again in consulting, we have
found that these obvious questions have never been asked before, and the result is
often very costly mistakes related to data.

Unifying is a continual practice. If misalignment creeps up, it
is important to begin the process again and look back at your
concept-first schemas to identify where misunderstandings may
have arisen.

Here are some of the benefits you will gain from unifying:

• In environments where alignment is prioritized and is thriving, teams collabo‐
rate far more seamlessly and are able to exchange insights, knowledge, and
resources. This collaboration leads to a culture of continuous learning and
improvement, which will empower your organization to swiftly adapt and
respond to challenges.

• The capacity to adapt and respond to change is crucial for any business to suc‐
ceed. Alignment means better understanding of how to operate in more nimble
and adaptive ways, as teams are better equipped to comprehend the implications
of new insights, trends, and shifting requirements.

• An unfortunate result of the bottlenecks around centralized data management
is that some teams become so desperate for solutions that they go around the
centralized data management team and build or contract out their own data
solutions to make forward progress. However, failing to involve the data team
in decisions about data architecture or storage can lead to data quality issues,
security breaches, and technical debt. Making sure everyone is aligned enables
teams that are facing bottlenecks to move forward with far less risk because they
know exactly what concepts to align and how to align them.

Summary
This chapter discussed the significance of organizations’ internal data products, draw‐
ing comparisons with commercial, customer-facing products. It emphasized the need
for good user experience in data products and underlined the role of a data product
manager in creating excellent data product experiences.
The four facets of a data product are data, structure, meaning, and context. This
chapter illustrated these facets using a coffee bean analogy, asserting that a data
product without any of these facets is incomplete and of inferior quality.

70 | Chapter 4: Concept-First Design for Data Products



We saw how a data product approach makes data search more efficient by standardiz‐
ing how information about the data is organized. The difference between searching in
data and searching for data was discussed.
We learned that being Agile, in the context of unifying, advocates for reducing
waste and inefficiency by bringing the dataset, metadata, semantic management, and
governance into a single, complete unit: a data product. This approach enhances
the data experience by streamlining data-related processes and proves to be more
effective and efficient compared to traditional methods of becoming a data-driven
organization.
We described the three steps to unifying: assess, align, accelerate. The assess phase
involves a structured exploration of the collective mind of the team to understand the
gaps and severity of misalignment across teams. The align phase focuses on dispelling
communication illusions and achieving effective alignment across all levels of the
organization. The accelerate phase emphasizes iterating quickly to get feedback and
implement and test solutions in an Agile manner.
Finally, we reviewed the risks of accelerating without satisfactorily unifying—doing
so might lead to building the wrong thing faster. Unifying prioritizes accuracy before
focusing on speed and emphasizes continually aligning data team efforts with the
organization’s overall objectives.
Going between high and low levels of abstraction in your quest for unifying is the
strategy you will continue to learn throughout this book. In Chapter 5, you, the data
champion, will zoom in from thinking in a high-level strategic way about unifying
concepts to a lower-level way of thinking about unifying concepts at a code level
using JSON Schema.

Summary | 71



CHAPTER 5
A Universal Language for Data

The limits of my language mean the limits of my world.
—Ludwig Wittgenstein

Humans require fluency in a common language in order to communicate complex
ideas. In the same way, breaking the data silos of an organization requires every party
to use a common language to talk about the structure, semantics, and governance of
data. As we saw in Chapter 4, a data definition that describes these concepts in an
unambiguous way is called a schema, and it is an essential part of any data-driven
strategy.
While there is no shortage of options out there, we choose JSON Schema as our
standard data language of choice. In this chapter, you will learn the basics of JSON
Schema and how to read and understand any JSON Schema. This chapter purposely
takes a modern top-down, vocabulary first approach to teaching you how to think
about JSON Schema.
Over the course of this book, you will gain expertise on JSON Schema and be able to
use it as the cornerstone of your data-centric innovation initiatives. We will exemplify
how schemas can provide immediate value by using them to render basic web forms
for our fictitious Intelligence.AI online coffee company.

This chapter builds on Chapter 2 and is aimed at the more techni‐
cal audience interested in implementing data products. To continue
learning about data alignment in a business sense, you can move to
Chapter 6.

73



What Is JSON Schema?
JSON Schema is a set of specifications that introduce a declarative language to define
the structure and meaning of JSON documents, referred to as instances. This lan‐
guage is a general-purpose tool to talk about data across a wide variety of use cases
from generative property testing to UI generation, data semantics, databases, and
more.
Due to the popularity of JSON as a universal data format, JSON Schema became the
industry standard for validating and describing data. Users of JSON Schema range
from Amazon, Microsoft, and Google, to institutions such as NASA, the U.S. Depart‐
ment of Commerce, the National Security Agency (NSA), and the UK government.
JSON Schema is also a key component of specifications like OpenAPI, AsyncAPI,
RAML, and W3C Web of Things.
The authors of this book have successfully deployed JSON Schema in the context of
documentation generation, UI generation, fuzzing, linting, analyzing API complexity,
code generation, data transformation pipelines, databases, data compression, and
more.

What Is a Schema?
A schema is a formal definition of a data structure that carries information about its
syntax (what the data structure looks like) and its semantics (what the data means).
As its name implies, JSON Schema is a standard schema language for JSON-based
data structures. A JSON document that is a valid JSON Schema is called a schema.
These schemas utilize a wide range of keywords to add meaning to the JSON instan‐
ces they are meant to describe.
Here is an example JSON Schema that describes object instances that define a name
string property and an age positive integer property:
{
  "$id": "https://example.com/my-schema",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "required": [ "name", "age" ],
  "properties": {
    "name": { "type": "string" },
    "age": { "type": "integer", "minimum": 0 }
  }
}

Here is an example JSON instance that is valid according to the preceding schema:
{ "name": "Tony Hoare", "age": 89 }

74 | Chapter 5: A Universal Language for Data



Don’t worry if you don’t fully understand that JSON Schema. We will go through the
basics of JSON Schema in the remainder of this chapter.

Schemas reduce errors and improve communication. Once your
organization starts defining and using schemas, any data you share
or obtain from others can be validated to make sure it is compliant
with expected formats. In other words, having schemas creates an
enforceable frame of reference to protect your organization from
inconsistent data, reducing errors and enabling efficient communi‐
cation as a result.

A Brief History of JSON Schema
JSON Schema exploded in popularity given its use in API specifications such as
Swagger and OpenAPI, but it is almost as old as JSON itself. Inspired by various
schema languages for XML, JSON Schema was conceived at the Ajax World confer‐
ence in 2007—only a year after the JSON specification was published—to solve
the problem of validating JSON documents in a declarative and language-agnostic
manner.
Since then, 10 revisions of the JSON Schema specification have been published. The
JSON Schema language evolved beyond the validation problem to become a general
and extensible foundational framework for talking about data, supporting a notably
wide range of use cases across industries.

The Building Blocks of JSON Schema
A programming language is defined in terms of its grammar and standard library.
Similarly, the JSON Schema language is defined on top of a small set of foundational
concepts: dialects, vocabularies, and meta-schemas.

Vocabularies and Dialects
The JSON Schema specifications define a rich set of keywords for different use cases.
The JSON Schema keywords we used in the previous example are $id, $schema,
type, required, properties, and minimum. As the number of keywords defined in
the specifications grew, they were split into groups of interrelated keywords called
vocabularies. In JSON Schema parlance, a collection of vocabularies is referred to as a
dialect.
So far, every revision of the JSON Schema specification comes with an official dialect.
The latest JSON Schema dialect at the time of this writing, which we cover in this
book, is codenamed 2020-12.

The Building Blocks of JSON Schema | 75



In Chapter 13, you will learn how to extend JSON Schema with your own
vocabularies.

Meta-Schemas: Schemas That Describe Other Schemas
JSON Schema is so expressive that it is capable of describing itself, and this is by
design. The idea of a JSON Schema that describes and validates itself or another
JSON Schema is called a meta-schema. Meta-schemas play a key role in the versioning
strategy of JSON Schema: every release of JSON Schema comes with a new official
meta-schema.
The $schema keyword allows schema authors to declare a Uniform Resource Identi‐
fier (URI) to the meta-schema that describes the given schema. In other words, a
JSON Schema says, “I am described by this other schema,” and as you might guess, a
schema that doesn’t respect its meta-schema is not a valid JSON Schema.
This is an example of a JSON Schema that sets its meta-schema to https://json-
schema.org/draft/2020-12/schema:
{
  …
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  …
}

Declaring the $schema keyword is the most common way for JSON Schema imple‐
mentations to know which dialect of JSON Schema a given schema is referring to. We
know https://json-schema.org/draft/2020-12/schema is the official meta-schema of the
2020-12 dialect, so the preceding schema is essentially saying, “I am described by the
schema that models 2020-12 schemas.”

Understanding JSON Schemas
Reading a JSON Schema is no different from reading a piece of code. To know what
the code means, you have to step through it, checking the external dependencies it
consumes and the functions it invokes. Similarly, to understand a JSON Schema, you
must determine the dialect in use, the vocabularies being used, and the keywords it
declares (see Figure 5-1).

76 | Chapter 5: A Universal Language for Data



Figure 5-1. Understanding a JSON Schema involves knowing its dialect, vocabularies,
and keywords.

Let’s analyze the JSON Schema given earlier in baby steps:
{
  "$id": "https://example.com/my-schema",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "properties": {
    "name": { "type": "string" },
    "age": { "type": "integer", "minimum": 0 }
  }
}

It might feel a bit slow at the beginning, but by the end of this section, you will not
only understand the schema you’ve seen before, but you will be able to methodically
understand any JSON Schema out there.

Understanding JSON Schemas | 77



Step 1: Determining the Schema Dialect: The $schema Keyword
The first step to understand a JSON Schema is to determine the dialect in use—in
other words, the vocabularies that the schema is using. Without knowing its dialect,
the keywords declared by a JSON Schema are literally meaningless; a JSON Schema
implementation will silently ignore unrecognized keywords.
The vast majority of JSON Schemas use the official dialect meta-schemas, so identify‐
ing the dialect is as simple as looking up the value of the $schema keyword:
{
  "$id": "https://example.com/my-schema",
  "$schema": "https://json-schema.org/draft/2020-12/schema", 
  "type": "object",
  "required": [ "name" ],
  "properties": {
    "name": { "type": "string" },
    "age": { "type": "integer", "minimum": 0 }
  }
}

 Armed with this knowledge, we can tell that our JSON Schema sets the $schema
keyword to https://json-schema.org/draft/2020-12/schema. This is the URI of the offi‐
cial meta-schema for the 2020-12 dialect, so we know that our schema uses the JSON
Schema 2020-12 dialect.
You can check the vocabularies imported by a dialect by looking at the $vocabulary
keyword defined in its meta-schema. For example, the JSON Schema 2020-12 dialect
hosted at https://json-schema.org/draft/2020-12/schema makes use of 7 official vocab‐
ularies:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://json-schema.org/draft/2020-12/schema",
  "$vocabulary": {
    "https://json-schema.org/draft/2020-12/vocab/core": true,
    "https://json-schema.org/draft/2020-12/vocab/applicator": true,
    "https://json-schema.org/draft/2020-12/vocab/unevaluated": true,
    "https://json-schema.org/draft/2020-12/vocab/validation": true,
    "https://json-schema.org/draft/2020-12/vocab/meta-data": true,
    "https://json-schema.org/draft/2020-12/vocab/format-annotation": true,
    "https://json-schema.org/draft/2020-12/vocab/content": true
  },
  ...
}

78 | Chapter 5: A Universal Language for Data



The attentive reader might have noticed that the official meta-
schemas are recursively defined; they declare the $schema keyword
to point to themselves. In fact, JSON Schema is itself a recursive
data structure. A relatively uncommon case that we won’t cover in
this chapter is how to determine the base dialect (specification revi‐
sion) of a JSON Schema that uses a custom meta-schema. In these
cases, you traverse the chain of meta-schemas by looking at every
$schema keyword until you find a meta-schema that describes
itself. That self-descriptive meta-schema URI corresponds to the
JSON Schema version in use.

Step 2: Determining the Schema Vocabularies
At this point, we know that our JSON Schema uses the 2020-12 dialect. However, we
still don’t know which vocabularies it uses. To determine this, we need to fetch the
meta-schema through its URI and take a look at its $vocabulary keyword.

Inspecting meta-schemas
We know that the meta-schema URI of our JSON Schema is https://json-schema.org/
draft/2020-12/schema. You can either download the schema using your favorite web
browser or open it directly through its URI using any of the JSON visualizers we
discussed in Chapter 2. Let’s use the “New from URL” feature of OK JSON (see
Figure 5-2).

Figure 5-2. Inspecting the 2020-12 official meta-schema using OK JSON. The key to
determining the vocabularies in use is the $vocabulary keyword.

Understanding JSON Schemas | 79



As we saw, a meta-schema is identified by a URI. However, these
are merely identifiers. JSON Schema does not guarantee that you
will get back the corresponding schema if you request such a URI.
That said, for convenience, this tends to be the case more often
than not.

Every JSON Schema vocabulary is identified through a URI. For example, the URI
of the core vocabulary for the 2020-12 dialect is https://json-schema.org/draft/2020-12/
vocab/core.

The $vocabulary keyword
The $vocabulary keyword is a JSON object whose keys are vocabulary URIs and
whose values are Booleans that determine whether the corresponding vocabulary is
considered to be required (true) or optional (false). A JSON Schema implementa‐
tion will abort processing if it encounters a required vocabulary that it doesn’t know
about and understand.

Optional Vocabularies
Optional vocabularies are not common in practice, so don’t worry much about them.
All the examples in this book and official JSON Schema dialects out there mark
vocabularies as required.
But if you are curious, when a vocabulary is marked as optional and the JSON
Schema implementation does not know about it, evaluation will continue. However,
its keywords will have no validation effect. We will explore what happens to unknown
keywords in Chapter 13.

In this case, we know that our JSON Schema imports the following vocabularies,
marking them all as required:

• https://json-schema.org/draft/2020-12/vocab/core
• https://json-schema.org/draft/2020-12/vocab/applicator
• https://json-schema.org/draft/2020-12/vocab/unevaluated
• https://json-schema.org/draft/2020-12/vocab/validation
• https://json-schema.org/draft/2020-12/vocab/meta-data
• https://json-schema.org/draft/2020-12/vocab/format-annotation
• https://json-schema.org/draft/2020-12/vocab/content

80 | Chapter 5: A Universal Language for Data



Step 3: Understanding Schema Vocabularies
So we know which vocabularies are in use for our JSON Schema, but how do we
connect this knowledge to specific keywords?

Using the reference documentation
For official vocabularies (hosted at https://json-schema.org), you can consult Learn
JSON Schema, a website that presents concise reference documentation for JSON
Schema dialects and vocabularies. The documentation for the 2020-12 dialect
presents all official vocabularies and the keywords each one defines. You can further
browse specific vocabularies, like Unevaluated, to see the list of keywords introduced
by that vocabulary. See Figure 5-3 for an example.

Figure 5-3. The list of keywords introduced by the Unevaluated vocabulary.

The attentive reader might recognize that some of the keywords we’ve been discus‐
sing so far, like $schema and $vocabulary, are part of the Core vocabulary. This
vocabulary is always required in every JSON Schema and declares the foundational
keywords that make the dialect and vocabulary system work.

Keyword namespacing (or lack thereof)
JSON Schema doesn’t protect schema authors from having two vocabularies defining
the same keyword name. If vocabularies Foo and Bar both declare a keyword called
test, then these vocabularies are incompatible with each other and cannot be used
together. There is no proper namespacing mechanism for referring to the test key‐
word of a specific vocabulary other than simple naming conventions, such as every
keyword from the Core vocabulary starting with the dollar sign $ character. This fact
often makes it tricky to determine which vocabulary a specific keyword belongs to.
For example, our JSON Schema makes use of the type keyword, but it is not obvious
to which vocabulary this keyword belongs. A JSON Schema implementation that
supports a set of vocabularies will intrinsically know which keywords are provided by
such vocabularies.
If you are curious about which vocabulary defines a given keyword, we recommend
searching for keywords on Learn JSON Schema. We can tell from a quick search that
the type keyword is defined by the Validation vocabulary.

Understanding JSON Schemas | 81



Step 4: Understanding Schema Keywords
Now that we know how to determine the vocabulary that defines specific keywords,
let’s attempt to understand them one by one. We recommend consulting the docu‐
mentation about every keyword you encounter.

A first pass on top-level keywords
First, we’ll take all the keywords used at the top level of our JSON Schema and group
them by vocabularies. We’ll ignore nested keywords for a little bit:
{
  "$id": "https://example.com/my-schema", 
  "$schema": "https://json-schema.org/draft/2020-12/schema", 
  "type": "object", 
  "required": [ "name" ], 
  "properties": { 
    "name": { "type": "string" },
    "age": { "type": "integer", "minimum": 0 }
  }
}

These top-level keywords are:
  $id and $schema from the Core vocabulary
  type and required from the Validation vocabulary
 properties from the Applicator vocabulary

As its name implies, the Core vocabulary introduces foundational keywords that are
essential for processing any JSON Schema. See Table 5-1 for the complete list of
keywords defined in this vocabulary.

Table 5-1. The complete list of keywords introduced by the Core vocabulary. The Instance
column signifies the type of JSON instances in which the corresponding keywords take effect.
We will discuss in more detail how JSON Schema keywords behave on different instance
types in Chapter 8.

Keyword Instance Summary
$schema Any Used as both a JSON Schema dialect identifier and as a reference to a JSON Schema that

describes the set of valid schemas written for this particular dialect
$id Any Declares an identifier for the schema resource
$ref Any Used to reference a statically identified schema
$defs Any Used as a container for common shared definitions reused throughout the schema
$comment Any Reserves a location for comments from schema authors to readers or maintainers of the

schema
$dynamicAnchor Any Used to create plain name fragments that are not tied to any particular structural location

for referencing purposes, which are taken into consideration for dynamic referencing

82 | Chapter 5: A Universal Language for Data



Keyword Instance Summary
$dynamicRef Any Used to reference an identified schema, deferring the full resolution until runtime, at

which point it is resolved each time it is encountered while evaluating an instance
$anchor Any Used to create plain name fragments that are not tied to any particular structural location

for referencing purposes, which are taken into consideration for static referencing
$vocabulary Any Used in meta-schemas to identify the required and optional vocabularies available for use

in schemas described by that meta-schema

Our JSON Schema makes use of two keywords from this vocabulary: $id and
$schema. The $id keyword is used to declare a URI that uniquely identifies the
schema. If the identifier is a Uniform Resource Locator (URL), it is common practice,
though not required, for the identifier to resolve to the schema. In other words, a
user is often able to obtain the schema by visiting its identifier on a web browser. You
can try this out with any of the official schemas, such as https://json-schema.org/draft/
2020-12/schema.
As we’ve seen before, the $schema keyword defines the meta-schema that describes
the given schema.

The validation vocabulary
This vocabulary introduces a rich set of keywords for asserting the structure and
contents of JSON instances. These keywords constrain the possible JSON instances
that successfully validate against the schema. See Table 5-2 for a complete list of
keywords defined in this vocabulary.

Table 5-2. The complete list of keywords introduced by the Validation vocabulary. The
Instance column signifies the type of JSON instances on which the corresponding keywords
take effect. We will discuss in more detail how JSON Schema keywords behave on different
instance types in Chapter 8.

Keyword Instance Summary
type Any Validation succeeds if the type of the instance matches at least one of the given types.
enum Any Validation succeeds if the instance is equal to one of the elements in this keyword’s

array value.
const Any Validation succeeds if the instance is equal to this keyword’s value.
minLength String A string instance is valid against this keyword if its length is greater than or equal to

the value of this keyword.
maxLength String A string instance is valid against this keyword if its length is less than or equal to the

value of this keyword.
pattern String A string instance is considered valid if the regular expression matches the instance

successfully.
maximum Number Validation succeeds if the numeric instance is less than or equal to the given number.
exclusiveMinimum Number Validation succeeds if the numeric instance is greater than the given number.

Understanding JSON Schemas | 83



Keyword Instance Summary
multipleOf Number A numeric instance is valid only if division by this keyword’s value results in an

integer.
exclusiveMaximum Number Validation succeeds if the numeric instance is less than the given number.
minimum Number Validation succeeds if the numeric instance is greater than or equal to the given

number.
dependentRequired Object Validation succeeds if, for each name that appears in both the instance and as a name

within this keyword’s value, every item in the corresponding array is also the name of
a property in the instance.

minProperties Object An object instance is valid if its number of properties is greater than or equal to the
value of this keyword.

maxProperties Object An object instance is valid if its number of properties is less than or equal to the value
of this keyword.

required Object An object instance is valid against this keyword if every item in the array is the name
of a property in the instance.

minItems Array An array instance is valid if its size is greater than or equal to the value of this
keyword.

maxItems Array An array instance is valid if its size is less than or equal to the value of this keyword.
minContains Array The number of times that the contains keyword (if set) successfully validates

against the instance must be greater than or equal to the given integer.
maxContains Array The number of times that the contains keyword (if set) successfully validates

against the instance must be less than or equal to the given integer.
uniqueItems Array If this keyword is set to the Boolean value true, the instance validates successfully if

all of its elements are unique.

Our JSON Schema makes use of two keywords from this vocabulary: type and
required. In this case, we set type to object to assert that this schema describes
JSON objects, and we set required to an array that contains name to assert that a
property called name must be defined in the instance. The type keyword can be set to
a diverse set of types such as integer, array, string, and more.
After declaring these keywords, JSON documents that are either not JSON objects
or do not declare a name property will be considered to be invalid according to our
schema.

The applicator vocabulary
This vocabulary introduces a set of keywords that combine other nested JSON Sche‐
mas in a variety of ways. Applicators are the key feature behind the expressiveness
of the language and are considered to be the bread and butter of JSON Schema. See
Table 5-3 for a complete list of keywords defined in this vocabulary.

Table 5-3. The complete list of keywords introduced by the Applicator vocabulary. The
Instance column signifies the type of JSON instances on which the corresponding keywords

84 | Chapter 5: A Universal Language for Data



take effect. We will discuss in more detail how JSON Schema keywords behave on different
instance types in Chapter 8.

Keyword Instance Summary
oneOf Any An instance validates successfully against this keyword if it validates successfully

against exactly one schema defined by this keyword’s value.
allOf Any An instance validates successfully against this keyword if it validates successfully

against all schemas defined by this keyword’s value.
anyOf Any An instance validates successfully against this keyword if it validates successfully

against at least one schema defined by this keyword’s value.
if Any This keyword declares a condition based on the validation result of the given

schema.
then Any When if is present, and the instance successfully validates against its

subschema, then validation succeeds against this keyword if the instance also
successfully validates against this keyword’s subschema.

else Any When if is present, and the instance fails to validate against its subschema,
then validation succeeds against this keyword if the instance successfully
validates against this keyword’s subschema.

not Any An instance is valid against this keyword if it fails to validate successfully against
the schema defined by this keyword.

properties Object Validation succeeds if, for each name that appears in both the instance and as a
name within this keyword’s value, the child instance for that name successfully
validates against the corresponding schema.

patternProperties Object Validation succeeds if, for each instance name that matches any regular
expressions that appear as a property name in this keyword’s value, the
child instance for that name successfully validates against each schema that
corresponds to a matching regular expression.

additionalProperties Object Validation succeeds if the schema validates against each value not matched by
other object applicators in this vocabulary.

dependentSchemas Object This keyword specifies subschemas that are evaluated if the instance is an object
and contains a certain property.

propertyNames Object Validation succeeds if the schema validates against every property name in the
instance.

prefixItems Array Validation succeeds if each element of the instance validates against the schema
at the same position, if any.

contains Array Validation succeeds if the instance contains an element that validates against this
schema.

items Array Validation succeeds if each element of the instance not covered by prefixItems
validates against this schema.

Our JSON Schema makes use of a single keyword from this vocabulary: properties.
This keyword lets the schema writer declare a JSON Schema for each property in an
object instance.
In this case, we associate the name property with a schema that declares that the
name must be a string (through the use of the type keyword from the Validation

Understanding JSON Schemas | 85



vocabulary), and we associate the age property with a schema that declares that the
age must be an integer (through the use of the type keyword from the Validation
vocabulary) and its inclusive minimum value must be zero (through the use of the
minimum keyword from the Validation vocabulary).
As you can see, applicator keywords introduce the possibility of JSON Schemas
declaring child JSON Schemas. To recap, this is the reason we say that JSON Schema
is a recursive data structure: a JSON Schema may include other JSON Schemas that
might even reference back the outer schemas.

JSON Schema as a Recursive Data Structure
A JSON Schema often makes use of keywords that take one or more other JSON
Schemas as arguments, creating a tree of “subschemas.” Our JSON Schema consists of
three subschemas, as shown in Figure 5-4.

Figure 5-4. JSON Schemas typically consist of a set of subschemas composed into a larger
schema. The JSON Schema in this example consists of three subschemas: the top-level
schema and two subschemas that correspond to each defined property.

Breaking down a JSON Schema into its corresponding subschemas
is a good strategy for simplifying and understanding larger and
more complex schemas.

86 | Chapter 5: A Universal Language for Data



Referencing Schemas
As with traditional programming languages, schemas are more maintainable if they
reuse existing schemas rather than reinvent the wheel. For example, you may write a
schema that introduces the concept of a person. If other schemas also involve people,
you’d rather reuse the same person definition instead of duplicating it in every one of
your schemas.
Though the JSON Schema examples we’ve seen so far, like the one in Figure 5-5,
declare subschemas in place JSON Schema has built-in capabilities for referencing
other JSON Schemas.

What does duplication look like?
Consider a JSON Schema that describes objects whose foo, bar, and baz properties
are even integers:
{
  "$id": "https://example.com/schema-with-duplication",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "properties": {
    "foo": { "type": "integer", "multipleOf": 2 }, 
    "bar": { "type": "integer", "multipleOf": 2 }, 
    "baz": { "type": "integer", "multipleOf": 2 }  
  }
}

   These subschemas are all the same!
Though this schema does the job, it duplicates the concept of an even integer multi‐
ple times, once for each property. Although this example is contrived, real-world
schemas often duplicate complex subschemas that are tens of lines each.

Referencing Schemas | 87



Local referencing
One way of keeping the schema tidy is to move the shared definition of an even
integer to a common location through the use of the $defs keyword and then
reference it through the $ref keyword every time it’s needed.
Here is a revised version of the JSON Schema from the previous section, this time
making use of local referencing to reduce duplication:
{
  "$id": "https://example.com/local-referencing",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "properties": {
    "foo": { "$ref": "#/$defs/even-integer" },
    "bar": { "$ref": "#/$defs/even-integer" },
    "baz": { "$ref": "#/$defs/even-integer" }
  },
  "$defs": {
    "even-integer": {
      "type": "integer", "multipleOf": 2
    }
  }
}

In this example, we used the $defs keyword to define a shared entity called even-
integer. Once this new entity is defined, we can reference it from anywhere in
the schema through the $ref keyword. The curious syntax for referencing the even-
integer schema comes from JSON Pointer, a standard for expressing navigation of
JSON documents.
As shown in Figure 5-5, this JSON Pointer is read as follows: start from the
top of the document (as denoted by the # character, an artifact of JSON Pointer
being encoded as URI fragment), navigate to the $defs property (as denoted by
the /$defs fragment), and then navigate to the even-integer property (as denoted
by the /even-integer fragment).

88 | Chapter 5: A Universal Language for Data



Figure 5-5. A breakdown of evaluating the #/$defs/even-integer JSON Pointer
against the https://example.com/local-referencing schema. The # JSON Pointer
evaluates to the entity of the schema. The #/$defs JSON Pointer evaluates to the
$defs property, and #/$defs/even-integer evaluates to the subschema under the
even-integer subproperty.

Referencing Schemas | 89



Remote referencing
Local referencing helps keep schemas clean when you need to reuse definitions from
within a single schema, but what happens if multiple schemas, potentially written by
different authors, need to reuse our even integer description?
Remote referencing is about extracting shared definitions as independent standalone
schemas and referencing them by their respective URIs. Note that JSON Schema is
agnostic about how URIs are resolved. Most JSON Schema implementations provide
a way to create an in-memory registry that associates URIs with schemas.
Let’s start by extracting our even integer definition into its own schema. We will
pretend to deploy this schema at https://example.com/even-integer, so we will set
the $id keyword accordingly:
{
  "$id": "https://example.com/even-integer",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "integer",
  "multipleOf": 2
}

Assuming our even integer schema is accessible via its declared URI, we can update
the schema from the previous section to point at the definition of even integers using
this new schema rather than its local definition:
{
  "$id": "https://example.com/remote-referencing",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "properties": {
    "foo": { "$ref": "https://example.com/even-integer" },
    "bar": { "$ref": "https://example.com/even-integer" },
    "baz": { "$ref": "https://example.com/even-integer" }
  }
}

This revised version of the schema uses remote referencing to import the even
integer schema. When a JSON Schema implementation starts processing this schema,
it will attempt to locate the even integer schema from its URI (based on how the
implementation is configured to retrieve external schemas) and use its definition in
the respective locations.

90 | Chapter 5: A Universal Language for Data



Your First JSON Schema Project
Chapter 1 introduced Intelligence.AI, an online coffee company specializing in selling
premium coffee beans from around the world with humor and inspiring artwork on
the bags. Chapter 2 discussed the creation of example JSON documents for product
entries in the catalog and orders. As the online store continues to grow, we need an
easy way for the store manager to add new listings to the catalog.
Turns out we can use JSON Schema to describe what product entries look like
and use this formal definition to automatically generate pretty web forms, requiring
zero additional code. With this approach, you don’t have to bother with HTML
and JavaScript to have rich and complex forms. What’s more, your forms will be
automatically updated every time your schema changes. Isn’t that cool? Let’s get
into it!
As a reminder, this is the example product entry we modeled in Chapter 2 and
revisited in the assess phase of Chapter 4:
{
  "sku": "SHBH00001",
  "timestamp": "2023-04-07T06:32:54-04:00",
  "url": "https://intelligence.ai/products/espresso-yourself-whole-latte-love",
  "title": "Espresso Yourself - Whole Latte Love!",
  "price": 30,
  "currency": "USD",
  "description": "Introducing \"Espresso Yourself\" coffee - the ultimate bean 
potion that's here to add a splash of silliness, a dollop of delight, and a whole 
latte love to your daily grind!",
  "image_url": "https://cdn.shopify.com/s/files/1/0749/8389/9441/products/
Frame7.png"
}

Writing a Schema: Step by Step
Let’s use JSON Schema to describe product entry JSON instances like the previous
one.
The first thing we’ll do is create a JSON Schema that declares the $schema keyword to
make use of the 2020-12 dialect official meta-schema, and uses the title keyword to
give it a human-readable name:

Your First JSON Schema Project | 91



{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A Intelligence.AI product entry"
}

The JSON instances we are describing are objects, so we will set the type keyword
accordingly:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A Intelligence.AI product entry",
  "type": "object"
}

Let’s describe product titles and descriptions. For this, we will use the properties
keyword, in conjunction with the type keyword:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A Intelligence.AI product entry",
  "type": "object",
  "properties": {
    "title": { "type": "string" },
    "description": { "type": "string" }
  }
}

Product prices are numbers. We don’t want to sell any products for free, so
product prices must be greater than zero, a constraint we can express using the
exclusiveMinimum keyword:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A Intelligence.AI product entry",
  "type": "object",
  "properties": {
    "title": { "type": "string" },
    "description": { "type": "string" },
    "price": { "type": "number", "exclusiveMinimum": 0 }
  }
}

We can express the price currency as an enumeration of possible currency codes
using the enum keyword. We plan to sell our beans in the US and in the UK, so we will
support USD and GBP currency codes:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A Intelligence.AI product entry",
  "type": "object",
  "properties": {

92 | Chapter 5: A Universal Language for Data



    "title": { "type": "string" },
    "description": { "type": "string" },
    "price": { "type": "number", "exclusiveMinimum": 0 },
    "currency": { "enum": [ "USD", "GBP" ] }
  }
}

When using the enum keyword, the type keyword is not needed.
That is because enum will only accept one of its declared values and
nothing else, making their types implicit.

The product URL and the image URL are strings. However, we can further signify
that these strings represent URIs using the format keyword, which we will discuss in
more detail in Chapter 8:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A Intelligence.AI product entry",
  "type": "object",
  "properties": {
    "title": { "type": "string" },
    "description": { "type": "string" },
    "price": { "type": "number", "exclusiveMinimum": 0 },
    "currency": { "enum": [ "USD", "GBP" ] },
    "url": { "type": "string", "format": "uri" },
    "image_url": { "type": "string", "format": "uri" }
  }
}

JSON does not have a built-in timestamp data type. The convention is to store
timestamps as strings and further annotate them using the format keyword, like we
did for URLs:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A Intelligence.AI product entry",
  "type": "object",
  "properties": {
    "title": { "type": "string" },
    "description": { "type": "string" },
    "price": { "type": "number", "exclusiveMinimum": 0 },
    "currency": { "enum": [ "USD", "GBP" ] },
    "url": { "type": "string", "format": "uri" },
    "image_url": { "type": "string", "format": "uri" },
    "timestamp": { "type": "string", "format": "date-time" }
  }
}

Your First JSON Schema Project | 93



The only missing property is the product SKU. We can define this property using a
regular expression that constrains the values to strings starting with SHBH followed by
five digits using the pattern keyword:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A Intelligence.AI product entry",
  "type": "object",
  "properties": {
    "title": { "type": "string" },
    "description": { "type": "string" },
    "price": { "type": "number", "exclusiveMinimum": 0 },
    "currency": { "enum": [ "USD", "GBP" ] },
    "url": { "type": "string", "format": "uri" },
    "image_url": { "type": "string", "format": "uri" },
    "timestamp": { "type": "string", "format": "date-time" },
    "sku": { "type": "string", "pattern": "^SHBH\\d{5}$" }
  }
}

Object properties defined with JSON Schema are optional by default. This means our
current schema will not complain if any of the properties we defined are missing. We
can fix this by declaring the list of properties that are required using the required
keyword:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A Intelligence.AI product entry",
  "type": "object",
  "required": [ "title", "description", "price", "currency", 
                "url", "image_url", "timestamp", "sku" ],
  "properties": {
    "title": { "type": "string" },
    "description": { "type": "string" },
    "price": { "type": "number", "exclusiveMinimum": 0 },
    "currency": { "enum": [ "USD", "GBP" ] },
    "url": { "type": "string", "format": "uri" },
    "image_url": { "type": "string", "format": "uri" },
    "timestamp": { "type": "string", "format": "date-time" },
    "sku": { "type": "string", "pattern": "^SHBH\\d{5}$" }
  }
}

94 | Chapter 5: A Universal Language for Data



Finally, JSON Schema permits object properties not covered by the properties
keyword by default. For example, our schema will allow a discount property to be
present, even though we never defined it. We can prevent additional properties by
declaring the additionalProperties keyword as false:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "An Intelligence.AI product entry",
  "type": "object",
  "required": [ "title", "description", "price", "currency", 
                "url", "image_url", "timestamp", "sku" ],
  "additionalProperties": false,
  "properties": {
    "title": { "type": "string" },
    "description": { "type": "string" },
    "price": { "type": "number", "exclusiveMinimum": 0 },
    "currency": { "enum": [ "USD", "GBP" ] },
    "url": { "type": "string", "format": "uri" },
    "image_url": { "type": "string", "format": "uri" },
    "timestamp": { "type": "string", "format": "date-time" },
    "sku": { "type": "string", "pattern": "^SHBH\\d{5}$" }
  }
}

When we set the additionalProperties keyword to false, we
are saying, “Any additional property in the instance that I did not
explicitly define must validate against the false schema.” As we
will see in Chapter 8, no instance can ever successfully validate
against false, therefore no additional properties are allowed.

Generating a Web Form
Now that we have a JSON Schema that describes product entries, let’s use it to
generate a beautiful web form out of it. It will literally take us seconds, whereas
coding a form from scratch might take hours! Due to the popularity of this approach
to form generation (and with good reason), there is no shortage of existing libraries
to render web forms out of JSON Schema for virtually every web framework.
In this example, we will use React JSON Schema Form, the most popular form
generator for the React UI framework. Teaching React is out of the scope of this book,
but see Figure 5-6 for an example output using React JSON Schema Form with the
default theme. The timestamp property even results in a property calendar entry!

Your First JSON Schema Project | 95



Figure 5-6. A web form generated from the JSON Schema definition we created in this
section.

96 | Chapter 5: A Universal Language for Data



Summary
In this chapter, you learned about the JSON Schema data language, its relevance
in the data industry, and its most fundamental concepts. You now have the skills
to methodically understand every JSON Schema and consult documentation for
keywords and vocabularies you haven’t seen before, a skill that will already set you
apart as a JSON Schema power user.
You also saw the process of modeling a nontrivial JSON instance using JSON Schema
and getting immediate value out of it: generating web forms. With this knowledge,
you can revamp your existing forms into having a declarative approach involving
much less code and reuse the schemas you used to generate your forms to also
perform validation on the backend, turning your schemas into the single source of
truth.
In Chapter 8, you will learn about the two fundamental operations of schemas and
how and when to apply them to real-world data problems. More importantly, you will
learn to identify and act on opportunities that can be significantly simplified through
a schema-first declarative approach.

Summary | 97



CHAPTER 6
The Art of Alignment

Imagine that you’re the CEO of a company, and you discover that 71% of your managers
are not fully aligned with your vision. Do you think there’s any chance that you’re going
to hit your targets this year? Or next? Of course not…In fact, it doesn’t matter how good
or innovative your company’s vision is; if leaders at every level of the company aren’t fully
aligned with that vision, it has almost no chance of success.

—“A Shocking Number of Leaders Are Not Aligned with Their Companies’ Visions”
(Mark Murphy, Forbes, August 2020)

Picture yourself at a concert. The lights dim, the curtains draw back, and you’re
enveloped by the stirring performance of a seasoned orchestra. The synchronization
of their instruments, the harmony of their playing; all that transpires is the creation
of soul-touching music. However, what if the violin players suddenly began missing
their notes or the percussionists started going off rhythm? The performance would
quickly descend into a chaotic, dissonant mess.
The delicate balance required in a symphony is mirrored in the dynamics of an
organization. The employees are the musicians, their diverse roles analogous to
different musical instruments, and the information generated by an organization is
akin to the melody they produce together. The CEO, like a conductor, must ensure
the harmonious alignment of all these moving parts.
What chance would the orchestra stand of performing a symphony, let alone achiev‐
ing greatness, if 71% of the musicians were not aligned with the conductor? Analo‐
gously, if the majority of your managers are not in alignment with your organization’s
vision, the chances of hitting your targets, irrespective of how innovative or ground‐
breaking they may be, are essentially negligible. The harmony necessary for a sym‐
phony to touch souls is the same harmony needed for an organization to achieve
success.

99



Alignment is not just a corporate buzzword; it’s the foundation on which successful
organizations are built. We call alignment of the concepts that people collaboratively
use to describe business logic, technological implementation, and operational execu‐
tion harmonization, and you will learn tools to achieve it in this chapter. In Chapter 7
you will learn to align an organization’s actions, problems, and financial goals, which
is called synchronization. When an organization is both harmonized and synchron‐
ized, it is unified, and that’s what we’re going to dive into in the align phase of
unifying.

Enemies of Alignment: Ambiguity and Assumptions
The main adversaries of alignment and unification are ambiguity and assumptions.
These insidious forces can fragment organizational cohesion and compromise strate‐
gic vision, impeding progress and fostering inefficiencies. By their very nature, ambi‐
guity and assumptions lead to misinterpretation and misunderstanding, destabilizing
the harmonious ecosystem of a well-integrated business.
The root of these issues often lies in the ambiguity present in human communication,
even when the meaning of concepts seems obvious, as shown in Figure 6-1. To
illustrate this with a symphony analogy, when the notes to be played and the tempo
to be followed are not precisely defined, individuals tend to bridge these knowledge
gaps with their own assumptions, which inevitably leads to uncertainty and mistakes.
Critically evaluating ambiguity and assumptions helps identify what needs to be
clarified and addressed in the align phase of unifying.

Figure 6-1. Often, people’s basic definitions of concepts differ even when they think they
are in agreement. To eliminate such ambiguities, ask people to identify and define key
concepts in your interviews.

100 | Chapter 6: The Art of Alignment



The Standish Group, a reputable consulting firm, has been
researching the success rates of software projects since 1994. Their
metrics cover a range of factors, including the timely on-budget
completion. Alarmingly, their findings reveal that a mere 8%–9% of
software projects meet both their time and budget constraints, and
that this success rate hasn’t really changed in 30 years!
Equally concerning is a report from Geneca, a software develop‐
ment firm, indicating that a staggering 75% of managers believe
their software projects are preordained for failure, even before a
single line of code has been written!

As you have conversations with stakeholders, it is important that you know what to
look for, what pitfalls to avoid, and what opportunities exist for you to be a more
effective data champion. Knowing what to look for will help you focus where you are
most needed.

Ambiguity: The Culprit in the Illusion of Communication
The single biggest problem in communication is the illusion that it has taken place.

—George Bernard Shaw

We have this idea that communication in an organization is like a symphony, where
everyone is playing together to create a harmonious masterpiece of productivity and
success. However, it’s often like a college band after they’ve had too much to drink.
You’ve read that the key to an organization playing like an orchestra is going through
the three unifying phases: assess, align, and accelerate. Sounds like a pretty routine
and boring corporate exercise, doesn’t it? Here’s the challenge: everyone’s playing
their own tune. The engineers are playing intellectual jazz, marketing wants to play
electronic dance music and make it a party, and accounting is attempting to learn
Gregorian chant.
Many organizations fail to achieve effective alignment due to the illusion of commu‐
nication. Just because people are talking to each other doesn’t mean they are commu‐
nicating or collaborating effectively, just as collecting a lot of data doesn’t mean an
organization is data driven. Effective communication is not just about talking or
sharing information—it’s about ensuring that messages are clear, understood, and
unambiguous.
We are often so ready to work on our tasks that we don’t pause to first check for
illusions in our communication with others. We may think we’re speaking the same
language when in reality, we’re not. In one department, streamlining could mean
improving the user experience; in another, it could mean making layoffs. The boss
could decide that streamlining means placing your desk, monitor, and coffee machine
in the restroom and calling it your new office.

Enemies of Alignment: Ambiguity and Assumptions | 101



If the illusion of communication is deemed acceptable, no amount of meetings or
investments in marketing, hiring, or technology will fix it. In fact, it could be said that
leaders who focus on quick fixes without doing the hard work of unifying teams are
knowingly and preventably wasting money.

Assumptions: Ambiguity’s Best Friend
In God we trust. All others bring data.

—W. Edwards Deming, American statistician

The biggest problem with the illusion of communication isn’t the wastefulness of
unnecessary meetings, low-quality data, and meaningless bureaucratic processes; it’s
the assumption of understanding and agreement.
Tribal knowledge refers to shared information, understanding, and language that is
informal and often undocumented. It can include historical decisions, processes, and
training that is shared among members of the group. Tribal knowledge can be taken
as truth without concrete evidence. Ambiguity refers to situations or information that
are unclear, uncertain, or open to interpretation. Assumptions are beliefs or ideas that
are accepted as true without proof or verification. When ambiguity arises, assump‐
tions based on tribal knowledge are used to fill in gaps in how teams communicate
and understand each other.
As the data champion, your job is to be objective, scientific, repeatable, and standard‐
ized in your approach aligning people’s perspectives. As you interview stakeholders
in the assess phase, you will capture very different perspectives on what is important
and why. You will quickly learn people think things are obvious, or that teams are on
the same page, when in fact they are not. Some people will have a hard time writing
down the key concepts in their role because they’ve never been asked to do it before.

Defining Success: Symmetry Between Concepts and JSON
Schema Equals Minimal Ambiguity
This book is taking you through the journey of learning to unify your business, data,
and code with JSON Schema. The unifying methodology does not require you to use
JSON Schema, but we recommend it as the optimal way.
The outcome of the align phase of unifying is to convert your concepts into a
mirror-image representation in JSON Schema. JSON Schema is a foundation that
data and code teams can use for constraints and validation as information is commu‐
nicated across systems. Unifying concepts with both a business-friendly spreadsheet
representation and a JSON Schema representation ensures that there is symmetry
between a business strategy’s intention and operations in practice. It also ensures that
the data collected for analytical and machine learning purposes is well-defined.

102 | Chapter 6: The Art of Alignment



Concept-first design is covered in detail in Chapter 4. Figure 6-2
shows how concepts can be represented as JSON Schema, which is
covered in Chapter 5.

Figure 6-2. The first box contains a concept described in detail in Chapter 4. The second
box contains the mirror image of the concepts using a JSON Schema. Each Level 1
concept has a separate JSON Schema representation. Using both allows business, data,
and code teams to view a concept in the format easiest for them to understand and use.

Defining Success: Symmetry Between Concepts and JSON Schema Equals Minimal Ambiguity | 103



Illuminating Misalignment with a Concept Compass
Those who are in love with practice without knowledge are like the sailor who gets into a ship
without rudder or compass and who never can be certain whether he is going. Practice must
always be founded on sound theory, and to this Perspective is the guide and the gateway; and
without this nothing can be done well in the matter of drawing.

—Leonardo da Vinci, Delphi Complete Works of Leonardo da Vinci
(Delphi Classics, 2014)

There is an elegant and insightful instrument to guide you through the maze of
ambiguity toward alignment: the concept compass. In order to transform concepts
into the logical, machine-friendly format of JSON Schema, the first task is to ensure
the harmony of those concepts across stakeholders and leaders, the individuals who
bear the most responsibility and whose teams are the key users of these concepts.
The concept compass, as depicted in Figure 6-3, is a simple two-dimensional matrix,
with the x-axis representing who the primary stakeholders are and the y-axis showing
where it materializes within the organizational landscape. Using this format, adding
symbols such as check marks, Xs, or question marks will enable you to flag where
ambiguity and disagreement exist. This chapter will guide you through constructing a
concept compass one step at a time.

Figure 6-3. The concept compass is a simple two-dimensional grid which has a list
of stakeholders (from within a team or across multiple teams) on the horizontal axis,
and a list of where the concept needs to be implemented (e.g., from a business logic,
operational, or technical perspective). What people disagree with or where concepts are
implemented, can be marked using an “X” or “✓” symbol, and a “?” symbol is a good
placeholder for something unknown. In the center is an example of a concept that needs
to be unified across the who and where continuums.

104 | Chapter 6: The Art of Alignment



In some institutions, the primary stakeholders, the who, are openly recognized as
being responsible for managing and educating other teams about how concepts are
defined. However, in many organizations, pinpointing who “owns” managing con‐
cepts from a cross-functional perspective may be a more enigmatic task. Why is that?
The answer is often an entrenched system of dated business logic set by management
and teams of bygone eras. As the torch is handed over to the new generation, they
unknowingly adopt this logic as an immutable axiom without grasping when and
why it came into existence.
The where axis is easier to predict, as it hinges on three fundamental elements:
Business logic

Do teams agree on the definitions and implications of key concepts?
Operations

Is there congruence between the stakeholders’ stated way—how things ought to
be—and the actual methods employed in practice?

JSON Schema
Has the business logic been translated into a computational format, such as
JSON Schema, that can be seamlessly integrated with and utilized by various
technologies?

Step 1: Harmonizing the What
When you interview stakeholders, meet with them separately to get their most unvar‐
nished perspectives. When you have converted their explanations of concepts into
the concept-first design perspective (as explained in Chapter 4), you will have the
stakeholders (the who) and their business logic perspectives (the what) captured in
a standardized format. If their perspectives don’t match up, you have a clear guide
identifying exactly where misalignment exists across these stakeholders, and can
move toward methodically eliminating it, which you will learn in this chapter.
We recommend that the who list (horizontal axis) in your concept compass have a
minimum of three perspectives from three interdependent teams. You might want to
start on aligning one team first before reaching out to others. Getting a diverse set of
perspectives from multiple teams is critical.
Let’s dive into a hypothetical scenario. Through interviewing the Finance, IT, and
Sales teams, you apply the concept-first design techniques you mastered in Chapter 4
to define a “sales order” concept, as shown in Figure 6-4.

Illuminating Misalignment with a Concept Compass | 105



Figure 6-4. A hypothetical scenario illustrated by the concept compass. The Finance and
IT teams agree on the way this concept was designed, but the Sales team disagrees with
the definition of revenue. Because the business logic (the what) is not yet agreed upon,
Operations and JSON Schema (the way and the how) cannot yet be determined.

Dissonance emerges—a semantic tug of war between the Sales and Finance teams
over the term sales order, specifically whether the revenue calculation deducts a
salesperson’s commission.
The IT and Finance teams are in agreement that a “sales order” includes subtracting
the salesperson’s commission from the total sales figure, meriting an approving check
mark next to their teams. An “X” next to the Sales team signifies their disagreement.
With the absence of a consensus on the business logic, the operational implementa‐
tion—captured as the way and how in Figure 6-4—remains uncertain, questioning
which direction truly points to the north and how it should be represented. Thus,
the concept compass becomes a beacon that illuminates the path through the maze
of organizational alignment, bringing order to chaos and nurturing a shared under‐
standing that propels the collective force toward success.

These concepts also have a place in the Agile methodology. They
are instrumental in user stories, which succinctly capture the
intricate business logic of user experiences. Such integrative rep‐
resentation is a powerful tool that knits together diverse ideas
and interpretations to create coherent understanding across an
organization.

Step 2: Harmonizing the Way
Too often, management constructs intricate processes and workflows that are dis‐
connected from reality and force boots-on-the-ground employees to jump through
hoops to fulfill their jobs in ways that look nothing like the way it’s supposed to
work and is completely inefficient. Staff are forced to maneuver through a maze

106 | Chapter 6: The Art of Alignment



of unhelpful apps and systems to stay compliant. Many, overwhelmed, abandon the
quest, and dirty, substandard, and sometimes corrupted data emerges as collateral
damage.
In operational processes, you need seamless alignment with your foundational princi‐
ples, both in the context of your business logic and the JSON Schema representations.
Should there be a disconnect, you need to recalibrate either the fundamental princi‐
ples or the operational processes.
Unveiling the power of alignment in the world of collaboration requires a fresh
perspective. Imagine your collaborative effort as a carefully constructed dance of
ideas: the who, what, and how. When they move in harmony, an idea has a palpable
symmetry across your business landscape. Here, symmetry is not merely aesthetic, but
a reflection of balance and integration between the different layers (in this case, busi‐
ness logic, operational execution, and JSON Schema) of your collaborative endeavor.
Consider this symmetry to be like the perfect balance of elements in a well-crafted
masterpiece of architecture. It can’t be “somewhat” balanced; it’s either balanced,
standing proud and sturdy, or it’s not, threatening to crumble at the slightest
disturbance.
Just as the structural integrity of a building relies on symmetry, the resilience and
potency of your collaborative concept rely on alignment. Greater degrees of symme‐
try nurture a robust and resilient architecture of collaboration, promising thriving
success in the face of change and adversity.
For example, if you have three teams in your who, and you have the three core when
dimensions (business logic, JSON Schema, and operations), you have six degrees of
symmetry for your concept. Six degrees of conceptual symmetry is considered the
minimum for a well-defined concept for a data product.
You can design the perfect concepts in JSON Schema and get everyone to agree to
the business logic, but if the concept is not implemented in operational practice, then
all of your work is meaningless. Operations are the way things are done. If there is
a difference between the way things are done and the way things are supposed to be
done, that discrepancy creates a gap that needs to be addressed for any unification
efforts to work effectively. Therefore, you need to harmonize the who (the individuals
or teams involved), the way (the operational tasks and processes for achieving goals),
and the how (the concepts codified with JSON Schema) to design for data-centric
collaboration.
The Way is where the conceptual rubber hits the road idea symmetry becomes
key; each part must reflect and support the others to create a unified, efficient, and
effective system that not only withstands the challenges of business but also catalyzes
growth and innovation.

Illuminating Misalignment with a Concept Compass | 107



Step 3: Harmonizing the How
JSON Schema is a representation of a concept as a coded specification. Unlike other
schemas, JSON Schema can define more than just structure. JSON Schema has two
primary purposes: validating data against the coded specification and extracting
information/annotations, both of which are covered in detail in Chapter 5. These
enable your schemas to be the trusted source of truth for people and systems.
Before schema representations can be worked on, business logic (the who) and opera‐
tional practice (the way) alignment must be done. This is because if stakeholders can’t
agree on concepts, there is no way to have accurate, technical, coded specifications of
them.
Your JSON Schema definitions are representations of your organization’s concepts. In
other words, a JSON Schema is the equivalent of the concept itself.
Da Vinci’s comment that without perspective nothing can be done well holds true for
data. Without an agreed-upon understanding of a concept with schema definitions
providing the shared specification, data management cannot be done well over the
long term, and this is especially true with increasing complexity.
To understand this further, think of a schema’s constraints-based language as provid‐
ing perspective lines for concepts, as shown in Figure 6-5.

Figure 6-5. The left image shows the perspective lines which are created by the schema. A
schema is a set of constraints, which in this example states that all points must fall on the
y-axis (numbers) and the x-axis (letters). Without an agreed-upon schema, there is no
shared perspective for data, which creates openings to introduce ambiguity.

The how dimension is about how concepts will be implemented at various technical
levels in different functional business units. You will have to make trade-offs based
on which systems are the most expensive to change, what talent is available, and

108 | Chapter 6: The Art of Alignment



how much risk change introduces to implement JSON Schemas into your technical
systems.

For the remainder of this book we will refer to JSON Schema
exclusively for harmonization of how concepts are to be imple‐
mented. Although other techniques exist, they are beyond the
scope of this book.

Harmonized Concepts
Once you have harmonized a concept across the who and where axes, you should
have a concept compass that looks similar to the example in Figure 6-6, where the
business logic, JSON Schema, and operational usage of the concept are all symmetri‐
cal across stakeholders and implementation representations.
Having the concept in the concept-first design format in a spreadsheet is already
making it easier for technical teams to implement. The concept compass format
identifies misalignment and, more importantly, lets you know when a concept is uni‐
fied. Using the concept compass will act as a translation layer between business and
technical teams and will greatly enhance the technical team’s ability to understand
risks, prevent mistakes, and move quickly.
The concept compass focuses on the source of where data, people, and systems are
out of sync. No technical solution will resolve the issue; this is an organizational
design problem that requires participation and an active focus on seeing data in the
organization from a holistic perspective.
Integrating a concept-first design approach into your organization’s system might
seem like a significant challenge, but think of it as finding a universal language—a
translation layer—that bridges the gap between your business and technical teams.
By arranging your ideas in a neat spreadsheet, you’re not only making your concept
easier for technical teams to implement, but also ensuring that everyone is on the
same page, reducing potential missteps and accelerating progress.
The concept compass helps you identify when your team is veering off course and
when your concepts align perfectly. By highlighting these discrepancies, it points out
exactly where data, people, and systems may be falling short.
But here’s the catch: no technical solution alone can address these issues. It’s a larger,
organizational design challenge that calls for everyone’s active participation and a
more holistic view of data within your organization.
Now, imagine a library—a Unified Concept Library to be precise—that contains all
of these harmoniously intertwined concepts. Each concept would be available in
both a business language that everyone can understand (the what) and a technical

Illuminating Misalignment with a Concept Compass | 109



representation for data and code teams as JSON Schema (the how). This is the power
of a concept-first design.

Figure 6-6. When concepts are aligned along the who axis (marketing, sales, and finance
teams), and along the where axis (business logic, JSON Schema, and operations) a
concept can be called unified.

110 | Chapter 6: The Art of Alignment



Validating Concepts: Belief Scoring and Hypotheticals
As you seek to align various concepts, you will inevitably encounter differing and
conflicting perspectives. Therefore, it is important that you have methodical ways
to think through challenges you may face as you design and align concepts with
stakeholders.
There are two key ways to validate concepts: counterfactuals and belief scoring.

Counterfactuals
As a stakeholder explains a concept to you, it is important to occasionally present
them with a counterfactual, a hypothetical statement that often includes an if or could
have.
Let’s say the stakeholder provides this requirement: “The system must provide daily
sales reports.” A counterfactual question for this requirement might be, “If the system
were to only provide weekly sales reports instead of daily, how would that impact
your operations?” This question proposes an alternative scenario (weekly reports
instead of daily) and asks for the stakeholder’s input on the hypothetical impact of
this different situation, which will help you understand the importance of the daily
aspect of the requirement.
Other examples might be, “What happens if there is no title?” or “How would a nega‐
tive price per unit affect the system?” If the stakeholder says, “Price per unit is always
greater than 0,” then you can add a column in your concept called Requirements, and
add the value to your concept. This can easily be translated into JSON Schema, as
shown in Figure 6-7.

Figure 6-7. Asking counterfactuals can help surface any requirements or logical rules
that you may need to consider for concepts. It is important to get alignment between
stakeholders across counterfactuals to make sure you aren’t missing anything when
designing concepts.

Validating Concepts: Belief Scoring and Hypotheticals | 111



Belief Scoring
One of your jobs as a data champion is to find and label incorrect assumptions or
inaccurate perspectives as quickly as possible. The word that describes confidence in
people’s perspectives is belief. The tool that you will use to evaluate beliefs is called
belief scoring.
Figure 6-8 shows two concepts defined by two stakeholders that are not aligned with
each other. As you interview stakeholders, you will need to document the validity of
various perspectives.

Figure 6-8. Two concept-first design spreadsheets with different definitions of the word
cost: the cost at time of sale, and the cost to produce. Which is the right one to use? The
question mark labels this concept as unsure.

Use this simple framework to keep track of what perspectives warrant being believed
and which warrant being challenged:
Invalidated

Concepts that a stakeholder believes to be true but that don’t fit into another
perspective that can be trusted. For example, if a salesperson doesn’t enter a field
on a form and tells you, “No one cares about that,” and the CFO says, “If we don’t
have accurate sales data, we won’t be compliant with SEC regulations, and I can
go to jail.”

112 | Chapter 6: The Art of Alignment



Unsure
Assumptions that haven’t been evaluated effectively but have been defined. For
example, if a salesperson says that revenue includes the full sale (which impacts
their bonus), until you check with Finance, you are unsure that their particular
perspective is valid.

Undefined
Imagine a customer service rep that says, “When the customer’s profile has a
status of 4, I am supposed to transfer them,” and when you ask them why,
they can’t explain it. That means you have no idea whether it is something that
actually needs to be aligned when designing data products or if it is part of some
legacy system that is no longer used.

Trusted
This means that you are unable to invalidate a perspective.

As you work toward aligning different concept designs, using red backgrounds or a
question mark to identify unsure beliefs, a check mark to identify validated beliefs,
and so on, can help keep track of where ambiguity and assumptions exist.
The symbols we recommend are as follows:

Trusted: Check mark (✓)
Unsure: Question mark (?)
Invalidated: X
Undefined: Minus sign (–)

Summary
In this chapter, you learned a set of simple, practical tools and theory for creating
alignment in an organization. Regardless of the competence of individual teams, an
organization’s progress is hindered by a lack of shared vision. The concept compass
is a tool for aligning teams toward a unified vision; converting these concepts into
a JSON Schema creates a bridge between business, data, and code. The compass
uses stakeholders as its x-axis and implementation as its y-axis to illustrate areas of
consensus and conflict. It elucidates the process of achieving alignment in three steps:
aligning the who and what on business logic, the way with operations, and the how
a concept is defined. It also underscores the role of leadership in fostering alignment
and the potential fallout of misalignment.
The chapter further explored the main obstacles data champions face—ambiguity
and assumptions—and how they inhibit effective communication within organiza‐
tions. The illusion of communication—when people assume they are communicating
effectively just because they are talking—leads to misunderstanding, wastes resources,

Summary | 113



and contributes to a lack of alignment. Much miscommunication is related to tribal
knowledge—informal, unrecorded understanding and language among teams that are
accepted as truth without concrete evidence.
Data champions must adopt an objective, scientific, and standardized approach to
identify and challenge these assumptions. We introduced you to a concept validation
framework that uses belief scoring and counterfactuals to categorize perspectives as
invalidated, unsure, undefined, or trusted. This framework aids data champions
in managing ambiguities and assumptions effectively, facilitating alignment, and
removing communication barriers. So you now have tools and strategies to navigate
organizational alignment and foster a more harmonious and productive business
environment.
Chapter 7 explores the latter half of the align phase within our unifying methodology:
synchronization. This aspect diverges from the conceptual focus of harmonization,
which primarily centers around defining ideas through the lens of language and con‐
cepts. Synchronization emphasizes the alignment of people, systems, and processes to
actions.

114 | Chapter 6: The Art of Alignment



CHAPTER 7
The Science of Synchronization

There are no separate systems. The world is a continuum. Where to draw a boundary around
a system depends on the purpose of the discussion.

—Donella H. Meadows, Thinking in Systems (Chelsea Green, 2008)

Your organization is a living, evolving entity, pulsing with vibrant interactions among
complex systems made of people, technology, and processes. Systems with many,
constantly changing connections are called networks.
Chapter 6 dove into the art of harmonizing concepts. We navigated the labyrinth
of aligning user perspectives (the Who) and the myriad systems across which these
concepts spring to life (the Where). We raised a toast to you, the data champion, as
you journeyed to achieve symmetry of concepts between distinct perspectives.
Let’s hold on to that champagne cork for just a bit longer; our voyage isn’t complete.
An agreement on definitions of concepts only pieces together part of our puzzle. The
unutilized potential of concepts, trapped as thoughts and not propelling action, is a
hollow victory. The align phase of unifying isn’t merely about harmonizing concepts
to the same musical key; it’s also about synchronizing how concepts are used in action
to the same musical time signature.
In order to direct this dance across the organization, we must shift our lens to a
wider panorama, viewing the organization as a collection of intricate, interconnected
networks that are collaborating and learning together. With this approach, we can
trace the graceful interplay of concepts as they surge, intertwine, and evolve among
individuals, systems, and processes. As if part of a grand symphony, they move in a
rhythm, directed by a common score—the resolution of organizational goals.
In this chapter, you’ll learn about organizational networks through the lens of an
organizational governance framework for designing effective human and machine
learning capabilities. CLEAN is an acronym for Collaborative Learning Networks,

115



and the CLEAN data governance framework like the concept compass in Chapter 6,
is a valuable tool for illuminating and resolving misalignment in organizational
networks.

An Introduction to Thinking in Networks
Thinking in networks is a conceptual approach where relationships, connections, and
interactions between various elements are emphasized over individual components. It
involves a deep understanding of how each part of a system contributes to the whole
and how changes in one part can ripple across the network. This mindset has both
theoretical and practical implications.
Imagine the world as a web of interconnected entities where the relationships
between the elements are as critical as the elements themselves. By focusing on
these interconnections and interdependencies, “thinking in networks” helps us to
illuminate the inner workings of complex systems. It offers a robust framework that
can empower us to better predict outcomes, understand emerging patterns, and
devise holistic solutions that consider the entire network.
In the realm of data governance, this approach can be instrumental in identifying
inefficiencies, bottlenecks, or areas for improvement. By enabling a bird’s-eye view
of the system’s interconnectedness, we can strategically tweak certain elements to
optimize the overall performance.

Example of Thinking in Networks: Athletes Versus Artists
The earning potential of both athletes and artists is closely tied to their respective
achievements. However, the way these achievements are measured and valued differs
significantly between athletics and the arts.
For athletes, success is often gauged through concrete, quantifiable metrics. Let’s take
a golfer as an example. In golf, the primary metric is the number of strokes taken
to complete a course. A golfer who consistently finishes courses with fewer strokes
than their competitors is objectively performing better. Winning or ranking highly
in prestigious tournaments, such as the Masters or the U.S. Open, earns significant
prize money. Additionally, consistent high performance increases a golfer’s chances of
securing lucrative sponsorship and endorsement deals. In golf, the financial rewards
can be linearly predicted based on tournament results and strokes recorded.
On the other hand, the success of an artist is measured in a more subjective way;
there is no universal yardstick to measure the greatness of an artist’s work. Instead, an
artist’s success is determined by the prestige of the galleries where they’ve exhibited
their work and by the art collectors who buy and sell their pieces.

116 | Chapter 7: The Science of Synchronization



Rather than a linear, easily measurable scale like an athlete’s race time, an artist’s
success depends on the interconnected network of galleries and collectors. This
network of relationships and transactions shapes the artist’s reputation and ultimately
determines the economic value of their work. The success of an artist and the price
their art can command can thus be predicted by examining this network.
So, while both athletes and artists have their successes tied to their earnings, the
ways these successes are assessed and valued present a fundamental contrast. Athletes’
earnings can be predicted in a linear manner based on concrete performance metrics,
while artists’ earnings are forecasted based on a network of relationships and subjec‐
tive assessments of their work.

Graphs: The Visual Language of Networks
A graph, in the field of mathematics and computer science, is a fundamental tool used
to represent pairwise relationships between objects. A graph is composed of vertices
(also known as nodes) and edges (also known as relationships or links) that connect
these nodes. Graphs can be either directed, meaning that each edge has a direction
from one node to another, or undirected, with edges having no orientation.
In the example shown in Figure 7-1, a person concept is represented as two nodes:
Bob and Alice. The relationship between Bob and Alice has a label describing the type
of relationship they have, in this case, the label is friend.

Figure 7-1. A graph example with two nodes as data points that instantiate the concept
“person.” In the first example, Alice and Bob are mutually friends; in the second, Bob is
friends with Alice, but there’s no evidence it is mutual; and in the third, Bob considers
Alice his best friend, but poor Bob isn’t getting any love back.

When links have an arrow, that means they are directed. A weight is when we add a
numerical value to a relationship, such as “Bob is a 5 out of 5 friend to Alice,” but this
graph doesn’t tell us the value of Alice’s friendship to Bob.
Graphs are useful because they provide a visual and intuitive way to represent and
analyze complex systems and networks. They’re used in numerous areas, such as
social network analysis, search algorithms, transportation networks, biological net‐
works, and the internet, to name a few. By using graphs, we can understand the
structure of these systems, find patterns, derive insights, and even make predictions.

An Introduction to Thinking in Networks | 117



Therefore, the concept of a graph serves as a cornerstone for many practical applica‐
tions in fields like computer science, data analysis, engineering, and more.
Here are some more examples of how graphs and edges can be used to understand
networks:

• In a transportation network represented as a graph, the nodes might be cities,
and the edges might represent highways between the cities. The labels on the
edges could represent the distance between the cities, the time it takes to travel,
the cost of travel, or any other relevant data.

• In a social network, the nodes might represent people, and the edges could
represent relationships between people. A label on an edge might represent the
strength of the relationship, the type of relationship (friend, family, coworker,
and so on), or other relevant information.

There are many types of networks, and graphs are a general way to
visualize and understand them. You can use graphs when drawing
on a whiteboard, when creating a presentation in PowerPoint, or
when creating a database of a social network.

Networks of Entities: Knowledge Graphs
Imagine having a tool that could seamlessly organize and integrate complex data
for dynamic exploration and understanding. Sounds ideal, doesn’t it? That’s precisely
what a knowledge graph offers. It’s more than just a diagram; it’s a visual represen‐
tation of a network of entities (or nodes) and their interconnected relationships,
typically signified by arrows or lines.
But a knowledge graph doesn’t just capture these connections—it also encapsu‐
lates the context and meaning of your data, creating a rich semantic map. It is
like a decoder ring for unstructured data, providing the means to make sense
of vast amounts of disparate information and rendering it into a comprehensible,
manageable form.
The benefits derived from knowledge graphs are extensive, with numerous prac‐
tical applications. By drawing connections between entities and facts, knowledge
graphs promote data discovery, knowledge management, decision making, and pre‐
dictive analytics. For instance, Google leverages a comprehensive knowledge graph
to enhance its search engine results by better predicting the searcher’s intent and
the contextual meaning of search terms. LinkedIn also uses a knowledge graph for
its “People You May Know” and “Jobs You May Be Interested In” features, which
promote networking and job opportunities.

118 | Chapter 7: The Science of Synchronization



A Simple Knowledge Graph
From a technical perspective, a knowledge graph consists of three main components:
nodes, edges, and properties. Nodes represent entities, such as people, places, or con‐
cepts. Edges represent relationships between these entities. And properties provide
additional information about nodes or edges.
The foundation of a knowledge graph is a graph database, which stores data in a
graph structure and uses graph theory principles for data querying. Graph databases
use index-free adjacency, meaning each node directly points to its adjacent node,
enabling high-speed traversal, even with billions of nodes.
Let’s consider a simple two-node knowledge graph that represents the relationship
between two entities: John and London. In this example, John is a person who lives in
the city of London.
To visualize this, Figure 7-2 represents John and London as nodes and the relation‐
ship lives_in as an edge connecting these two nodes. This graph thus carries the
semantic information that the person named John lives in the city of London.

Figure 7-2. A simple two-node knowledge graph example

Challenges with Knowledge Graphs
Though knowledge graphs are powerful tools, their implementation comes with
significant challenges. One of the most common obstacles is ensuring the quality and
consistency of data input, as knowledge graphs are highly dependent on the input
data’s structure and quality. Poor data quality can result in inaccurate insights and
predictions.
Moreover, creating and maintaining a knowledge graph can be labor-intensive and
may require substantial domain expertise. There’s a need to define entities, relation‐
ships, and properties correctly, and to continuously update the graph as new infor‐
mation becomes available. A knowledge graph can become an organization’s source
of truth, but those that are charged with maintaining and implementing knowledge
graphs can become overwhelmed by the number of concepts to manage and man‐
ually control. This creates bottlenecks, which leaves teams to go around the system,
thus creating more chaos in data quality.

Networks of Entities: Knowledge Graphs | 119



Aligning Knowledge for the 99%
Successfully implementing, maintaining, and utilizing knowledge graphs can be a
significant cultural and operational change for many organizations, with advanced
technical requirements and expensive operational costs. Some companies, including
Google, LinkedIn, and Facebook, having made massive investments over decades,
now have truly amazing knowledge graph capabilities with dedicated experts to
maintain them. However, 99% of companies have not committed the same resources
and long-term vision nor actively recruited the talent to architect and sustain robust
knowledge graph strategies.
Additionally, if your organization invests heavily in knowledge graphs but not in
aligning the organization’s concepts, then it doesn’t matter how good your knowledge
graph is. If leaders are not aligned strategically and operational teams are not aligned
on the meaning of the concepts they are supposed to implement, a knowledge graph
is an expensive investment building upon faulty foundations.
Therefore, it is important to learn the organizing principles of designing knowledge-
based systems, whether they are implemented in knowledge graphs or not. Unifying
is about bringing large parts of the organization together under a single framework,
and therefore the methodology is designed to be as simple as possible (so that anyone
can understand it) and as general as possible (so that anyone can apply it with any
technology).
In Chapter 6, you learned how to use the concept compass to map concepts, people,
and operations in order to identify misalignment. In this chapter, you will learn the
CLEAN data framework, which is a set of simple organizing principles for identifying
and resolving misalignment in how concepts are connected, shared, and communica‐
ted in networks.

If you’d like to learn more about knowledge graphs, Building
Knowledge Graphs, by Jesus Barrasa and Jim Webber (O’Reilly,
2023) is an excellent book.

Fundamentals of CLEAN Data Governance
You never change things by fighting the existing reality. To change something, build a new
model that makes the existing model obsolete.

—Buckminster Fuller

As we’ve discussed, unifying an organization under a single framework is about much
more than just creating connections; it’s about ensuring those connections are aligned
and can efficiently facilitate the sharing and communication of data and concepts.

120 | Chapter 7: The Science of Synchronization



With the knowledge we’ve gained about networks and the power of thinking in
networks, we’re ready to delve into the four key components of the CLEAN data
governance framework:
Collaboration

Stakeholders and organizational structures
Knowledge

Data, data management, and learning experiences
Business Logic

IP, how business value is determined, compliance, and processes
Activity

Sequences of setting goals, taking actions, and measuring outcomes
Using simple organizing principles, CLEAN provides a method that is both easy
to understand and widely applicable, regardless of technology. Just as a knowledge
graph can connect two entities with a relationship, such as “Bob is friends with Alice,”
the goal of this approach is to connect contexts such as who is using the concepts that
Bob is friends with Alice, what is the business value, and why does knowing about Bob
and Alice create business value?
The four components of CLEAN can be further broken down into more granular
categories that serve as an outline for what an organization needs to consider to
ensure effective data governance, as shown in Figure 7-3. The specific steps for imple‐
mentation are dependent on the individual organization’s context, goals, and needs;
however, by using a standardized framework, you can more easily have informed
discussions with stakeholders across the organization, with a shared understanding
that in order to have clean and valuable data that can be used to design human
and machine learning systems, you need a map of the way concepts are used and
connected beyond any one team’s silo.
As with the concept compass, your goal is to capture different people’s perspectives
on how the dimensions of CLEAN all fit together through stakeholder engagement.
When you find disagreement, you will need to score beliefs, just as you learned in
Chapter 6, and work to resolve a ground truth. You may be surprised at how many
different perspectives may emerge as you ask people to walk through the four main
components.

Fundamentals of CLEAN Data Governance | 121



Figure 7-3. The CLEAN data governance framework has four main components: collab‐
oration, knowledge, business logic, and activity. The visualization is in the shape of a
network because CLEAN views these 4 main components and their 12 dimensions from
a holistic perspective—where they each have a relationship with and impact on each
other, collectively.

The CLEAN data governance framework does give you a simple frame of reference
to align everyone to. Your job is to find misalignment, create awareness of it, invite
stakeholders to be thought partners in how to create alignment, and evangelize
CLEAN in order to gain adoption. It is also important to regularly check in with
stakeholders to make sure they have a voice in the continual process of adapting and
learning a shared understanding of the framework.
When you complete the CLEAN data governance process with stakeholders, they
should be aligned or know what they need to do to align their answers to questions
such as who is involved on objective X, and what compliance rules do we need to
follow? Synchronization is about removing the barriers (ambiguity and assumptions)
to being able to move quickly, efficiently, and with confidence.

Collaboration
The heart of collaborative learning networks are people. However, people are part
of a larger context, such as their role in the organization, what team they are on, or
whether they are clients or partners. When we think about data, we want to have clear

122 | Chapter 7: The Science of Synchronization



documentation that states who can use which data, from which team, and with which
rights.
The following three dimensions are recommended to include in your collaboration
conversations:
Organization

What are the different teams, hierarchical relationships, and responsibilities?
What are the sequential steps various teams and functional departments take to
achieve organizational goals?

People
Who are the different people on the different teams, what roles do they have, and
what access do they have to various data and other information? Is there a clear
way to identify customers, third-party vendors, or internal employees related to
various components of CLEAN?

Partners
Are there any additional organizations that are customers or suppliers, and if so,
what are their organizational structures (similar to the organization dimension
above)?

Knowledge
Creating knowledge is a multifaceted process (as shown in Chapter 4, where we
examined data products and their four facets: context, structure, meaning, and data).
The CLEAN framework facilitates the creation of high-quality data products. It does
this by providing a shared understanding of how individual data sets relate to the
organization as a whole. This shared understanding empowers people to define the
context of their data more effectively:
Resources

This can be documents, video, audio, spreadsheets, presentations, or any other
kind of content that people use to achieve their goals.

Data
The information that is created, consumed, and distributed by applications and
people in order to computationally understand situations and make decisions.

Code
Any kind of code, whether data science/machine learning models or web code,
that can be organized in a modular fashion.

Fundamentals of CLEAN Data Governance | 123



Business Logic
Every organization has processes for managing resources and making transactions.
Take, for instance, a retail company’s inventory management system; data related to
supply, demand, and sales trends is crucial to maintain stock levels and minimize
costs. This underlines the importance of understanding which processes will utilize
the data and why. Let’s say the data is employed to address an issue potentially costing
the company $100M. Knowing this value helps teams prioritize their tasks efficiently,
ensuring they don’t spend time solving less significant problems, like a $1K issue. 
As for legal compliance, consider the General Data Protection Regulation (GDPR),
a set of rules governing the privacy and security of personal data in the European
Union. Such mandates shape the processes and constraints around data use, thus they
should be included in the discussions. Consider these categories:
Financial

How are the associated costs, assets, and revenues related to information being
calculated? Without an understanding of the financial aspects of the business, it
is difficult to make effective decisions related to investments in talent, technology,
or change.

Compliance
What are the legal or industry requirements with which an organization needs
to comply in order to operate? This could be the General Data Protection Regula‐
tion (GDPR) in Europe, the Health Insurance Portability and Accountability Act
(HIPAA) in the US for health data, System and Organization Controls 2 (SOC
2) standards for cloud providers, accessibility standards for digital content, or
other industry-specific regulations. It is important to have a way to understand
or quickly find how to reference and validate compliance needs in order to make
effective decisions.

Processes
When everyone has their own map, there is no well-defined process. Companies
spend a lot of time and money hiring consultants to create process maps, but that
rarely works because teams often need to adapt processes. It’s more effective to
draft an initial process map using simple tools like whiteboards and presentation
slides, gain consensus, and then refine as needed, rather than rely on a process
map that’s sophisticated but outdated and unused. Process mapping is very
valuable, though, in analyzing processes and identifying problems, which we will
cover in the Accelerate phase of unifying, which begins in Chapter 9.

Activity
Understanding why people are taking actions and using data is critical in mapping
how concepts are used across the organization and illuminating where misalignment

124 | Chapter 7: The Science of Synchronization



might be happening. Often, there is a lack of understanding on how to measure the
value of outcomes. Here’s how to map the process out:
Objectives

Getting a simple list of what the objectives are is a foundational, beginner-level
task, and yet teams with multimillion dollar budgets often fail to make sure
objectives line up with and support each other. Make it a clear priority to do so
and ensure that the objectives support each other and the financial needs of the
organization.

Actions
How are actions observed? Is it through a daily meeting, an automatic behavioral
analytics tool, or through quarterly one-on-ones? Identify how actions and the
lack thereof can be tracked toward achieving objectives.

Outcomes
What are the results of the actions toward objectives? How far did the team get?
What is the impact and the financial reward (or loss)? Identify clear ways to
capture this information.

CLEAN implementation at a company is an iterative, evolving
process, allowing stakeholders, champions, and leaders to continu‐
ously adjust their approach to data governance as new challenges
and opportunities arise. You get back what you put in, and ulti‐
mately, leadership needs to enforce alignment with a governance
model if it wants one implemented.

CLEAN Data Governance in Practice
Data governance is a strategic corporate initiative that aims to ensure the quality,
availability, integrity, security, and usability of data assets. Rather than just a series of
meetings or reports, it is a holistic, ongoing process that empowers organizations to
manage their data efficiently and optimize its value.
You can think of it as an air traffic control tower overseeing an organization’s data
landscape. It monitors all data movement, ensuring optimal utilization and protec‐
tion against misuse or error. This process touches every department, making it a
collaborative initiative requiring universal commitment.
In practice, CLEAN data governance starts with establishing a data governance group
that can then begin the process to map out the networks of the flow of data across
the organization in a holistic way. Comprising representatives from different depart‐
ments, this group ensures all stakeholders’ needs and objectives are considered and
enforces data policies, standards, and strategies. It’s an ongoing process that adapts as
the organization and its data needs evolve.

CLEAN Data Governance in Practice | 125



Consider a hypothetical company operating in a distributed environment that wants
to address data silos and redundancy. Implementing the CLEAN framework, they
map out teams, roles, and interrelationships, documenting each team’s data handling
and usage. They create an internal online portal to facilitate collaboration and share
insights, fostering a culture of collaboration. When this company expands into a
new market with different regulatory requirements, the council swiftly revises data
policies and educates all departments on the changes, showcasing the adaptability of
this process as organizational data needs evolve. Figure 7-3 provides a guide to the
various stakeholder perspectives that need to be captured and related while looking
for misalignment and designing appropriate ways to meet stakeholders’ needs.
Imagine being on an R&D team that is striving to create efficient machine learning
techniques for health record analysis. Without a clear map of objectives or necessary
knowledge, it would be challenging to inspire confidence or investment. This sce‐
nario often plays out in the corporate world, leading to financial loss and reputational
damage due to inconsistent data handling and shifting project objectives.
Created specifically for the unifying methodology, CLEAN data governance offers a
remedy to this issue, providing a checklist approach to understanding how teams,
datasets, processes, and financial decisions interconnect. This leads to a shift in
mindset, transforming data governance from a simple policy to a company-wide
initiative. With this approach, a collaborative learning network emerges, fostering
understanding and unified goals throughout the organization.

The Four Facets of Data Products and CLEAN
Imagine the scene: a data science team convenes for a stakeholder meeting. The
agenda is a CSV file brimming with potential insights, but they are stymied in their
efforts to understand it. The columns lack semantic definitions, which leaves the
team scrambling in search of the right stakeholder who can provide clarity. The data’s
provenance is a mystery; no context is given for why it was generated, which business
problem it addresses, or the compliance issues at play. The inefficiency is palpable.
Enter our methodology: CLEAN. Through a series of data governance interviews,
the pain points surface. The need for semantics, context, structure, and compliance
becomes crystal clear.
From these insights, we formulated the four facets of the data products model intro‐
duced in Chapter 4, which posits that data should come packaged with context,
meaning, and structure.
The essential lesson here? By incorporating perspectives from a wide array of stake‐
holders—data users, legal and regulatory enforcers, project managers, and beyond
—you can gain a 360-degree view. This comprehensive understanding enables
governance that respects and integrates the insights of all involved parties. With

126 | Chapter 7: The Science of Synchronization



CLEAN, you can ensure that every voice is not just heard but understood and acted
upon.

The Four Horsemen of Data Death
The only way CLEAN can be successful is if leadership empowers data champions
to work cross-functionally in terms of education, communication, and being able to
evaluate implementation transparently. One challenge data champions will face is the
four horsemen of data death, shown in Figure 7-4, which can imperil the success of
your efforts.

Figure 7-4. The four horsemen of data death are the most common management impedi‐
ments to unifying your organization. Leaders need to be aware of and address these in
order for data champions to be successful.

Management styles vastly differ; some inspire and empower individuals, others
engender chaos, inefficiency, toxicity, and poor decision making. Our consultancy
experience has highlighted four recurring, detrimental behaviors that significantly
hinder successful implementation of a data governance framework. It is essential
for senior leadership to address these issues for the organization to be healthy

The Four Horsemen of Data Death | 127



and operate effectively. This is not an exhaustive list, but the behaviors outlined
in Figure 7-4 certainly are the most common and harmful.

Ignorance
Some leaders express the desire to be data driven, or even believe they are so, simply
because they have made significant financial investments in data. However, a closer
examination often reveals that the data teams have no say in business operations
or investment decisions, which severely impacts data quality. Although data teams
are expected to convey data-related challenges to the business in layman’s terms,
that alone isn’t a comprehensive solution. Often, senior business leaders hesitate to
admit their lack of knowledge about data management concepts, which undermines
their understanding and decision-making capabilities. Leaders must be willing to
invest time and effort in learning foundational principles, change their operational
processes and decision-making factors to improve data quality, and acknowledge that
having large amounts of data doesn’t make them data driven. Expecting the data team
leader to resolve issues single-handedly is unrealistic.

Siloed Incentives
Productivity and business value can be severely compromised when individual per‐
formance incentives are not aligned with overall organizational objectives. Rewarding
individuals solely based on their personal goals can encourage actions that are detri‐
mental to the organization. For instance, consider a high-performing sales team that
takes longer than a quarter to reach their collective goals versus a toxic sales team
that hits their quarterly targets but disregards data quality and rules. The former
creates a healthy working environment, whereas the latter leads to long-term costs
for the company through poor data and high employee turnover. It’s essential to
recognize that siloed incentives can encourage harmful behavior and undermine
broader organizational success.

Shortsightedness
A prevalent issue within data teams is the constant need to put out fires rather
than generate value. This reactive approach leads to frustration, unnecessary hiring,
technology investments, and rulemaking, and increases in training and integration
costs. The emphasis should be on identifying and addressing root causes of problems
from a holistic perspective and what is best for the organization as a whole in the long
term instead of on the symptom and the short term.

Indecisiveness
If you recall the most frustrating meetings you’ve attended, chances are they were
characterized by conflicting perspectives that stalled progress. Trying something,

128 | Chapter 7: The Science of Synchronization



even if it fails, provides an opportunity to learn, adjust strategies, and move forward.
Some managers, in an attempt to avoid taking sides or being wrong, let disagreements
fester or even instigate conflicts under the mistaken belief that individuals will “work
things out” independently. Such avoidance is draining and unproductive. Managers
should either guide conversations and decision making or delegate responsibility if
they are uncertain.

The Power of Design in Collaborative Networks
Performance drives success, but when performance can’t be measured, networks drive
success.

—Albert-László Barabási, The Formula (Little, Brown, 2018)

A golfer benefits from clear performance metrics, but organizations often lack such
straightforward measurements. In complex organizations, where collaboration across
teams and processes resembles the intricacies of art more closely than the simplicity
of a stopwatch, performance metrics can be elusive. If networks drive success in these
scenarios, how can we design these networks effectively?
The Galton board, illustrated in Figure 7-5, is a vertical board with pegs arranged in
a triangular pattern. When beads are dropped from the top, they randomly bounce
off these pegs, eventually forming a bell curve at the bottom. While each bead’s
journey seems unpredictable, the overall outcome is a predictable pattern. This board
demonstrates how numerous random events can culminate in an expected result.

Figure 7-5. A Galton board starts with a collection of beads at the top, and then they
randomly move through a series of pegs to form a Gaussian (bell) curve.

The Power of Design in Collaborative Networks | 129



The essence of the Galton board is not in the chaos of the balls but in the strategic
placement of the pegs. Each peg on the board subtly influences the trajectory of the
balls without dictating their exact path. In the same vein, in an organization, the goal
is not to control every decision but to set simple guiding principles that help shape
outcomes amidst the inherent unpredictability. Just as the pegs create a predictable
end pattern from random ball movements, these principles can help an organization
find order amidst apparent chaos.
In the realm of organizations, people and decisions are the moving beads, and the
guiding principles and framework are the pegs. Though you cannot predict every
decision an individual makes, you can provide guiding principles that shape the over‐
all outcome. Just as the Galton board transforms random bounces into a bell curve, a
well-structured organization can channel myriad decisions toward a collective goal.
The CLEAN data governance framework acts as those guiding pegs. It doesn’t micro‐
manage, but rather offers clear and accessible principles for everyone. By adhering to
these principles, individuals in an organization contribute to a harmonized objective,
turning seemingly chaotic decisions into a well-orchestrated dance.
With CLEAN, randomness gives way to rhythm, unpredictability molds into align‐
ment, and apparent chaos evolves into orchestrated success. All of this is achieved
through simple, shared standards that everyone understands and adopts. Such is the
transformative power of the CLEAN data governance framework.

Summary
This chapter explored the critical role of data governance in organizations and out‐
lined how the CLEAN data governance framework can empower data champions
to foster a successful, data-driven culture. Leadership plays an integral part in this
transformation, as leadership must enable data champions to work cross-functionally,
facilitating clear communication, fostering education, and transparently evaluating
implementation.
A pivotal concept covered here is thinking in networks and the fact that we can
visualize and leverage our data with knowledge graphs. Going further in our network
thinking, we can see how data, processes, and people interconnect and interact by
viewing the organization holistically through a collaborative learning network lens.
CLEAN is a simple, easy-to-learn template that provides a quick, holistic solution to
get your organization back on track.
Finally we looked at the common challenges that data champions face, the four
horsemen of data death: ignorance, siloed incentives, shortsightedness, and indeci‐
siveness. Addressing these impediments is crucial for achieving a data-driven orga‐
nization. The four horsemen of data death emphasize the necessity of decisive

130 | Chapter 7: The Science of Synchronization



leadership, holistic problem solving, alignment of individual and organizational
goals, and a willingness to learn and understand data management principles.
Drawing a parallel to the Galton board, a device that demonstrates the aggregation
of independent random events into a predictable pattern, we illustrated how organi‐
zations, despite their complexity, can achieve predictable and successful outcomes.
The CLEAN framework, like the pins on our organizational Galton board, provides
clear and accessible standards to guide individuals’ actions, resulting in patterns that
align with our collective objectives.
Through shared understanding and simple standards, we can find alignment and
success. That’s the power of network thinking and the promise of the CLEAN data
governance framework. In Chapter 8, you will learn how to continue your journey
of aligning your business, data, and code by using JSON Schema to validate and
annotate data given the information you learn through CLEAN data governance.

Summary | 131



CHAPTER 8
The Two Fundamental
Operations of Schemas

We cannot solve our problems with the same thinking we used when we created them.
—Albert Einstein

In Chapter 5, you learned the basics of JSON Schema and saw a practical way to put it
to use: generating web forms.
Schemas are powerful declarative artifacts that can elegantly solve a wide range of
problems in your data-centric innovation journey, often requiring a tiny fraction of
the code you would have to write otherwise. However, these opportunities are easy to
miss unless you have a good grasp on how to break down problems in terms of what
schemas can do for you.
In Chapter 4, we discussed that data alone does not make a well-designed data
product. It must be complemented with three other equally important facets: context,
structure, and meaning. Here, we will explore the two fundamental operations you
can perform using JSON Schema: data validation and annotation extraction (see
Figure 8-1), and how they relate to these four facets.
This chapter dives deep into these two fundamental operations and presents exam‐
ples you can use as inspiration for adopting schemas in your projects, or increasing
the amount of value you already get from them.

133



Figure 8-1. A JSON Schema is typically combined with a JSON instance to produce
validation results, annotation results, or both.

Like Chapters 2 and 5, this chapter is aimed at the more technical
audience interested in implementing data products. To continue
learning about data processes and knowledge frameworks in a
business sense, you can move to Chapter 9.

Validating the Structure of Data
Get the wrong data or bad data and suddenly the AI goes haywire.

—Steve MacLaughlin, best-selling author and Vice President of Data & Analytics at
Blackbaud

Validating JSON instances remains the simplest and most common use case for
JSON Schema. Whether you are developing a RESTful JSON-based API or defining
a JSON-based configuration file format, sooner or later you will face the validation
problem—How do I make sure that the input JSON data matches the structure I expect
in every case? With JSON Schema, a developer writes a declarative, language-agnostic,
constraints-based description of the data and makes use of a JSON Schema imple‐
mentation library to validate input JSON instances against it.
When performing validation, you are combining a JSON Schema with a JSON
instance to produce validation results.

134 | Chapter 8: The Two Fundamental Operations of Schemas



For the validation use case, the JSON Schema Validation, Applicator, and Unevalu‐
ated vocabularies provide a rich and easy-to-use set of keywords to model both
simple and highly complex recursive data structures. JSON Schema keywords for
data validation are referred to as assertion keywords that result in either successful
or unsuccessful validation. For example, if the type keyword is set to integer, the
assertion passes for integers and fails otherwise.
As introduced in Chapter 5, in JSON Schema parlance, a JSON document that repre‐
sents a JSON Schema is referred to as a schema, and a JSON document being valida‐
ted against a schema is referred to as an instance. For example: I’m validating many
instances against this schema. If an instance matches a schema, then the instance is
said to successfully validate against the schema.

Using an Online Validator
Experimenting with JSON Schema and validating your assumptions is the best way to
learn. We encourage you to try out the examples in this book using a modern online
JSON Schema validator. We recommend Hyperjump (see Figure 8-2), a popular
open source online validator maintained by a core member of the JSON Schema
organization. The user writes one or more JSON Schemas at the top-left area of the
screen and tests it against one or more JSON instances at the top-right area of the
screen. The bottom-left area of the screen provides information about the validity
of the JSON Schemas, and the bottom-right area of the screen provides information
about the validity of the JSON instances according to the given schemas.

Figure 8-2. An example of the Hyperjump online JSON Schema validator in use.

In comparison to other online JSON Schema validators, the Hyperjump validator
allows users to define more than one schema at the same time (see the + symbol

Validating the Structure of Data | 135



to the right of Schema). This is particularly useful for testing JSON Schemas that
reference other JSON Schemas.

Validation Example
Consider the following JSON Schema adapted from the official test suite, which
makes use of three object validation keywords defined in the Applicator vocabulary:
properties, patternProperties, and additionalProperties.
All of these keywords take other JSON Schemas as arguments. The properties key‐
word declares how a specific property must look if defined. The patternProperties
keyword is a variation of the properties keyword that defines how certain properties
whose keys match a specific regular expression must look if defined. Finally, the
additionalProperties keyword defines how any property not covered by either
properties or patternProperties must look:
{
  "$id": "https://example.com/validation-example",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "properties": { 
    "foo": { "type": "boolean" },
    "bar": { "type": "integer" } 
  },
  "patternProperties": { 
    "^v": { "type": "string" } 
  },
  "additionalProperties": {
    "type": "array"
  }
}

Consider the following instances against the previous schema:
{} 
{ "foo": 1 } 
{ "foo": true } 
{ "vvv": 1 } 
{ "baz": {} } 
{ "baz": [] } 
{ "foo": true, "vvv": "test" } 

 Valid. No property is defined.
 Invalid. The foo property must be a Boolean.
 Valid.
 Invalid. Properties that match the ^v regular expression must be strings.

136 | Chapter 8: The Two Fundamental Operations of Schemas



 Invalid. baz must be an array.
 Valid.
 Valid.

Note that if a property defined by properties also matches a regular expression
defined in patternProperties, then the property must successfully validate against
both the properties and patternProperties schemas.

JSON Schema as a Constraints Language
To make the best use of JSON Schema, it’s important to understand that it is not a
modeling language, but a constraints language. While this distinction is subtle, it has
philosophical ramifications that result in unexpected behavior for the unaware.
Formally, a schema can be thought of as the potentially infinite or potentially empty
set of entities it successfully describes (see Figure 8-3). An entity validates against a
schema if it belongs to the set of its matching instances.

Figure 8-3. A schema defined in terms of the set of its matching instances. A schema
has an infinite set of matching instances if any of its subschemas has an infinite set of
matching instances.

A schema language that adopts the modeling philosophy does not initially describe any
matching instance; these schemas start as the empty set. Under this philosophy, the
schema author starts defining what permissible instances look like. For example, the
schema author may declare a foo integer property so that only instances that have a
foo integer property—nothing more and nothing less—are considered valid.
A schema language that adopts the constraints philosophy starts at the other end of
the spectrum; these schemas start as the infinite set of all JSON instances. Under
this philosophy, the schema author defines what constraints are applied to matching
instances (as in Figure 8-4). For example, the schema author may declare that if the
instance is an object and declares the foo property, then this property must not be an
array. Schemas that are not objects, that do not declare the foo property, or whose foo
property is not an array, are still valid.

Validating the Structure of Data | 137



Figure 8-4. A series of JSON Schemas where each subsequent schema adds an additional
keyword indicated by a surrounding box. The presence of each additional keyword
imposes one further constraint to the set of matching instances, as described to the right.

138 | Chapter 8: The Two Fundamental Operations of Schemas



As the excellent post on the topic by Henry Andrews (coauthor of most modern
JSON Schema specifications) suggests, the key question to ask yourself when using
JSON Schema for validation purposes is, have I forbidden everything I don’t want?

Boolean Schemas
A JSON Schema is either an object or a Boolean. Any other type, like an array, would
never represent a JSON Schema. While you are probably aware of object schemas,
you may have used Boolean schemas without realizing they were indeed schemas. In
fact, Boolean schemas are aliases to certain special object schemas (see Figure 8-5).

Figure 8-5. The true Boolean schema matches every instance, whereas the false
Boolean schema matches no instance.

In JSON Schema, the empty object {} is the wildcard schema that successfully
validates against every instance. This schema is aliased to the Boolean schema true.
By the constraints philosophy, it follows that the use of any JSON Schema assertion
keyword imposes a new constraint on the schema. We can conclude that a JSON
Schema that does not make use of any keyword is a schema that successfully validates
against every JSON instance. Hence, the {} or true schema imposes no constraints,
and matches every JSON value. The set of its matching instances is infinite.
Naturally, if the Boolean schema true matches every possible instance, the Boolean
schema false is the schema that matches no instance. We know that true is an alias
to {}, but the definition of false is not as simple. The set of its matching instances is
empty, so in theory, any JSON Schema that represents unsatisfiable constraints (such
as a contradiction) is semantically equivalent to the false schema. For example, a
schema that successfully validates instances that are both a number and a string (an
impossible conjunction) is the false schema. In practice, the canonical definition of
the false schema relies on the not logical operator; JSON Schema typically refers to
the false schema as { not: {} }, the negation of the empty schema.

Validating the Structure of Data | 139



Think of true as the wildcard schema and false as the impos‐
sible schema. If you are authoring a schema that utilizes these
special schemas, you should use the Boolean aliases over their
object-based definitions. While both representations are semanti‐
cally equivalent, Boolean schemas are more concise and provide
a clear answer for removing the ambiguity of how to refer to the
impossible schema.

A common pattern to disallow extra properties that are not explicitly defined by a
schema, which we used in Chapter 5, is to set the additionalProperties keyword
to false. For example, the following is a JSON Schema that successfully validates
against objects that have a foo string property and a bar integer property, and no
other property:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "required": [ "foo", "bar" ],
  "additionalProperties": false,
  "properties": {
    "foo": { "type": "string" },
    "bar": { "type": "integer" }
  }
}

The prevention of object properties that are not defined by the properties keyword
is accomplished through the use of the additionalProperties keyword, which takes
as an argument a subschema that must successfully validate against every property
not covered by its object-related applicator keywords (like properties).
In the preceding example, our use of the additionalProperties keyword is telling
JSON Schema that every property not matched by the properties keyword must
successfully validate against the false schema. As we saw, the false schema is not
satisfiable. Therefore, every additional property fails validation, effectively prohibit‐
ing any additional property.

Heterogeneous Data Structures
Surprisingly to some, in JSON Schema, keywords that apply to specific data types
successfully validate against a value of a different type. For example, setting the multi
pleOf keyword to the value 2 ensures that a numeric instance corresponds to an even
number. However, multipleOf set to 2 successfully validates against any string value,
arbitrary object, or any other type of JSON instance that does not represent a number.
In JSON Schema, keywords that apply to a specific data type come with a hidden
implication—they are taken into account if and only if the instance is of the corre‐

140 | Chapter 8: The Two Fundamental Operations of Schemas



sponding type and are ignored otherwise. While this behavior is a common pitfall
for newcomers, it enables proficient schema authors to elegantly and concisely write
schemas that describe heterogeneous data structures.
For example, assume that you want to validate a JSON instance that is either a
positive integer, a string that represents a positive integer, or an array of at least
one positive integer. Using this capability, you can write a schema that successfully
validates instances that are either positive integers (using type and minimum), strings
that represent positive integers (using type and pattern), or nonempty arrays of
positive integers (using type, minItems, and items), as follows:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": [ "integer", "string", "array" ],
  "minimum": 0,
  "pattern": "^\\d+$",
  "minItems": 1,
  "items": {
    "type": "integer", 
    "minimum": 0
  }
}

If validation keywords that apply to specific data types would not successfully validate
against other data types, the preceding schema would have to be decomposed as a
disjunction that separately handles each case. The anyOf keyword enables you to
express the same schema in an arguably more verbose and nested manner:
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "anyOf": [
    { "type": "integer", "minimum": 0 },
    { "type": "string", "pattern": "^\\d+$" },
    {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "integer", 
        "minimum": 0
      }
    }
  ]
}

This JSON Schema is equivalent to the preceding schema, but the anyOf logic opera‐
tor keyword has been used to separately define each valid case.

Validating the Structure of Data | 141



The format Keyword
If you used older versions of JSON Schema, you might be familiar with the format
optional keyword introduced by the JSON Schema validation specification. This
keyword was originally created to define string-based logical data types. For example,
a schema writer may declare that a string property represents an IPv4 address by
setting the format keyword to ipv4. Back then, a fully adhering JSON Schema imple‐
mentation could optionally validate the content of the string against this predefined
logical type.
The Hyperjump online JSON Schema validation we have used so far no longer
supports older versions of JSON Schema. Figure 8-6 shows an example JSON Schema
Draft 4 that validates strings (through the use of the type keyword) that represent
email addresses (through the use of the format keyword) using the Newtonsoft
online JSON Schema validator.

Figure 8-6. In JSON Schema Draft 4, strings that do not represent valid email addresses,
like foo, fail validation.

In practice, it was hard for implementations to come into agreement about how to
validate textual input properly and efficiently according to these logical types. This

142 | Chapter 8: The Two Fundamental Operations of Schemas



would often result in inconsistencies between JSON Schema implementations. For
example, JSON Schema supports setting the format keyword to the regex logical
type. However, efficiently validating that a string is a valid regular expression and
unambiguously rejecting the ones that are not is not trivial. As a consequence,
implementations would sometimes make mistakes and not provide uniform results
with respect to other implementations.
As a solution, modern JSON Schema dialects (starting from 2019-09) introduced a
breaking change—the format keyword would no longer be an assertion keyword by
default. The format keyword still has its use to describe the semantics of data (as you
will see in the next section) but does not play a role in validation anymore.
For example, setting the format keyword from the Format Annotation vocabulary to
email no longer allows implementations to make sure the string actually corresponds
to an email (see Figure 8-7). If schema writers require validation of the content of a
string, we recommend making use of the pattern keyword. In this way, the schema
writer is explicit about how implementations are meant to validate their strings.

Figure 8-7. This is a modern JSON Schema 2020-12 counterpart to the JSON Schema
Draft 4 email validation schema in Figure 8-6. In comparison to the previous Draft
4 schema, JSON strings that do not represent email addresses now pass validation by
default.

In comparison to Format Annotation, JSON Schema 2020-12
defines a vocabulary called Format Assertion that introduces a for
mat used by default by the official JSON Schema dialect. Not many
implementations support it. To use it, you also have to define your
own JSON Schema dialect, a topic we will cover in Chapter 13.

Validating the Structure of Data | 143



Using Annotations to Define Meaning
To a collector of curios, the dust is metadata.

—David Weinberger, author and proponent of the philosophical and ethical
implications of machine learning

Now, we turn our attention to the other half of the equation: defining the meaning
behind certain parts of your JSON instances. This is done using a capability called
annotations.
JSON Schema keywords that provide additional information without affecting vali‐
dation are called annotation keywords. These keywords associate specific parts of
your data with specific bits of metadata. The software application making use of
JSON Schema can choose to collect these annotations and act on their values as the
application sees fit.

The JSON Schema Meta Data, Format Annotation, and Content
vocabularies provide a rich set of annotation keywords. We encour‐
age you to consult the documentation of these vocabularies to
check which annotation each keyword produces.

Annotation Extraction Example
A common use case for JSON Schema, briefly explored in Chapter 5, is to generate
web-based forms that result in data that matches the expected format. These form
generators typically implement support for commonly used annotation keywords to
decorate the resulting form. For example, the React JSON Schema Form open source
project collects the value of the readOnly annotation keyword to set input elements as
disabled (see Figure 8-8).

144 | Chapter 8: The Two Fundamental Operations of Schemas



Figure 8-8. An example form generated using the React JSON Schema Form online play‐
ground. Setting the annotation keyword readOnly in the firstName property results in
a disabled text input. Similarly, you can see how the title annotation keyword affects
the human-readable labels of each generated input, and how the default annotation
keyword populates the form with prefilled information.

A Simple Use Case: Deprecations
A simple example of an annotation keyword is the deprecated keyword. Setting it
to true for a specific property does not affect validation; it simply signifies that the
property will likely be removed in the future and users should not set it anymore.
How you react to the presence of this keyword is up to you and the needs of your
application. You may log a warning, present an alert, or simply ignore it. Figure 8-9
shows a basic example of deprecated in action.

Using Annotations to Define Meaning | 145



Figure 8-9. An example schema that defines two properties: name and address. The
address property is marked as deprecated, but this does not make addresses invalid.
Instead, the system that evaluates this schema can decide how to interpret this depre‐
cated annotation as it sees fit, or simply ignore it.

Imagine you are tasked to write an API that processes JSON instances that match
the schema in Figure 8-9. When processing incoming instances, you might want to
report a warning if the client is setting the deprecated address property. A simple
way of accomplishing this task is to statically check the schema by manually consult‐
ing the value at /properties/address/deprecated.
For example, if you are using JavaScript, you might write code like this to log use of
the deprecated property to standard error:
if (instance.address && schema.properties.address.deprecated) {
  console.error('The instance sets the address deprecated property');
}

Runtime Extraction
While statically traversing the schema for instances of the deprecated keyword
works in this simplistic case, reality is often more complicated. For schemas that

146 | Chapter 8: The Two Fundamental Operations of Schemas



make use of advanced features such as logic operators, conditions, or recursive
definitions, determining which annotations apply where using only the schema can
be impossible. See Figure 8-10 for an example.

Figure 8-10. An example schema that defines an object with two properties: type and
data. The schema uses different annotation keywords (from the Meta Data vocabulary)
for the same data property depending on the value of the type property. If the type
property is set to secret, then data is considered write-only. If the type property is
set to label, then data is considered read-only. Otherwise, data is considered to be
deprecated.

Using Annotations to Define Meaning | 147



With a schema such as the one in Figure 8-10, the resulting annotations directly
depend on the instance itself. Unlike in the simpler example in Figure 8-9, it is
impossible to determine which annotations apply without also looking at the instance
data. Furthermore, writing logic to handle this case would involve the daunting task
of producing a complete JSON Schema implementation.
To solve this problem, JSON Schema supports a standardized mechanism for extract‐
ing annotations at runtime, handling all of this complex logic for you. This interop‐
erable runtime annotation collection mechanism is based on the standard output
formats of JSON Schema.

A Note on Performance
Collecting annotations can impact runtime performance. For example, if you collect
annotations on a JSON Schema that makes use of the anyOf applicator, the imple‐
mentation is forced to evaluate the instance against every disjunction in the anyOf
applicator. In comparison, if not collecting annotations, implementations may stop
evaluation as soon as one anyOf subschema successfully validates against the instance.
In the interest of runtime efficiency, we recommend collecting annotations only if
your use case demands it.

Standard Output Formats
Annotations are only useful if an application using JSON Schema can read them back
in a convenient manner. For example, it is often useful for an application to list any
deprecated object subproperties. To increase interoperability among applications that
use JSON Schema, the JSON Schema Core specification standardizes what the output
of a compliant JSON Schema implementation should look like. The standard output
formats defined by JSON Schema are JSON based and offer a clear interface for how
annotations can be read and used by an application.

As you would expect, the JSON Schema organization publishes a
JSON Schema that validates and describes JSON Schema output
instances.

JSON Schema is utilized across use cases that demand different output requirements.
For this reason, JSON Schema specifies different output formats with different levels
of detail. At the time of writing, these formats are flag, basic, detailed, and verbose. A
comprehensive study of every standard output format is outside of the scope of this
book, as some of them are often only useful for JSON Schema implementers.

148 | Chapter 8: The Two Fundamental Operations of Schemas



Not every JSON Schema implementation out there supports every
standard output format. In fact, implementations are only recom‐
mended to support standard output formats. Make sure to select a
JSON Schema implementation that provides the output format that
best suits your application.

For simplicity, this book will adopt basic as our preferred output format. This is an
example output of unsuccessful schema validation using the basic standard format.
The output will contain an object for every error (if any) or annotation (if any)
encountered during evaluation:
{
  "valid": false,
  "errors": [
    {
      "keywordLocation": "",
      "instanceLocation": "",
    }
    {
      "keywordLocation": "/properties",
      "instanceLocation": ""
    },
    {
      "keywordLocation": "/properties/id",
      "instanceLocation": "/id"
    },
    {
      "keyword": "/properties/id/type",
      "instanceLocation": "/id"
    }
  ]
}

Every standard output format other than flag can produce detailed error and anno‐
tation information. When collecting annotations, the basic standard output format
will include a top-level annotations array that includes the annotation value, the
keyword that originated it, and where in the instance the annotation was produced.
Applications may loop over this array and consume the annotations in any way they
see fit.
Consider the following JSON Schema that uses the title annotation keyword from
the Meta Data vocabulary:
{
  "$id": "https://example.com/schema-with-title",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "My Schema"
}

Using Annotations to Define Meaning | 149



A compliant implementation would collect My Schema as an annotation value for the
title keyword. This is how it would look when making use of the basic standard
output format:
{
  "valid": true,
  "annotations": [
    {
      "valid": true,
      "keywordLocation": "/title",
      "absoluteKeywordLocation": "https://example.com/schema-with-title#/title",
      "instanceLocation": "",
      "annotation": "My Schema"
    }
  ]
}

Revisiting the format Keyword
Previously, we discussed that the format keyword was historically an assertion key‐
word used for validation purposes. Now, under the official JSON Schema 2020-12
dialect, it is only used to indicate what data type such value represents. Using the
knowledge we’ve acquired in this section, we can say that currently the format
keyword only produces an annotation.
For example, consider the following schema that uses the format keyword to declare
that the instance represents a URI:
{
  "$id": "https://example.com/format",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "string",
  "format": "uri"
}

While a compliant JSON Schema will not check that the given string indeed repre‐
sents a URI, it will collect an annotation for the format keyword with a value that
equals the declared format, uri:
{
  "valid": true,
  "keywordLocation": "/format",
  "absoluteKeywordLocation": "https://example.com/format#/format",
  "instanceLocation": "",
  "annotation": "uri"
}

150 | Chapter 8: The Two Fundamental Operations of Schemas



Using an Online Validator
Many online JSON Schema validators skip presenting validation output in standard
formats in favor of human-consumable visual results. To play with annotations, we
recommend choosing an online validator that directly presents output results. One of
those is json-everything (see Figure 8-11), the playground of a popular JSON Schema
implementation for C#.

Figure 8-11. An example of the json-everything online JSON Schema validator in use.
The user can write one JSON Schema at the top-left area of the screen and provide one
JSON instance at the top-right area of the screen. The bottom area of the screen shows
the validation result, including collected annotations, according to the selected standard
output format.

Thinking in Schemas
We say that JSON Schema is a universal data language because it is capable of both
defining structure (through the data validation operation) and conveying context
and meaning (through the annotation extraction operation). With JSON Schema, the
pinnacle of a data product occurs while evaluating the schema against an instance,
which unifies data, structure, and metadata.
Most users only use JSON Schema as a data validation tool, solely considering the
data and structure facets. However, as you will see in Chapters 12–14, using annota‐
tions to gather context and meaning can open up interesting use cases.

Thinking in Schemas | 151



Summary
This chapter explored in depth the two operations you can perform using JSON
Schema, data validation and annotation extraction, and demonstrated how JSON
Schema relates to the four facets of a data product (introduced in Chapter 4). By
now, you know how to identify real-world data problems that can be broken down
in terms of these two operations, allowing you to solve them in declarative and
simplified ways.
In the next chapters, you will learn how to use annotation and validation keywords
to create a unified concept library for your organization, which will enable you to
view concepts in both business-friendly language and in technical representations
that your code and data teams can utilize.

152 | Chapter 8: The Two Fundamental Operations of Schemas



CHAPTER 9
Illuminating Pathways of Acceleration

Imagine a room bathed in the unfocused light of a 150-watt bulb, its beams scattering
in all directions without a clear aim. Now, visualize that same energy concentrated
into a laser beam capable of slicing through metal (see Figure 9-1). This shift from
diffuse illumination to targeted intensity embodies the transformative power of align‐
ment in innovation. It’s a lens through which we can understand how harmonizing
goals, processes, and technologies can propel us forward toward success.

Figure 9-1. A laser epitomizes alignment, emitting light in a perfectly synchronized,
focused beam. A light bulb generates unsynchronized light, resulting in a scattered glow.

The power of alignment is epitomized by this comparison. Whether in the realm of
physics or the world of business, the focus and direction of energy can manifest vastly
different results. A scattered approach results in diluted influence, while a purposeful,
laser-like focus can drive transformation that reshapes entire industries.
In previous chapters, we’ve explored tools that identify and eliminate misalignment.
From harmonizing language through the concept compass and its data representa‐
tions using JSON Schema to synchronizing rules of effective collaboration across
business operations with CLEAN governance, we’ve laid the groundwork for what’s

153



next: the accelerate phase of unifying, where you sharpen your organization’s innova‐
tion into a laser.
Getting people to align on the meaning of concepts is one thing. Achieving agree‐
ment on what success looks like and what direction to focus efforts in, especially
when there are competing ideas, is an entirely different task.
This chapter explores how to transform scattered organizational efforts into a stream‐
lined and focused powerhouse. You’ll gain a nuanced understanding of how intercon‐
nected elements shape an organization’s journey toward aligning processes, decisions,
and key insights.
This chapter will teach you how to apply data hygiene, as first defined in “What You
Can’t See Can Kill You, and the Same Is True for Data” on page xii, to better align
processes, decision making, and language, and to reveal where alignment is needed.
You want to eliminate the following:
Ambiguity

Uncovering the elusive and often unclear aspects that can hinder progress and
learning how to navigate them to improve clarity and direction

Knowledge gaps
Identifying what specific knowledge is needed to make progress and understand‐
ing how gaps in knowledge can create barriers to success

Blind spots
Gaining the self-awareness to begin mapping how ambiguity and knowledge gaps
might be impacting processes and progress toward goals

You will learn the following tools in this chapter:
Process maps

Creating detailed visuals to show progress and decision-making processes
Annotating process maps

Overlaying annotations such as risk, ambiguity, and pain points on process maps,
for a comprehensive perspective on organizational challenges and opportunities

From simple questions to complex organizational strategies, we will uncover layers
of ambiguity, knowledge gaps, and pathways to success. Whether you are a data
champion in a small startup or a large corporation, this chapter will equip you
with the skills to align your business, data, and goals into a laser-focused strategy,
transforming the mundane into the extraordinary.

154 | Chapter 9: Illuminating Pathways of Acceleration



How Ambiguity, Knowledge Gaps, and Blind Spots
Influence Decisions and Progress Toward Goals
Ambiguity, knowledge gaps, and blind spots are three often invisible forces within
organizations that pose insidious threats to effective decision making. They can create
costly misunderstandings that steer actions and outcomes away from intended goals
and weakening overall strategy.
The root causes of organizational mistakes often manifest in downstream processes
but are caused by misalignment upstream. Identifying this misalignment is a complex
task because ambiguity tends to be invisible and can grow more intricate and abstract
over time. When compounded by frequent miscommunication, this situation can
quickly deteriorate.
Let’s explore ambiguity and knowledge gaps a bit further in Figure 9-2:
Ambiguity

This hides true intent, fostering mistakes. It’s like using different phrases to
describe the same thing or using the same phrases to describe different things.

Knowledge gaps
These are literal missing pieces of information needed to achieve a goal. Without
them, it becomes difficult to take actions with confidence.

Figure 9-2. Ambiguity can be described as using different phrases to describe the same
thing or using the same phrases to describe different things. Knowledge gaps are missing
pieces of information needed to achieve a goal, making it difficult to take actions with
confidence.

How Ambiguity, Knowledge Gaps, and Blind Spots Influence Decisions and Progress Toward Goals | 155



The third dimension of data hygiene is blind spots, which is when an organization has
unknown unknowns. As long as teams and leaders are not aware of what knowledge
they need to succeed, and where ambiguity is increasing the risk of assumptions and
mistakes, executing the desired intent toward an outcome with reliability is difficult.
One important skill to develop as a data champion is recognizing where ambiguity
and knowledge gaps exist in decision-making processes and understanding their
impact on data.

Which Is Bigger: Greenland or the US?
Building on what you’ve learned about ambiguity and knowledge gaps, let’s explore
how these concepts play out in a seemingly simple question (see Figure 9-3): which is
bigger, Greenland or the US?

Figure 9-3. A flattened map is a data representation that distorts geographic scale.

This example may seem straightforward, but it contains several hidden layers of
ambiguity:

• Ambiguity in data. Transforming the earth, a three-dimensional sphere, into a
two-dimensional representation like a map necessitates certain compromises.
This transformation is known as a map projection. There are various methods of
map projection, and each method represents the surface of the earth differently.
The Mercator projection, for instance, preserves angles and shapes but distorts
the size of land masses as they get farther from the equator. As a result, Green‐
land, which is much closer to the North Pole, appears significantly larger on a
Mercator map than it is in reality, making it seem comparable in size to the US
even though it’s considerably smaller. When you look at a map, are you aware of

156 | Chapter 9: Illuminating Pathways of Acceleration



the projection method used? Would you recognize how the distortion caused by
the map projection method changes the representation?

• Ambiguity in decisions. Even if you recognize the distortion, ambiguity still exists;
you may not know whether the question refers to geographic or visual size, or
whether anyone knows how to convert between the two.

• Ambiguity in goals. Understanding the map’s distortion and the question’s refer‐
ence to geographic size may not be enough. What if success in this scenario
requires identifying the many layers of ambiguity—not arriving at the “right”
answer, but comprehending the complexity of the question itself?

No matter how the question of which is bigger is answered, the ways this question
could be answered and how success could be measured are many. This ambiguity may
have been invisible to you until now, as it often is in organizations. Documents are
shared, questions are asked, decisions are made, performance is judged, all without
recognizing the underlying ambiguity.

Mapping Pathways of Processes and Progress
Just as a well-planned itinerary helps ensure a seamless trip, defining decisions
and measuring progress through process maps is crucial for achieving favorable
outcomes. Success requires understanding the “location” of each decision point,
connecting them into a coherent path, and empowering the organization to move
with purpose and agility, reduce inefficiencies and promote innovation.

Measuring Progress Toward Goals
In real estate, the phrase “location, location, location” underscores the paramount
importance of a property’s surroundings, which can often overshadow other consid‐
erations. Similarly, in the realm of decision making and achieving success, “knowing
where you stand” relative to the desired outcome—having a clear sense of direction
and knowing how far you’ve progressed toward your goals—is essential. As a data
champion, one of your core objectives is to dispel ambiguity around organizational
goals. This involves understanding the measures and metrics of success and ensuring
clear and reliable communication between various business functions and those
success metrics.
Suppose your team aims to get sign-ups for a subscription service via a series of web
pages. Without a clearly defined user flow, pinpointing where potential subscribers
drop off, much less conducting analytics or formulating strategies for improvement,
would be a daunting task.

Mapping Pathways of Processes and Progress | 157



Defining Decisions and Steps with Process Maps
A process map is more than a diagram; it’s a strategic tool that unveils the dance of
activities within a process. Think of it as a choreographed routine, where each step,
decision, and outcome must flow harmoniously.
Process maps don’t just depict the sequence; they reveal the stakeholders’ unique
perspectives, the critical decisions, and the potential roadblocks. This transparency
is essential for pinpointing inefficiencies and paving the way for innovation and
improvement. Figure 9-4 provides an example of this choreography at play.

Figure 9-4. This process map illustrates the sequential events in a workflow. Decision
nodes, depicted as diamonds, guide the flow through yes/no or true/false pathways. The
current location within the process is referred to as the state.

Process maps generally have at least four types of nodes:
Start/end nodes

These nodes represent the start or the end of a process.
Process nodes

These show a task or operation that needs to be performed.
Decision nodes

These represent a point in the process where a decision needs to be made that
will determine the next step(s). These are usually a diamond shape.

Current state
This is the node that represents where a system or person is in a process. It serves
as a beacon, enabling efficient navigation through complex workflows.

158 | Chapter 9: Illuminating Pathways of Acceleration



Process maps often require significant time to create and are useful in fields like
software development or UX design. However, they may not suit the data champion’s
needs in identifying ambiguity and misalignment in intent, data, decisions, and goals.
As businesses adapt and change to situations, managing and updating process maps
can require significant work. Therefore, unifying’s relationship with process maps
is specific to addressing ambiguity, knowledge gaps, and creating alignment—not
in documenting or architecting processes, unless it becomes necessary for the data
champion to include formal process mapping and/or process documentation (also
called “work instructions”) in their scope.
In unifying, we recommend that you use the process mapping approach with a set of
very specific contexts:
Learning quickly

In consulting, process maps are a great way to quickly get a snapshot of how
a business unit runs. Focus on your ability to understand process inputs and
outputs, rather than creating perfect documentation.

Revealing ambiguity
If you ask five people to define the processes of a business function and you get
five different maps, then that is a pretty good indicator of ambiguity.

Identifying knowledge gaps
If people can’t explain how processes work, that is an indicator of knowledge
gaps. Ambiguity in processes will reveal your own knowledge gaps, the explora‐
tion of which will help you understand the organization’s challenges.

Highlighting misalignment
If there are pain points, problems, and challenges in processes where expecta‐
tions and operational reality aren’t aligned, being able to pinpoint their locations
is valuable.

How Process Maps Reveal Ambiguity
Imagine yourself as a data champion on a new innovation project for the customer
service department that handles order fulfillment. You get a list of stakeholders to
interview to examine opportunities for optimizing the business with a data-centric
approach. The first interview reveals a process map that mirrors Figure 9-4. However,
the second interview reveals a process map that looks like Figure 9-5.

Mapping Pathways of Processes and Progress | 159



Figure 9-5. This process map shows that before attempting to fulfill an order, the
customer service representative checks whether the customer has outstanding debt more
than 90 days old and, if so, blocks the order from proceeding.

The difference between these two process maps is that the second map has intro‐
duced two nodes: a decision node as to whether the customer has debt that has not
been paid for at least 90 days, and a process node to block the order if the answer is
yes. Even though you are new to the project and have a knowledge gap of how debt
logic and order blocking rules work, because the two stakeholders gave you different
process maps, you can see there is ambiguity between the two process definitions and
knowledge gaps have surfaced that you need to resolve. This is similar to the concept
compass in Chapter 6. You need to harmonize the business logic, processes, and data
representations.

Visualizing and Removing Ambiguity in Processes
At the start of your illumination journey, we likened the process to the transforma‐
tion of a light bulb’s diffused energy into the focused power of a laser beam. As
you gather, analyze, and reconcile different process maps, think of this endeavor
as refining that diffused understanding—scattered like a light bulb’s rays—into a
precise, laser-focused insight. As you remove ambiguity and fill knowledge gaps,
you’re sharpening that beam, ensuring your organization’s efforts are as effective and
impactful as that laser cutting through steel.

160 | Chapter 9: Illuminating Pathways of Acceleration



It is helpful to quantify exactly how many different process maps you get from
the stakeholder interviews. Consider the following results from three different stake‐
holder groups:
Process map 1

Contains 1 decision node and 4 process nodes.
Process map 2

Contains 2 decision nodes and 6 process nodes.
Process map 3

Contains 5 decision nodes and 10 process nodes.
In the align phase of unifying, we were able to use a spreadsheet to highlight exactly
where and how many concepts were misaligned. In the accelerate phase we aim to do
the same for processes, decisions, and goals, and provide the insights you collect on
misalignment, ambiguity, and knowledge gaps in a simple visualization, as shown in
Figure 9-6.

Figure 9-6. A simple visualization shows that the three process maps of the same business
function have different numbers of decision and process nodes.

As the data champion, you now have demonstrable ambiguity to investigate and a
knowledge gap as to why they are different. Therefore, you need to dive deeper to
clarify any differences to make a proper map, which in our hypothetical situation,
might look like Map 4 in Figure 9-7.

Visualizing and Removing Ambiguity in Processes | 161



Figure 9-7. Your own perspective, after shadowing stakeholders in their workflows, might
reveal vastly different process maps that might be combined into one in order to align
everyone to formal process management.

Enriching Process Maps with Annotations
Process maps may also reveal ambiguity and knowledge gaps, particularly when
they uncover problems or challenges. Unifying recommends an annotation-based
approach using a few common and useful symbols (see Figure 9-8), but you can
create your own symbols and icons as you see fit:
Pain points

A frustration in a process—a fire icon
Blocks

A user/employee is unable to complete a task—a brick or wall icon.
Uncertainty

Information or results are unreliable—a question icon
Risk

A dangerous situation—a bomb icon
Value

A particular point that is highly valuable

162 | Chapter 9: Illuminating Pathways of Acceleration



Figure 9-8. This process map uses the annotation icons to identify where pain points,
blocks, uncertainty, risk, and value-related issues are expressed during stakeholder inter‐
views. Pain levels are rated a 5 in this example (see the Tip on levels).

We recommend adding levels to annotations when possible. For
example, a pain point level 2 (on a scale of 1–5) is different from
a pain point level 5. The differences between described pain point
levels of stakeholders can help identify priorities, showing where
ambiguity needs to be explored further.

Process Maps Reveal Innovation Opportunities
In Figure 9-8, risk is shown in the “Check customer balance” node because the
stakeholder revealed that not everyone checks the balance and that it is hard to find
on the screen. This insight reveals the opportunity to create a user experience that
highlights the customer balance.
The “Has debt?” decision box has a question icon, which in our hypothetical example,
indicates that the accounting system updates the debt warning only once a month.
Therefore, if someone calls to place an order before the debt calculation update, the
customer service rep doesn’t know if they are viewing accurate information or not.
The brick/blocked icon reveals that the customer service representatives feel that they
have no way to verify whether an item is in stock, so they are blocked from providing
a high-quality experience for customers.
Lastly, the stakeholders said that if the system could provide alternatives to items
out of stock instead of relying on memory, selling more merchandise would be a lot
easier.

Visualizing and Removing Ambiguity in Processes | 163



Summary
As data champion, your role is to serve as a beacon, guiding others in your organiza‐
tion toward success with a data-centric approach of uncovering layers of complexity
and navigating them with clarity.
In this chapter, we took a journey that illuminated pathways to acceleration, revealing
the transformative potential of alignment and the power of focusing energy, akin
to transforming a diffuse light bulb into a laser. This analogy set the stage for our
exploration of the complex world of business, where the focus and direction of
energy can reshape entire industries.
Data champions use strategies to navigate the often invisible barriers of ambiguity
and knowledge gaps, which can hinder progress. You’ve learned the art of process
mapping: connecting decision points into coherent paths and providing tools to
measure progress toward goals. You also learned how to enrich process maps with
annotations, identifying pain points, blocks, uncertainty, risk, and value, and how
these insights can pave the way for innovation.
As a data champion, you are equipped with actionable tools to question, identify,
and eliminate ambiguity and knowledge gaps, transforming scattered efforts into a
focused, potent force to accelerate innovation.
In Chapter 10, we’ll look further into illuminating pathways of acceleration. You’ll be
introduced to success spectrums—an approach that seamlessly blends process maps,
KPIs, and success metrics while placing knowledge at the forefront.

164 | Chapter 9: Illuminating Pathways of Acceleration



CHAPTER 10
Spectrums of Success

In Chapter 9, we embarked on a journey to illuminate the path from ambiguity to
clarity, transforming the diffuse light of a bulb into the sharp focus of a laser. The
transformational nature of unifying allowed us to identify and address ambiguity in
processes and decision points, leading to stronger and more effective collaboration.
Building on the metaphor of light from Chapter 9, we introduce success spectrums
in this chapter—a colorful, linear progression that symbolizes the various stages of
success in an innovation project.
Much like the visible light spectrum progresses through the colors from red to violet,
the spectrum of success represents a journey. It’s a pathway where each color signifies
a different state or stage of success, guiding us from the initial idea to the realization
of our goal.
In this chapter you will learn strategies and tools for:
Success spectrums

Continuums that define the current state to the ideal future, with clear, sequential
steps

Structured knowledge
Defining objectives, knowledge, and progress in a way that can be computation‐
ally implemented

Problem landscapes
A tool to visualize and prioritize organizational challenges

Creative thought experimentation using “demons”
A novel way to explore possibilities and break barriers in innovation

165



An Introduction to Knowledge Frameworks
Consider a hypothetical situation: you are a student in college, taking a math test
that has a single question. If you successfully answer the question, you pass the
course and graduate. If you don’t, you fail and you don’t graduate. A common
educational approach involves using a knowledge framework, which consists of four
main elements: knowledge objectives, assessments, knowledge gaps, and interventions.
Each of these components plays a crucial role in the learning process, as visualized in
Figure 10-1:
Knowledge objectives

These state that the objective is for a learner to demonstrate mastery of a sub‐
ject, such as multiplication. Knowledge is the information you need to solve a
problem.

Assessments
These are questions designed to quantify how knowledge will be evaluated in a
standardized way. By what measure and metrics (e.g., verbal definitions, analyti‐
cal evaluation of a thought process) will knowledge be evaluated?

Knowledge gaps
The information that a learner does not know and is preventing them from
moving forward. For example, if someone doesn’t know how to add negative
numbers, then multiplying negative numbers will not be possible.

Interventions
The information the learner needs in order to fill the gap and propel them
forward.

Pathways
A collection of learning objectives, assessments, and interventions.

Ambiguity in how questions are asked, in how progress is evaluated, and in what the
person already knows, that makes it difficult to have robust and reliable knowledge-
centric approaches to solving problems. The more complex the knowledge, the more
important it is to understand the relationship between ambiguity, knowledge, deci‐
sions, and goals.

166 | Chapter 10: Spectrums of Success



Figure 10-1. A knowledge framework is a sequence of knowledge objectives, assessments
of knowledge, identification of knowledge gaps, and interventions to move learners
toward success. The constructed continuum from start to success in a knowledge frame‐
work can be called a pathway. The final state of completing all knowledge objectives can
be called a purpose.

Knowledge Experiences and Pathways
The benefit of this knowledge-centric approach is its versatility. Almost any goal,
whether in education or business, can be broken down and understood through
this lens. In the business world, goals often reference objectives and key results
(OKRs). These can be combined with the principles of a knowledge framework,
which emphasizes the creation and collection of data about essential assessments,
gaps, and interventions.
Knowledge frameworks can also be used for objectives for developing a data model,
for a customer’s user journey, for an entire data team in delivering features, or for an
entire organization. They can be represented in a document and represented as code.
The example in Figure 10-2 represents a user experience flow across five states, or
unique points in a continuum of the user’s journey.

An Introduction to Knowledge Frameworks | 167



Figure 10-2. A series of objectives in a web application to get a user to give their personal
information in a sign-up process.

In each stage of the user flow, specific objectives guide the user to advance to the
subsequent stage. Each experience is crafted to propel the user closer to the overall
organizational goal. What sets knowledge experience design apart from traditional UX
design is a simple set of rules. Though many top-notch design teams employ similar
strategies, they—and the product and engineering teams they collaborate with—don’t
necessarily apply an agreed-upon standard.
The advantage of adopting a systematic approach to design through a knowledge
framework is that it allows for a formalized, standardized set of guidelines that
guarantees a consistent and effective user experience at every touchpoint in the flow:
KPIs

The efficiency (how fast) and efficacy (how much) of the experience in progress‐
ing the user toward the next state from the current. While simple analytics can
capture the raw data, true power comes from correlating this data to individual
user experiences. Advanced systems may even allow for the integration of per‐
sonalized analytics as opposed to aggregate metrics. The key is taking the time to
define what is being measured and to understand what different teams need to
measure in order to optimize their efforts.

System state
Reflects the specific choices made by the system to guide the user’s experience.
For instance, in an A/B test, the system state could refer to whether the user
is presented with a green or blue call-to-action button. More advanced systems
might dynamically present different experiences and options to users based on
predictive engines for personalization.

User state
Captures the real-time status of the application based on the user’s most recent
actions. For example, if a user clicks a button, this state will store a history of that
action, allowing for more targeted future interactions.

Objectives
Clearly defining the goal of each state (also referred to as a stage) in the pathway
that will move the user toward the next state, and the overarching goal of the
pathway itself.

168 | Chapter 10: Spectrums of Success



Success criteria
Employ computational logic to clearly define the conditions (true/false) that
determine whether a user has successfully transitioned from one state to another.
In OKR language, success criteria can be considered a key result.

Value
The rationale of the benefit gained by the system when a user successfully com‐
pletes a stage in the user flow in a particular order. This benefit is not uniform
across all stages; some stages may offer higher value to the organization than
others. To optimize the value created by the user experience flow, it’s crucial
to identify and prioritize these high-value stages and their relationships. This
involves assessing what specific objectives, actions, or data collection points are
most valuable to your organizational goals so that you can focus on enhancing
those parts of the user journey.

A Tool for Designing Knowledge Experiences
Creating a knowledge experience is more than just designing a user interface; it’s
about eliminating ambiguities and aligning your design, data, development, and
product teams. The knowledge experience format offers a structured approach, mak‐
ing the requirements transparent for every team involved. This, in turn, fosters
effective collaboration.
Benefits of this approach:

• Eliminates ambiguity in user flow definitions
• Facilitates alignment across various departments
• Streamlines communication of requirements to technical teams

The template for creating a knowledge experience consists of the following elements:

• Pathway name: Name of the user flow
• Pathway objective: The overall goal of the pathway

— State. The name of the state and the details it contains are as follows:
— Objective: What the user or system is expected to achieve
— Success criteria: Conditions to be met for successful state transition
— KPIs: Metrics to measure performance
— User state: Data that captures the user’s current state
— System state: Information about the system’s current configuration
— Value: Importance of this state in the overall user flow

An Introduction to Knowledge Frameworks | 169



— State X
— …

— State Y
— …

Reviewing the simple example of a sign-up form in Figure 10-2, this is what our
simple knowledge might look like. It can be written down in a bulleted list or spread‐
sheet, making it easy for technical teams to convert to JSON and JSON Schema.

It is often helpful to write objectives in the user story format com‐
monly used in Agile, “as a user I can,” “I can,” or “user can,” etc. For
a system action, terminology might be “system can,” “application
can,” etc.

• Pathway name: Sign-up flow
• Pathway objective: Minimize the speed to complete and maximize completion

rate.
— State 1

— Objective: A user can click the sign-up button.
— Success criteria: Button has been clicked.
— User state: User has gone to the sign-up page URL. Note: this is usually

captured in data, but defining what will be captured as state is important.
— System state: Design ID of which style was used (green or blue button, for

example).
— KPIs:

— Efficiency
— Efficacy
— Anything else to measure…

— Value: Initiates journey…
— State 2

— Objective: User can provide contact information.
— Success criteria: First name, last name, and email address filled.
— …
— Value: Contact information is critical, otherwise we don’t know what the

other data belongs to.

170 | Chapter 10: Spectrums of Success



From Structured Knowledge to Computational Knowledge
Translating user experiences and flows into a structured knowledge format brings a
multitude of advantages that transcend traditional design and data paradigms. One
of the most transformative benefits is how it enables computational intelligence to
flourish.
Structuring knowledge experiences will enable your technical teams to implement
them in JSON, paving the way for truly automated and adaptive user experiences.
This digital foundation allows for real-time changes to be made in the user interface
based on the current state, objectives, and variables defined in the framework.
The structured nature of the framework sets the stage for cutting-edge analytics and
offers these benefits:
Ability to measure efficiency and efficacy

Well-defined KPIs are seamlessly integrated into the analytics systems, facilitating
real-time adjustments for greater efficiency and efficacy.

A/B testing capability
The JSON structure streamlines the setup for A/B testing at various stages,
making it easy to gather data on what interventions are most effective.

Ease and compatibility
The JSON format can be utilized between development, data, and data science
teams, and offers a shared format for designing data, experiences, and business
logic in a uniform manner.

Personalized experiences
Machine learning models can easily interpret the structured data to provide
personalized user pathways and adapt in real time based on individual user
behavior.

Predictive analytics
Advanced analytics tools can forecast user behavior or preferences, allowing you
to predict and prepare for different user pathways.

Optimization algorithms
AI can identify the shortest or most efficient paths through the user experience,
automatically updating the pathway in real time to guide users more effectively
toward their objectives.

As you’ll see in Chapter 15, the real magic happens when this structured knowl‐
edge framework interacts with computational algorithms, enabling computational
knowledge. This sets the stage for AI-driven analytics that not only interpret user
behavior but also predict future actions, further streamlining and enhancing the user
experience.

An Introduction to Knowledge Frameworks | 171



Success Spectrums
Progress bars and process maps do not tell us how “good” or “bad” something is
or why; they merely show how something changes and progresses through available
options. KPIs are specific, numerical metrics used to track vital measures such as
revenue, profit margin, or customer satisfaction scores.
Well-formulated success criteria is critical in unifying, and success spectrums define a
continuum of least ideal to most ideal, along with the various changes in states across
each known state. The key to success spectrums is to tie sequential states back to the
following:
Financial value

Can be a numeric or quantitative monetary amount, or a qualitative value, such
as best to worst.

KPIs
Like assessments in a knowledge framework, KPIs are defined measures and
metrics that answer the question “How do we track our progress?”

Success metrics
Simply measuring KPIs doesn’t tell you what successful KPIs are. Those thresh‐
olds of success need to be defined. For example, a 90% on a test exam will enable
you to pass, but a 50% will not.

A well-crafted success spectrum visualization provides a strategic road map, illustrat‐
ing how to enhance the value within a product, team, or organization. Think of it as
a process map like the ones you learned about in Chapter 9, that displays the pathway
from one state to another. That process map can even be represented as JSON.

Mapping Progress and Value
To begin creating a success spectrum, let’s continue to build upon Figure 10-2, where
we have a user sign-up page. Similar to a knowledge framework, we want to map the
journey of moving toward goals and a purpose in a linear continuum. The sequence
of the goals should be that the next most desirable state will build upon the previous
state, similar to how the multiplication learning objective builds upon the additional
learning objective.
Figure 10-3 shows a hypothetical example of a company spending $10 on getting
someone who clicks on an ad to a landing page. The value of being on the landing
page to the company is –$10, but if a customer signs up, their average value is $20,
resulting in a $10 profit.

172 | Chapter 10: Spectrums of Success



Figure 10-3. A simple success spectrum visualization that shows the five states along a
continuum, with each node representing a different goal to move the user toward signing
up. On the x-axis, the states in this example are represented as different UX views as the
user progresses from the landing page to completing the sign-up process. On the y-axis,
the value increases from “Very low” to “Very high,” with any known financial value
listed.

Quantifying the financial value of intermediary goals can be challenging. In this
example, we will represent value on the y-axis using terms ranging from “Very
low” to “Very high.” Why? Certain steps, like collecting potential customer email
addresses, have tangible but hard-to-measure value, common in marketing efforts.
Collecting billing information adds even more value, although pinning a precise
financial figure to these states might remain elusive.

Visualizing and Adding “Next Best States”
Using our knowledge framework approach, moving a user to the next state in the
website page flow, such as the contact info page, billing info page, and so on, is
effectively the system’s objective, or goal. Defining objectives for projects in this way
naturally aligns teams and has several benefits:

• The design team has a clearly defined objective to run UX testing against, such
as time to complete a page, user drop-offs, and questions they can ask about how
to make the designs more efficiently achieve the objective—versus, for example,
how to make it more beautiful.

Success Spectrums | 173



• The software engineering team can define APIs that are optimized for the spe‐
cific objective, narrowing scope of maintainability to what is considered mission
critical.

• The data architecture team can create data models that are aligned with the
business goals.

• The data science team can create models to predict who is likely to complete or
drop off (and where), and what interventions may be tested in unison with the
design team to reduce incomplete form sign-ups and gain analytics for targeted
marketing campaigns.

• The marketing team can add incentives such as discounts for inviting friends.

Figure 10-4 shows how easy it is to add “next best state” to a success spectrum—sim‐
ply place a node on what you think is better, then place your estimates on the relative
value of each state. The financial values can be adjusted to match the corresponding
state, and this approach invites questions connecting data to financial value. How
valuable is it if the user invites friends?

Figure 10-4. A success spectrum is incredibly flexible in adding new states, hypotheses,
and visualizing questions that map financial value to desired state. The “Invite friends”
node was added on the x-axis, and the “$?” was added on the y-axis, identifying
opportunities for a value-based analysis to understand what success could look like.

Removing Blind Spots
An effective way to remove blind spots is to have a simple map that explicitly
shows the exact data needed to move from one state to another and reveals what

174 | Chapter 10: Spectrums of Success



information is known to be unknown. This process begins to transition unknown
unknowns into known unknowns.
For example, in Figure 10-5, the “Contact info,” “Billing info,” and “Terms and condi‐
tions” states are all mapped to a simple table stating what data is required to be
collected from a user to move to the next state. Some might be optional, such as
“Phone number.”

Figure 10-5. The act of designing success spectrums supports removing blind spots and a
robust exploit strategy. Each data field is explicit and can be defined with JSON Schema
to minimize ambiguity and identify knowledge gaps. In this example, we can see what
data we are guaranteed to have collected when a user is in the “Billing info” state, and
what the value is to the organization of the user being in that state.

The goal for the data champion is to be able to show a sequence of what information
is a priority to collect and how valuable different types of data are to collect. In

Success Spectrums | 175



Figure 10-5, for example, having a user submit their billing information means they
are able to make transactions, and therefore that is a more valuable state.
Moreover, the financial value tied to inviting friends is marked as “Very high,” accom‐
panied by a question mark symbol to indicate its “known unknown” status. This
method simplifies the process of aligning team members and linking data to business
value. By establishing a tiered and sequential road map in a mutually agreed upon
document, you can mitigate risks of blind spots in business logic, understanding, and
implementation.

Embracing Multiperspective Design and Road Maps
In today’s complex and user-centric digital landscape, successful product design and
development require a profound shift in perspective. Instead of solely focusing on
what the organization needs and values, it’s crucial to adopt a holistic approach that
considers the user’s perspective as well.
Using our light bulb and laser analogy, imagine the user’s perspective as light from a
diffuse light bulb. To create a laser, we need to focus the user’s perspective and tightly
synchronize it. In the example shown in Figure 10-5, the focus is on advancing users
to the next desired state based on what the organization deems valuable.
Let’s align it with the user’s needs and aspirations, using the same success spectrum
format. Instead of asking how to move users forward from the company’s perspective
and benefit, let’s inquire about what users need from the experience the company
is giving them, so that they uncover more value as they progress. This approach
can drastically improve your user experience and the quality of your app, and
also translate to directly supporting goal completion from the company perspective.
Understanding the needs and wants of the user is a critical role of the design team,
and using this technique is how you can integrate them into the unifying process.
For example, users might seek immediate benefits of sharing their billing information
or have concerns about data privacy and security. By understanding the user’s journey
and mapping it within the success spectrum framework, you’re better positioned
to ask the right questions, reducing ambiguity and knowledge gaps. This method
integrates design thinking into your transformation initiatives, ensuring a human-
centered approach.
Creating a success spectrum gives you a road map with various states, outlining
what’s required for each state from multiple perspectives (system and user, for
instance). It equips other teams within your organizational network with richer
information for their own project planning.
Let’s synchronize another beam of light to our laser, aligning a technical team to
our success spectrum. Instead of data collected, as shown in Figure 10-5, let’s align

176 | Chapter 10: Spectrums of Success



a technical team to it with a feature status table for one of the states, as shown in
Figure 10-6.

Figure 10-6. A success spectrum is effective for creating flowcharts tracking a user’s
journey and can be used for multiple perspectives, supporting developing technical
requirements, prioritizing features, creating product road maps, and tracking progress.

Measuring progress aligned to a success spectrum is useful in getting product man‐
agement, design, and software engineering on the same page as the business team
regarding what each state is and reducing ambiguity, knowledge gaps, and blind spots
when planning and prioritizing their road map. The following status indicators are
simple and common ways to track progress, as shown in Figure 10-6:
Not yet started

The feature is in the planning phase but has not been actively worked on by
the development team. It may involve initial discussions, scoping, and gathering
requirements.

Success Spectrums | 177



In progress
This stage means that the feature is actively being worked on by the development
team. Developers are implementing the required code, design, or other elements
to bring the feature to completion.

Complete
All development work is finished and the feature is ready for testing and quality
assurance. It may also mean that the feature has been deployed to a staging
environment or is awaiting deployment to the production environment.

Blocked
The feature cannot progress due to a specific obstacle or issue. It may be waiting
for additional resources, dependencies, or resolution of a problem before it can
move forward. The development team is unable to make further progress until
the blocking issue is resolved.

Unifying is meant to get the business, data, and code teams
together, and although explicitly not mentioned in the title of
this book, design and UX teams are also incredibly important to
include in the unifying conversation. It is recommended that the
business teams, such as project and product managers, also include
the perspectives of designers.

Defining KPIs for Success Measures and Metrics (Assessments)
In addition to mapping the continuum of states, it is also important to define KPIs
(how success is measured) and what the KPI metrics need to be to determine if a
measure has achieved the success criteria, as shown in Figure 10-7. This exercise
can raise important questions and opportunities for hypotheses and experimentation,
such as Why are people dropping off at the Welcome page?
In Figure 10-7, the terms Objective, Key result, KPI, and Status are shown; collectively,
these terms are often called OKRs. Here is a deeper dive into their definitions:
Objective

A clear, qualitative goal set for a specific time frame that is designed to move
you closer to achieving your larger mission or vision. It serves as a directional
guidepost for what your team aims to accomplish.

Key result
Specific, measurable definitions of success. They’re quantitative measures that
indicate whether an objective has been accomplished.

KPI
Metrics used to quantify and evaluate how close or far one is from success.

178 | Chapter 10: Spectrums of Success



Status
Whether or not success was achieved.

Figure 10-7. You can create a simple spreadsheet that is aligned to your success spectrum
and minimizes ambiguity and knowledge gaps for teams so that everyone can see how
success and progress are measured.

In our example, each state on the customer journey, from the Welcome page state to
the Invite friends state, can have its own set of success metrics, all of which can serve
as excellent diagnostic tools to support continuous optimization and exploit-phase
problem solving. If success metrics are not yet defined and there is ambiguity and
knowledge gaps around what success looks like, then the exercise of creating your
success spectrum aids in the explore phase of problem solving.

Using Demons and Magical Thinking for Innovation
Another benefit of success spectrum visualizations is that they can tie alignment
across teams, objectives, KPIs, and financial impact. An ideal success spectrum visu‐
alization starts with the current state, then is mapped to the next best state until an
ideal, almost magical, state is reached. The mental tool for this type of thinking is the
use of “demons” in thought experiments.
In the world of science, a demon is a hypothetical entity used to explore complex
principles or challenge existing knowledge. Here, the term is not about the superna‐
tural, but about sparking creativity and breaking through the limitations of what
currently exists. In our unifying framework, we refer to these types of thought exerci‐
ses as imagining magical possibilities when interviewing stakeholders. If demons are
not appropriate for your audience, “rainbows and unicorns” are perfectly acceptable.

Success Spectrums | 179



Demons in Practice
James Clerk Maxwell, a 19th-century Scottish physicist and mathematician, stands
among the pantheon of science luminaries such as Isaac Newton and Albert Einstein.
He revolutionized the study of electromagnetism, unifying electricity, magnetism,
and light under a single theoretical framework.
Among Maxwell’s myriad contributions, the thought experiment known as Maxwell’s
demon stands out for its audacity and creativity. In this mental exercise, Maxwell
envisioned a tiny, hypothetical creature (the “demon”) that could selectively allow
faster or slower gas molecules to pass through a partitioned chamber, separating
hot molecules from cold without any expenditure of energy. This seemingly benign
activity posed a direct challenge to the second law of thermodynamics, which states
that the entropy, or disorder, of an isolated system can only increase. Maxwell used
this demon not to refute the law, but to probe its boundaries and implications. While
the demon itself is a fictional construct, the discussions it sparked revolutionized
thermodynamics and deepened our understanding of the microscopic nature of
matter and energy.

Faster Horses
You may have heard the phrase “If I had asked my customers what they wanted, they
would have said a faster horse,” which is often (and potentially mistakenly) attributed
to Henry Ford. In unifying, we recommend listening with obsessive amounts of care
and detail to customer problems, but not so much to the solutions they suggest.
The reason is that the solutions they recommend are within the constraints of what
they believe are possible, which places limits on the art of the possible from technical,
design, and business perspectives. Additionally, customers are not privy to your
holistic perspective on the priorities, the needs of other teams, and how changes
somewhere else will impact the choices and opportunities for innovation.
Here is a flow you can follow that ties together the various tools you’ve learned:

1. Use the concept compass to measure misalignment of concepts between teams
and across language, data, and operational planes.

2. Use the CLEAN data governance framework to connect the dots of how concepts
connect across the organization to map business logic, collaboration, knowledge,
activities, and to understand any requirements.

3. Use process maps and success spectrums to capture current state.

180 | Chapter 10: Spectrums of Success



4. Do the imagining magical possibilities exercises (see the next section) with your
stakeholders.

5. Redo the CLEAN data governance process and then evaluate which possible
states accelerate and align teams, concepts, and operations.

6. Map those possible states back onto success spectrums and get feedback from
your clients, internal or external. Those possible future states will help your
organization decide which success spectrum aligns with leadership goals and
available resources.

Imagining Magical Possibilities
When creating an innovation success spectrum visualization, imagining demons can
be a transformative exercise that unlocks innovation capabilities.
Consider the following approach to create your own demon thought experiments:

1. Identify a problem. This is a clear starting point that focuses the mind on a
particular challenge or question.

2. Define the rules your demon breaks and how your demon breaks them. This step
encourages imaginative thinking about how the demon operates, helping to craft
a unique and engaging scenario.

3. What contradictions or insights does it reveal? This step guides the thinker to
analyze the implications of the demon’s actions, fostering deeper understanding.

4. What questions does it raise? This encourages further exploration and considera‐
tion of new ideas or challenges.

5. How might you test it? Adding a practical element, this step prompts the thinker
to consider how the thought experiment might be explored or validated in
real-world terms.

Imagine you have two marketing analytics teams: one to predict
advertising targets to juice up subscriptions to your service, and
another marketing team to predict who is at risk of quitting and
determine how to convince customers to stay.
An ideal demon behavior would be that the demon could perfectly
predict perfect advertising targets least likely to quit and most
likely to join on the smallest advertising budget. Thinking from
the perspective of the demon ignites valuable questions like What
would it take to achieve something like this?

Success Spectrums | 181



Problem Landscapes: Quantifying Pain Points Threatening Value
One benefit of the success spectrum visualization is that it naturally enables for visu‐
alizing problems, their connectivity to objectives, and their financial value. This can
help you visually determine which problems to prioritize, as shown in Figure 10-8.

Figure 10-8. The higher a problem’s pain level, the more negative value it creates, as
shown on the y-axis. The x-axis can be used to represent different types of problems and
to aggregate the pain level related to objectives and the negative impact and risk they
create.

In the realm of modern organizational dynamics, it is often the unseen and unquanti‐
fied elements that lead to failure or inefficiency. Problem landscapes offer an elegant
inversion of the success spectrum, laying bare the hidden complexities and intercon‐
nections of different problems and their impact on objectives and financial value.
Like a topographical map reveals the contours of a terrain, the problem landscape
graphically represents the “terrain” of challenges faced by the organization.

182 | Chapter 10: Spectrums of Success



The distinctiveness of problem landscapes lies in their ability to not only illustrate
problems but also their interconnections, alignment with organizational goals, and
the magnitude of their negative impact, leading you to a deeper understanding of
where efforts should be focused to mitigate these problems. Connecting problem
landscapes with success spectrums harmonizes an organization’s approach to both
triumphs and tribulations, creating an integrated view that caters to technical and
nontechnical teams alike and ensuring that both problems and successes are framed
in a language and perspective that resonates with all.
Success spectrums and problem landscapes represent two sides of the same coin, pro‐
viding a comprehensive view of an organization’s growth and challenges. Together,
they create a harmonious alignment between all stakeholders, transforming ambigu‐
ity into clarity and fostering a collaborative culture. This approach goes beyond con‐
ventional problem-solving methodologies by forming an interconnected framework
that validates concepts, identifies and eliminates misalignments, and places a keen
focus on data-centric innovation. Moreover, its adaptability allows it to be applied
in various contexts such as combining network perspectives, thus unlocking the
potential of data science to create innovation strategies and reduce redundancies
and inefficiencies. It is an embodiment of a unified strategy that transcends barriers,
catalyzes growth, and paves the way for a more coherent and efficient organization.

Nudges: The Right Information at the Right Time
Imagine visiting a website and facing the prospect of scrolling through 10 pages of
text before clicking a button. You’d likely hesitate, right? But place the Terms and
Conditions at the end and you’ll likely accept without a second thought. The simple
yet powerful concept of nudging users toward specific actions stems from behavioral
analytics. It’s not just a tool for web developers, but a universal principle that can
align users’ behavior with organizational goals (see Figure 10-9).
In our data-driven world, nudges are more than mere suggestions; they’re carefully
crafted springs, designed to propel employees, users, and leaders toward well-defined
success spectrums. By leveraging data science and personalization, we can predict
the exact content that will aid each individual at the right time. For instance, an
employee grappling with complex challenges may benefit from precise knowledge
presented at the opportune moment. This aligns with our unifying systems, where
synchronization and alignment are key.

Nudges: The Right Information at the Right Time | 183



Figure 10-9. The user starts at the Welcome page. Depending on the information presen‐
ted at each state of the success spectrum, the user might be more or less likely to move
to the next stage. The goal of thinking of information in this way is to discover the
information that most effectively accelerates the user of your unifying systems to the
most valuable state.

Information is a powerful force and understanding how to harness
it for organizational success becomes vital. Technology not only
makes this possible but allows for an unprecedented level of accu‐
racy and customization. This idea is central to the methodology
outlined in our book.

A Real-Life Problem Landscape and Demon Example That
Led to a Unified Data Product Model
The concept of demons in thought exercises, magical thinking, and problem land‐
scapes isn’t merely a philosophical idea; it has practical applications and was used
to design the data product specification used in this book. The four facets of data
products, as introduced in Chapter 4, can be traced back to these thought exercises.

Understanding the Problem Landscape
Through years of diligent research, interviews, and analysis of best practices and
failures, a vivid problem landscape emerged. It became apparent that the professio‐
nals at the front line of data—business intelligence analysts, data scientists, software
developers, and data visualization designers—were drowning in inefficiencies and
confusion.

184 | Chapter 10: Spectrums of Success



Imagine the torment of sifting through data swamps, encountering .csv files with
hundreds of ambiguous columns and no clear definition of their meaning. The
professionals were trapped in an agonizing process of detective work, attempting to
uncover why the data set was created, the significance of each column, and the origins
of it all.

The Staggering Impact
This scenario is not hypothetical. In UX research interviews with data practitioners
at large enterprise companies, it was quite ordinary, wreaking havoc on productivity
with a destructive impact that was nothing short of staggering. The time wasted,
the risk created, and the disillusionment and overwhelm among data teams led to
insights being lost, replaced by help desk tickets. Root causes were left unaddressed.
Desperate for quick fixes, executives resorted to purchasing expensive enterprise
software that only added complexity. Such solutions rarely hit the mark because they
fail to address the simple yet vital need for translating business requirements into
clearly defined and universally agreed-upon documentation.

A Meeting of Minds and the Birth of a Solution
It was against this backdrop that the authors of this book first met, united by a shared
recognition of the problem and a vision for a solution. We contemplated whether
we could create a specification that would guarantee that all essential information
was available and at hand, making the process of working with data seamless and
efficient.
Now that you’ve learned about success spectrums, let’s revisit the data products idea
you learned about in Chapter 4, using the success spectrum approach, as shown in
Figure 10-10.
The scientific thought experiment tradition of demons as a way to imagine a magi‐
cally perfect way to control a problem led to a tactical and tangible tool for harmoniz‐
ing datasets to improve data management requiring no technology to implement. The
solution was a data product specification that would include all necessary metadata
with the data itself.
This integration is both a theoretical alignment and a practical method that bridges
the knowledge gap, eliminates ambiguity, and fosters synchronicity. It proves that
unifying can create simple best practices and pave the way for a more coherent and
efficient organization.

A Real-Life Problem Landscape and Demon Example That Led to a Unified Data Product Model | 185



The authors of this book are working on an open source specifica‐
tion to represent all four facets of a data product in one standalone
JSON document. This specification is called JSON Unify.

Figure 10-10. The origin of the data products specification shown in this book started
with a success spectrum. Data professionals expressed what they wished they had—
which are shown in the table as “I can”—in the Agile user story format or expressed their
biggest pain points—shown in the table as “I can’t” in a problem statement format. If
the average data team member salary is $150K, and 50% of their time is wasted due to
these problems, then a hypothetical cost for a team of 10 means that implementing data
products can save the organization $7.5M.

186 | Chapter 10: Spectrums of Success



Beyond Data Products: Data Product Management
The success spectrum approach enables you to zoom in and zoom out to set transfor‐
mation goals at various levels of granularity. It is useful in minimizing ambiguity
and knowledge gaps when designing end-to-end and 360° systems, which require
thinking about interconnected and interdependent data. For example, Figure 10-11 is
a high-level success spectrum, including an objective of defining success metrics. This
is where the CLEAN data governance model is helpful; since you have already used
that in Chapter 7, you can add their perspectives to any spectrums you wish to design
and test.

Figure 10-11. An example of a data champion mapping end-to-end considerations for
having successful data product management capability.

Every organization is different. That being said, the following are several categories
of data product management that you might want to consider for testing the success
spectrum approach:
Data foundation and integrity

Data sources: identifying available data and what needs to be collected
Quality control: ensuring data accuracy and consistency
Security: protecting data from unauthorized access
Testing: validating the entire data process

Alignment, language, and metrics
Business logic translation: aligning business goals with technical requirements
Semantics: establishing common terminology
KPI setting: identifying key performance indicators
OKR metrics: aligning efforts with organizational goals

Governance, ethics, and compliance
Governance: implementing data management policies (linked to CLEAN
governance)
Ethics review: aligning data handling with ethical principles

Beyond Data Products: Data Product Management | 187



Interoperability and accessibility
APIs: facilitating interaction between systems
Query management: managing data access and retrieval

Data utilization and visualization
Data products: creating and managing products leveraging the data
Data visualizations: translating data into visual formats

The Circular Nature of Unifying
Once you have gotten alignment in the accelerate phase, it is important to go back
to the assess phase and start over, harmonizing concepts, synchronizing teams, and
continually learning and refining concepts, decision points, goals, and KPIs. For a
review of cycles in unifying, see Figure P-2 to compare the explore and exploit phases
of problem solving, and Chapter 4 for a review of the assess phase.
The more cycles you do, the more alignment you will achieve at all levels and across
business functions. Visualize a bicycle with an imperfect front wheel, slightly warped
and bent in places, attempting to navigate a road filled with potholes and unexpected
twists. Riding this bicycle is not only a struggle but an exercise in frustration, ineffi‐
ciency, and exhaustion, where the rider’s energy is wasted in combating misalignment
rather than propelling forward.
The imperfect wheel metaphor vividly represents the complications that arise when
collaboration is weakened by misalignment. Just as a misaligned wheel demands
more effort and wastes energy, so does misalignment in business strategy and execu‐
tion. It consumes resources, dilutes focus, and restricts progress.
Now, consider misalignment within a business. As misalignment is uncovered, some
issues, such as defining revenue in a dataset, might be simple to solve. But what
happens when deeper, more entrenched problems are discovered? Imagine an entire
database team working for years on a certain approach only to find that their
direction clashes with the data science team. Suddenly, the challenge becomes more
profound and a major rift needs to be addressed. Conquering that challenge is akin
to repairing a warped wheel; the rider can accelerate more efficiently. Data champions
enable organizations to accelerate by finding and repairing the misalignment causing
inefficiencies across business, data, and code.

188 | Chapter 10: Spectrums of Success



Summary
In this chapter, you learned that to accelerate innovation, you need to recognize the
relationship between knowledge gaps and ambiguity, along with how they can impact
concepts, goal setting, and decision making.
You delved into success spectrum visualizations, which a clear picture of the path
from the current state to the desired outcome. By tying objectives, KPIs, and financial
impacts together, you gain robust framework from which to foster collaboration and
strategic planning.
You saw how the concept of demons adds a new layer to creative thinking and can
help you unlock innovation and break conventional limitations.
The “faster horses” anecdote illustrates the importance of understanding customer
problems instead of accepting the solutions they suggest, emphasizing a deeper level
of alignment and innovation. The Concept Compass and CLEAN data governance
principles enable mapping and aligning teams, concepts, and operations—a more
visionary approach to problem solving.
Finally, we showed you how alignment, innovation, financial insights, and visuali‐
zations can be skillfully integrated, opening new opportunities for collaboration,
breaking down silos, and leveraging data to make informed decisions.

Chapters 11–14 are focused on the technical parts of implementa‐
tion. If your main interest is in the nontechnical and theoretical
aspects of the unifying methodology, you may want to skip to
Chapter 15.

Summary | 189



CHAPTER 11
Deploying a JSON Schema Registry

It’s not about ideas. It’s about making ideas happen.
—Scott Belsky, author, entrepreneur and investor

This book has proposed a new methodology for thinking about and working with
data. Data champions have applied this methodology to their organization using
spreadsheets, process maps, whiteboards, diagrams, and more. After countless meet‐
ings, they felt everybody was aligned toward the bright future ahead. However, they
could only go so far by writing documents and guidelines. Eventually, documents
were ignored and misalignment found its way in again. As it turns out, reaching
alignment only solves half of the problem. You also need a strategy to stay there.
To maintain alignment, the outcome of the unifying process has to be encoded
and connected into the software systems that store and process your data, making
divergence quickly detectable and correctable at its root. This is where a schema
registry comes in.
A schema registry is a centralized collection of schemas meant to serve as the single
source of truth for three out of the four facets of data introduced in Chapter 4:
context, structure, and meaning.
By connecting all of your software systems to your schema registry, you can
ensure data integrity through schema validation and, when needed, extract semantics
through schema annotations, as covered in Chapter 8.

Schemas Over HTTP
A schema registry—a collection of schemas—can be made available in many forms.
For example, some schema writers publish them to package managers such as
npm in Node.js or Python’s PyPI, allowing consumers to access them through a

191



programmatic interface. Others keep their schemas on version control and embed
them with the microservices they deploy. Instead, the authors of this book recom‐
mend hosting schemas over the Hypertext Transfer Protocol (HTTP), the standard
internet protocol that serves websites and APIs. This approach is extremely flexible;
JSON Schema implementations may initially fetch the schemas over the internet and
locally cache them, or developers may download them during a build phase and
redistribute them for future use.
In this chapter, you will learn how to deploy a basic schema registry over the HTTP
protocol using GitHub and Cloudflare Pages, upload and retrieve some sample sche‐
mas, and configure a custom domain. You should be able to follow along even if it’s
your first time making use of these services.

Step 1: Setting Up a GitHub Repository
As with any other software in your organization, you should keep your schemas
under a version control system such as Git. If you haven’t yet, create a free account on
GitHub, the most popular Git provider around.

Creating a GitHub Repository
Once you are in, create a new repository to host your schema files. In Figure 11-1, we
create a public repository called “unify101.”

Figure 11-1. To create a GitHub repository, we clicked on the plus sign at the top right of
the screen and then on the “New repository” option.

192 | Chapter 11: Deploying a JSON Schema Registry



Uploading Your First Schema
A GitHub repository will allow you to organize your schemas into files and directo‐
ries of your choosing. We suggest the following file system convention:

• Place all your schemas under a schemas subdirectory. That way, you can add
other files to your GitHub repository, like a README file, without polluting
your schemas.

• Inside the “schemas” directory, create one directory per project. This allows you
to keep your schemas grouped in a way that makes sense in your organization.
For example, you may have a project called “genomics.”

• Inside a project directory, create one directory per schema for versioning pur‐
poses.

• Inside a schema directory, you will have one JSON file per version. For example:
v1.json, v2.json, etc. We will come back to the topic of schema versioning later in
the chapter.

Again, this is just a suggested file structure, and you are welcome to adopt a different
one that better accommodates your needs. As rules of thumb, we recommend that
your file structure keeps schemas separate from other types of files and that schemas
are organized into domain or project directories.
Let’s upload the schema we created at the end of Chapter 5. This schema describes a
product entry for the fictitious Intelligence.AI coffee shop. Following the file system
convention explained above, our project name will be intelligence-ai and the
schema name will be product-entry. Therefore, the full path to its first version will
be schemas/intelligence-ai/product-entry/v1.json.
On GitHub, click the “Create a new file” link, type in the right path above the
editor and copy-paste the schema from Chapter 5, adding an $id property that
equals https://schemas.unify101.com/intelligence-ai/product-entry/v1.json, as shown in
Figure 11-2.
When you are ready, click the “Commit changes” button. GitHub will ask you for
a commit message. We typed “Create Intelligence.AI product entry v1”, as shown in
Figure 11-3.

Step 1: Setting Up a GitHub Repository | 193



Figure 11-2. GitHub allows you to easily create new files using the web interface. For
more power, you might be interested in checking out the GitHub Desktop application.

Figure 11-3. Every change (commit) on a Git repository is associated with a commit
message and optional description. When writing commit messages, it is considered best
practice to use the present tense.

194 | Chapter 11: Deploying a JSON Schema Registry



If you are new to Git and want to learn more, we recommend the
excellent book Learning Git by Anna Skoulikari (O’Reilly, 2023).

You should now be able to see your brand new schema file in GitHub’s file explorer, as
shown in Figure 11-4. Make sure the path is right before continuing.

Figure 11-4. An example of the Intelligence.AI product entry schema on GitHub’s file
explorer.

Step 2: Deploying to Cloudflare Pages
Cloudflare Pages is an excellent, free HTTP static hosting platform. In comparison to
competitors such as GitHub Pages, Cloudflare Pages allows you to set custom HTTP
headers, which we will use for JSON Schema compliance. You can register for free.

Creating a New Cloudflare Pages Website Project
Once you are registered on Cloudflare, you can create a new website project through
the left sidebar by clicking the Workers & Pages link. Once there, click the Pages tab
and the “Connect to Git” blue button, as shown in Figure 11-5.

Step 2: Deploying to Cloudflare Pages | 195



Figure 11-5. Cloudflare Pages allows you to conveniently deploy from an existing GitHub
repository. That way, your registry will be automatically updated any time you make
changes to your GitHub repository.

If it is your first time deploying to Cloudflare Pages, you will be presented with a
screen to authenticate Cloudflare against your version control platform of choice.
Click the GitHub tab and click Connect GitHub, as shown in Figure 11-6.

Figure 11-6. Cloudflare Pages needs access to your GitHub account to detect changes to
your repositories for the purpose of triggering deployments.

196 | Chapter 11: Deploying a JSON Schema Registry



GitHub provides fine-grained access control to third-party services like Cloudflare.
You can give Cloudflare Pages access to selected GitHub organizations or specific
GitHub repositories. For security reasons, we recommend only authenticating Cloud‐
flare Pages against your schema registry repository, as shown in Figure 11-7.

Figure 11-7. We will only authorize Cloudflare Pages to access the “unify101” GitHub
repository we created earlier in this chapter by selecting “Only select repositories” and
choosing our repository from the drop-down menu.

Step 2: Deploying to Cloudflare Pages | 197



Once you are done associating Cloudflare Pages with your GitHub account, select
your GitHub repository of choice on the Cloudflare Pages wizard and click “Begin
setup,” as shown in Figure 11-8.

Figure 11-8. After the authorization step, you allowed Cloudflare Pages to see your
GitHub repositories. Now it’s time to actually select the one you want to deploy. In this
case, it’s “unify101.”

Next, you will be presented with a form to configure your project deployment, as
shown in Figure 11-9. Create a project name of your choice, select “main” as the
“Production branch,” avoid setting up any “Framework preset” or “Build command,”
and set “schemas” as the “Build output directory.” This corresponds to the schemas
subdirectory in the GitHub repository.
Once you click “Save and deploy,” Cloudflare Pages will start deploying your registry
to a preproduction domain of their choice. In our case, as shown in Figure 11-10,
Cloudflare Pages deployed the “main” branch to https://c5b5018a.unify101.pages.dev.
Visiting the URL directly will result in an invalid request, as we are not serving
a landing page yet (we will get to that later!) However, we can already fetch our
example schema using its full URL path. We instructed Cloudflare Pages to deploy
the schemas directory in our GitHub repository, so in our case, the full URL to our
example schema is https://c5b5018a.unify101.pages.dev/intelligence-ai/product-entry/
v1.json.

198 | Chapter 11: Deploying a JSON Schema Registry



Figure 11-9. Cloudflare Pages allows for extensive deployment configuration. In this
case, we only require a basic deployment setup. Check out their official documentation to
learn more.

Figure 11-10. Your recent deployment should pop up on the Cloudflare Pages dashboard.
There, you will be able to see the unique URL Cloudflare created for this specific
deployment.

Step 2: Deploying to Cloudflare Pages | 199



Let’s request the previous Cloudflare Pages URL using Postman, a powerful free
HTTP client application, as shown in Figure 11-11.

Figure 11-11. Postman is a powerful tool for playing with HTTP applications like our
schema registry. If you paste the schema URL into the input box, select GET as the HTTP
method of choice, and hit the Send button. You will see the example schema show up in
the bottom half of the screen.

Step 3: Configuring HTTP Headers
While this section might not make a lot of sense if you are not familiar with HTTP,
you should be able to follow along. If this is your first time working with HTTP,
we recommend consulting HTTP: The Definitive Guide by David Gourley (O’Reilly,
2002) to learn more.
The JSON Schema specification describes a set of interoperability recommendations
for hosting schemas over HTTP; schemas should be associated with the applica
tion/schema+json media type, and the response may associate the instance with its
meta-schema using the Link HTTP header through the describedby relationship.

We strongly suggest following these recommendations when
deploying schemas over HTTP, as some strict JSON Schema imple‐
mentations may otherwise require additional configuration. For
example, at the time of this writing, the popular Hyperjump valida‐
tor we used in Chapter 8 will by default refuse to load schemas over
HTTP that have an incorrect media type.

200 | Chapter 11: Deploying a JSON Schema Registry



Inspecting the Current HTTP Headers
Currently, we are not respecting either of the recommendations from the JSON
Schema specification by using Postman to display the HTTP response headers we get
from requesting our example schema, as shown in Figure 11-12. The Content-Type
header is set to application/json instead of application/schema+json, and the
Link header is not present at all.

Figure 11-12. After a successful request, Postman allows you to explore response headers
by clicking the Headers tab in the header of the bottom pane.

Declaring Custom HTTP Headers on Cloudflare Pages
Luckily, Cloudflare Pages makes it trivial for us to specify custom HTTP headers.
To do so, go back to GitHub, click “Add file,” and from the drop-down menu, select
“Create new file,” as shown in Figure 11-13.

Figure 11-13. In Cloudflare Pages, custom HTTP headers can be set through a config‐
uration file that you can commit to your GitHub repository. Remember that after
connecting your GitHub repository to Cloudflare Pages, any change you apply in GitHub
will trigger a Cloudflare Pages redeployment.

For Cloudflare Pages to recognize it, this new file must be called _headers and it must
live directly inside the schemas top-level directory, as shown in Figure 11-14.

Step 3: Configuring HTTP Headers | 201



Figure 11-14. Make sure you type in the right file name when creating the _headers file
on GitHub and that you place it into the right directory. Otherwise, Cloudflare Pages
will not be able to find it.

This file declares a set of file-matching rules and HTTP headers that will be applied to
them. In our case, it looks like this:
/*.json 
  Content-Type: application/schema+json 
  Link: <https://json-schema.org/draft/2020-12/schema>; rel="describedby" 

This creates a rule that matches any JSON file , sets the Content-Type header to
application/schema+json , and sets the Link header to express that every schema
definition in this repository is based on the JSON Schema official 2020-12 dialect .
If you plan to have schemas following other JSON Schema dialects, such as ones
using custom vocabularies of your own (which will be explored in Chapter 12), make
sure to update this configuration file accordingly.

Checking the Results
As soon as you commit your changes, Cloudflare Pages will detect the new file and
trigger a new deployment. You can check that Cloudflare Pages was able to find your
_headers configuration file by taking a look at the Headers tab. In there, Cloudflare
Pages will echo back the custom headers it will apply, as you can see in Figure 11-15.
As before, Cloudflare Pages will deploy the new version of the registry to
a preproduction domain of their choice. In our case, this is now https://
97cd1989.unify101.pages.dev. We can confirm our custom HTTP headers were
applied by requesting our example schema from the new URL using Postman, as
shown in Figure 11-16, and inspecting the Headers tab as we did in the previous
section.

202 | Chapter 11: Deploying a JSON Schema Registry



Figure 11-15. Cloudflare Pages will report back the custom headers it detected. If you
don’t see this, then you either mistyped the _headers filename or placed it in the
incorrect directory.

Figure 11-16. After our recent changes, our registry serves schemas following the HTTP
recommendations of the JSON Schema specification.

Step 3: Configuring HTTP Headers | 203



Step 4: Creating a Landing Page
At this point, our schema registry is usable, as long as clients know what schemas it
contains and what their URLs are. To improve user experience, let’s create a simple
landing page that lists all the schemas currently present in the registry.

Adding an HTML Entry Point
On GitHub, create a new file called index.html, place it inside the schemas top-level
directory, and make its content as follows:
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>My First Schema Registry</title>
  <link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css">
</head>
<body>
  <header>
    <h1>My First Schema Registry</h1>
    <p>Published to Cloudflare Pages</p>
  </header>
  <main>
    <table style="width: 100%;">
      <thead>
        <tr>
          <th>Description</th>
          <th>Version</th>
          <th>Link</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>An Intelligence.AI product entry</td>
          <td>v1</td>
          <td>
            <a href="intelligence-ai/product-entry/v1.json">/intelligence-ai/
product-entry/v1.json</a>
          </td>
        </tr>
      </tbody>
    </table>
  </main>
</body>
</html>

204 | Chapter 11: Deploying a JSON Schema Registry



This HTML page will be presented when users navigate to the top-level URL of the
registry (Figure 11-17). It presents a table with a hardcoded list of schemas and uses
the SimpleCSS framework to add some basic styling.

Figure 11-17. As before, when making use of the GitHub UI, double check your filename
and commit your changes.

Once the HTML file is set, wait for the new Cloudflare Pages new deployment, then
visit the new URL on your browser. It should look something like Figure 11-18.

Figure 11-18. Our simple registry landing page. While this version requires you to
manually update the contents of the table, you can iterate on it to add some logic for
automatically constructing the lists of schemas or any other feature you like.

Step 5: Adding a Custom Domain
It is common practice that the URL you use to fetch a schema equals the value in its
$id property. The $id property of the example schema we adapted from Chapter 5
is set to https://schemas.unify101.com/intelligence-ai/product-entry/v1.json. However,
the schema is currently getting served from https://unify101.pages.dev/intelligence-ai/
product-entry/v1.json.

Step 5: Adding a Custom Domain | 205



To fix this, let’s configure Cloudflare Pages to serve our registry from our own custom
domain instead of its preproduction pages.dev subdomain. If you don’t have a custom
domain yet, you can obtain one from a variety of domain name registrars such as
Namecheap and GoDaddy. We own unify101.com and will deploy the schema registry
at schemas.unify101.com.

Configuring a Custom Domain in Cloudflare Pages
Cloudflare Pages allows you to use a custom domain for free. To configure it,
head over to the Cloudflare Pages dashboard, select your website project, select the
“Custom domains” tab, and click the blue “Set up a custom domain” button (see
Figure 11-19).

Figure 11-19. Cloudflare Pages provides an easy-to-follow wizard for setting up custom
domains.

Once into the wizard, type in your expected domain or subdomain and click Con‐
tinue (see Figure 11-20).
You can either transfer management of your domain to Cloudflare or perform the
configuration on your domain registrar of choice. We acquired the domain from an
external domain registrar, so in Figure 11-21 we select the latter option.

206 | Chapter 11: Deploying a JSON Schema Registry



Figure 11-20. Cloudflare Pages allows you to deploy your project to a top-level domain
or a subdomain of your choice, as long as you actually own the domain.

Figure 11-21. We recommend setting up your custom domain through your custom
registrar unless you expect to only use your domain in conjunction with Cloudflare
cloud offerings.

Step 5: Adding a Custom Domain | 207



Setting Up a CNAME DNS Record
Deploying a Cloudflare Pages project to a custom subdomain, in our case sche‐
mas.unify101.com, requires some basic Domain Name System (DNS) configuration.
In DNS lingo, we want to create a CNAME record that points to subdomain at the
Cloudflare Pages deployment. If you have never done this before, don’t panic! It is
much easier than it sounds.
Head over to the domain registrar you used to buy your domain. Every domain
registrar provides a way to adjust the DNS settings of your domain of choice; for
example, GoDaddy provides a DNS tab that presents a DNS Records section. Consult
the documentation of your domain registrar of choice if you cannot find it.
Once in there, create a new DNS entry of the type CNAME, set the record name to
your subdomain of choice and the value to your Cloudflare Pages project subdomain
(from the URL you have been using all along to test your deployment), leave every‐
thing else untouched, and save your changes. In our case, we will set the record name
to schemas and its value to unify101.pages.dev (see Figure 11-22).

Figure 11-22. Here we are setting up the CNAME record for our unify101.com domain
on GoDaddy. In other words, we are asking DNS to serve unify101.pages.dev whenever
somebody visits schemas.unify101.com.

208 | Chapter 11: Deploying a JSON Schema Registry



Checking the Results
DNS is a distributed internet protocol for associating human-readable domains to IP
addresses. Due to its nature, it might take some time for your new DNS record to
propagate through the network; it usually takes about 30 minutes, but it might take
up to 48 hours depending on your location and internet provider, so be patient.
Once the changes are successfully propagated, Cloudflare Pages will mark your cus‐
tom domain as active under the “Custom domains” tab (see Figure 11-23).

Figure 11-23. DNS propagation takes some time. Even if Cloudflage Pages detects it as
active, it might still take a bit longer for such changes to propagate to your personal
network. Flushing your computer’s DNS cache might help.

After waiting a bit, try to point your web browser to the custom domain you
selected. With any luck, you will get your schema registry landing page back (see
Figure 11-24).

Figure 11-24. Our new schema registry is up and running at
https://schemas.unify101.com.

Step 5: Adding a Custom Domain | 209



Best Practices
Operating a schema registry is a smooth process as long as you follow a couple of
simple but important best practices.

Schemas Are Immutable
Make sure to treat schemas as immutable resources. In other words, once you publish
a schema to the registry, don’t update it in place, and don’t remove it. Otherwise,
you risk breaking applications or other schemas that depend on or reference these
schemas.

If you need to change a schema, publish a new version instead.

Adopt a Versioning Strategy
The last point takes us to versioning. You should not update schemas in place, but
you can always publish a new version. JSON Schema does not define a version
keyword; instead, a common practice is for the version to be part of the URL. This is
why, at the beginning of this chapter, we suggested creating one directory per schema
where the contents correspond to one file per version: v1.json, v2.json, etc. However,
we encourage you to develop the file organization and versioning strategy that works
best for you. As an alternative, the JSON Schema organization adopted a YYYY-MM
date versioning scheme for the official meta-schemas, such as 2019-09 and 2020-12.
If you are familiar with Semantic Versioning, you might find SchemaVer appealing.
SchemaVer is a specification that proposes adapting of Semantic Versioning major,
minor, and patch version fragments from software interfaces to schemas. With
SchemaVer, you can be very precise about the type of change you are applying to
your schema.
However, in our experience, SchemaVer is hard to apply correctly. Determining what
type of change an update corresponds to may require in-depth knowledge of JSON
Schema and how certain keywords subtly affect schema semantics. For this reason,
we recommend a simple single-digit, ever-increasing versioning schema like we have
exemplified in this chapter. The first schema version is v1, the second one is v2, and
so forth.

210 | Chapter 11: Deploying a JSON Schema Registry



Summary
If you followed along with the chapter, now you have a simple registry that you can
use to host every aligned concept that resulted from applying the book’s methodology
and let your applications consume them as the source of truth. With some additional
effort, you can add analytics to your schemas to see who is using what, support
private schemas, and any other feature you desire. The sky’s the limit.
In the next chapter, you will learn how to bring a data product idea (from Chapter 4)
into life using JSON, JSON Schema, and your brand new schema registry.

Summary | 211



CHAPTER 12
Designing Data Products

Using JSON Schema

Everything is designed. Few things are designed well.
—Brian Reed, renowned industrial designer

Chapter 4 introduced the idea of a data product as a self-contained object with four
facets: data, structure, meaning, and context. In practice, some of these facets tend
to be ignored. For example, data scientists get collections of CSVs with inconsistent
rows and insufficient information about what each column means, when the dataset
was created, and so on. The lack of these facets introduces ambiguity, therefore
extracting key insights out of these badly designed sources of data becomes extremely
challenging, no matter how much expensive tooling or expertise is thrown at the task.
To address this data problem, you learned a proven methodology for achieving
alignment in your organization. You also learned the fundamental technologies that
make this methodology applicable: JSON and JSON Schema. In this chapter, we will
put all of it into practice by walking you through how to design a data product with
a concept-first approach using JSON and JSON Schema. We will look at each facet of
a data product in sequence, building upon the JSON Schema registry you deployed in
Chapter 11.

213



First Facet: Data
The first facet of a data product we will look into is data itself. Chapter 10 introduced
the idea of spectrums of success, using as an example a user journey through com‐
pleting a website sign-up form. In such an example, the user starts by visiting the
landing page, fills in their contact and billing information, agrees to the website terms
and services, signs up, and, finally, invites their friends.

An Example CSV Dataset
Table 12-1 represents a fictitious website analytics CSV dataset that records the
corresponding milestones as five users complete them. From left to right, we record
the time of the event, the IP address of the user, the email of the user (if known),
the US state the user is connecting from, the milestone they achieved, and the user
acquisition cost for such an event (e.g., an ad impression that pointed a user to the
landing page). Note that not every user completes every milestone; some drop out in
the process.

Table 12-1. An example website analytics CSV dataset we will use throughout this chapter
Timestamp IP address Email State Milestone Cost Currency
2023-08-21T13:24:49Z 84.216.114.94 — CO visit_landing_page 0.92 USD
2023-08-21T13:26:33Z 167.16.122.17 — WA visit_landing_page 1.75 USD
2023-08-21T13:28:09Z 182.28.182.59 — CA visit_landing_page 2.62 USD
2023-08-21T13:29:52Z 115.93.200.62 — NY visit_landing_page 2.89 USD
2023-08-30T13:30:16Z 182.28.182.59 gator@example.com CA set_contact_info 0 USD
2023-08-21T13:32:28Z 105.0.165.55 — PA visit_landing_page 0.45 USD
2023-08-30T13:33:39Z 182.28.182.59 gator@example.com CA set_billing_info 0 USD
2023-08-21T13:34:07Z 84.216.114.94 bsikdar@example.com CO set_contact_info 0 USD
2023-08-21T13:35:51Z 105.0.165.55 kenja@example.com PA set_contact_info 0 USD
2023-08-30T13:36:22Z 84.216.114.94 bsikdar@example.com CO set_billing_info 0 USD
2023-08-30T13:36:51Z 84.216.114.94 bsikdar@example.com CO agree_to_terms 0 USD
2023-08-30T13:38:17Z 84.216.114.94 bsikdar@example.com CO sign_up 0 USD
2023-08-21T13:40:14Z 115.93.200.62 crove@example.com NY set_contact_info 0 USD
2023-08-30T13:43:01Z 84.216.114.94 bsikdar@example.com CO invite_friends 0 USD
2023-08-30T13:45:25Z 115.93.200.62 crove@example.com NY set_billing_info 0 USD
2023-08-30T13:46:52Z 115.93.200.62 crove@example.com NY agree_to_terms 0 USD
2023-08-30T13:51:31Z 115.93.200.62 crove@example.com NY sign_up 0 USD

214 | Chapter 12: Designing Data Products Using JSON Schema



A JSON Row Representation
To make good use of JSON Schema, we need to convert our dataset to a JSON
array. Using the knowledge discussed in Chapter 2, one way to model the dataset in
Table 12-1 is as a JSON array whose items are JSON objects that look like this:
[
  {
    "timestamp": "2023-08-21T13:24:49Z",
    "ip": "84.216.114.94",
    "state": "CO",
    "milestone": "visit_landing_page",
    "cost": { "amount": 0.92, "currency": "USD" }
  },
  …
  {
    "timestamp": "2023-08-30T13:51:31Z",
    "ip": "115.93.200.62",
    "email": "crove@example.com",
    "state": "NY",
    "milestone": "sign_up",
    "cost": { "amount": 0, "currency": "USD" }
  }
]

For each item in the array, we can omit the email property if not set. In our model,
the cost and currency columns from the CSV are tightly related, so we decided to
combine them into a single cost object property.

Second Facet: Structure
The structure facet is concerned with how the information is formatted: what object
properties does each item define, which ones are strings, and so forth. In JSON
Schema, this facet is directly related to the validation operation introduced in
Chapter 8.

General-Purpose Concepts
Let’s look at every general-purpose concept from our example dataset and define and
publish schemas to the registry for each of them. For reusability purposes, we will
deploy these fundamental concepts under a directory called common. By the end of
this section, the schema registry will look like Figure 12-1.

Second Facet: Structure | 215



Figure 12-1. We will be deploying and listing the new concepts covered in this chapter to
the JSON Schema registry we published in Chapter 11.

As you will see, many of these fundamental concepts are already
defined by well-known standard and specification bodies like the
International Organization for Standardization (ISO), the Ameri‐
can National Standards Institute (ANSI), and the Internet Engi‐
neering Task Force (IETF). As we do in this chapter, we strongly
suggest following these universal standards and specifications
whenever possible instead of inventing your own. These organiza‐
tions provide guidance on an enormous number of concepts to
ensure interoperability.

Instead of creating schemas for each individual concept that reference one another,
it might be tempting to define all of the concepts directly within a single schema in
a monolithic manner. While it is technically possible, we recommend breaking down
your schemas like we do in this chapter so that each of them describes a single atomic
entity in your registry. This way, you force yourself to think about what each of your
concepts represents and promote reusability, as you never know how your concepts
may end up being used in the future.
As you will see in Chapter 14, it is always possible to dereference a JSON Schema into
a single monolithic entity through a process called JSON Schema bundling.

216 | Chapter 12: Designing Data Products Using JSON Schema



Timestamp
The first field in our example dataset is a string that looks like
"2023-08-21T13:26:33Z". This kind of string corresponds to the Basic Format date-
time Coordinated Universal Time (UTC) type defined by the ISO 8601 Date and
Time Format standard. A basic JSON Schema called timestamp that validates this
kind of string with a regular expression may look like this:
{
  "$id": "https://schemas.unify101.com/common/timestamp/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "string",
  "pattern": "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$"
}

Note that our regular expression is optimized for conciseness over correctness for
the sake of the illustration. For example, this regular expression defines months as a
sequence of two digits without disallowing invalid months like 00 or 13. We leave a
more exhaustive regular expression as an exercise for the reader.

Regular expressions may look intimidating at first. If you are
not familiar with them, we recommend consulting these excellent
resources:

• Mastering Regular Expressions by Jeffrey E. F. Friedl (O’Reilly,
2006)

• Regular Expressions Cookbook by Jan Goyvaerts and Steven
Levithan (O’Reilly, 2012).

IP address
The next field corresponds to the Internet Protocol (IP) address of the website visitor.
(For example: 167.16.122.17.) For simplicity, we are only concerned with IP version
4 addresses defined by the IETF RFC 791 Internet Protocol specification. A basic
JSON Schema called ipv4 that validates these kind of IP addresses with a simple
regular expression may look like this:
{
  "$id": "https://schemas.unify101.com/common/ipv4/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "string",
  "pattern": "^((25[0-5]|(2[0-4]|1\\d|[1-9]|)\\d)\\.?\\b){4}$"
}

Email
The format for email addresses we all use and love is defined by the IETF RFC 5322
Internet Message Format specification. (For example: gator@example.com.) Validating

Second Facet: Structure | 217



email address strings is a complex, seldom-needed affair. If you really need to ensure
an email address is valid, you should send an email with a validation link.
A basic JSON Schema called email that declares a regular expression optimized for
simplicity may look like this:
{
  "$id": "https://schemas.unify101.com/common/email/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "string",
  "pattern": "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"
}

Note that in our example dataset, the email property may be unset. However, this is
not the place for encoding this fact, as the email concept must stay agnostic to how
it is used. Instead, we will declare email optionality in the schema that will reference
this one.

US state
The next concept determines the United States state code that the user is connecting
from. (For example, "CO" for Colorado.) US two-letter state codes are defined by the
ANSI INCITS 38-2009 standard. A JSON Schema called us-state that defines an
enumeration of possible state codes may look like this:
{
  "$id": "https://schemas.unify101.com/common/us-state/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "enum": [
    "AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",
    "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",
    "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",
    "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",
    "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"
  ]
}

Cost and currency
The concepts of monetary cost and currency are tightly coupled because talking
about cost without knowing what currency we are referring to is meaningless; for this
reason, we will consider cost and currency together. The idea is to define currency
first and then define cost as a composite type that associates a given amount with a
currency.
You are probably familiar with currency codes like USD, GBP, and EUR. The full
list of world currencies is defined by the ISO 4217 currency codes standard. A JSON
Schema called currency that defines an enumeration of possible currency codes may
look like this:

218 | Chapter 12: Designing Data Products Using JSON Schema



{
  "$id": "https://schemas.unify101.com/common/currency/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "enum": [
    "AED", "AFN", "ALL", "AMD", "ANG", "AOA", "ARS", "AUD", "AWG", "AZN",
    "BAM", "BBD", "BDT", "BGN", "BHD", "BIF", "BMD", "BND", "BOB", "BOV",
    "BRL", "BSD", "BTN", "BWP", "BYN", "BZD", "CAD", "CDF", "CHE", "CHF",
    "CHW", "CLF", "CLP", "CNY", "COP", "COU", "CRC", "CUC", "CUP", "CVE",
    "CZK", "DJF", "DKK", "DOP", "DZD", "EGP", "ERN", "ETB", "EUR", "FJD",
    "FKP", "GBP", "GEL", "GHS", "GIP", "GMD", "GNF", "GTQ", "GYD", "HKD",
    "HNL", "HRK", "HTG", "HUF", "IDR", "ILS", "INR", "IQD", "IRR", "ISK",
    "JMD", "JOD", "JPY", "KES", "KGS", "KHR", "KMF", "KPW", "KRW", "KWD",
    "KYD", "KZT", "LAK", "LBP", "LKR", "LRD", "LSL", "LYD", "MAD", "MDL",
    "MGA", "MKD", "MMK", "MNT", "MOP", "MRU", "MUR", "MVR", "MWK", "MXN",
    "MXV", "MYR", "MZN", "NAD", "NGN", "NIO", "NOK", "NPR", "NZD", "OMR",
    "PAB", "PEN", "PGK", "PHP", "PKR", "PLN", "PYG", "QAR", "RON", "RSD",
    "RUB", "RWF", "SAR", "SBD", "SCR", "SDG", "SEK", "SGD", "SHP", "SLL",
    "SOS", "SRD", "SSP", "STN", "SVC", "SYP", "SZL", "THB", "TJS", "TMT",
    "TND", "TOP", "TRY", "TTD", "TWD", "TZS", "UAH", "UGX", "USD", "USN",
    "UYI", "UYU", "UYW", "UZS", "VES", "VND", "VUV", "WST", "XAF", "XAG",
    "XAU", "XBA", "XBB", "XBC", "XBD", "XCD", "XDR", "XOF", "XPD", "XPF",
    "XPT", "XSU", "XTS", "XUA", "XXX", "YER", "ZAR", "ZMW", "ZWL"
  ]
}

With this JSON Schema deployed and accessible over the registry, let’s turn our
attention to a composite JSON Schema that consumes the currency schema and
introduces the concept of a monetary amount. A composite JSON Schema called
price may look like this:
{
  "$id": "https://schemas.unify101.com/common/price/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "required": [ "amount", "currency" ],
  "properties": {
    "amount": { "type": "number", "minimum": 0 },
    "currency": { 
      "$ref": "https://schemas.unify101.com/common/currency/v1.json" 
    }
  }
}

Second Facet: Structure | 219



The price schema defines a JSON object with two required properties: amount and
currency. The amount property is a positive number, as price amounts cannot be
negative. The currency property uses the $ref keyword to point to the currency
schema we defined before, using its absolute URL.
Here is an example JSON object that successfully validates against the price schema:
{ "amount": 99.9, "currency": "USD" }

Application-Specific Concepts
So far, we have used JSON Schema to define six concepts: timestamp, ipv4, email,
us-state, currency, and price. We call these general-purpose concepts as a wide
range of JSON Schema applications could use them without modification.
Apart from general-purpose concepts, we have one concept that is application spe‐
cific: the milestones the user can achieve while filling out our fictitious website
sign-up form. This concept comes from Chapter 10, where we defined the flow as:

1. User visits the landing page.
2. User fills out contact information.
3. User fills out billing information.
4. User agrees to terms and conditions.
5. User signs up.
6. User invites their friends.

The CSV dataset at the beginning of this chapter maps these milestones to a set
of string identifiers. A JSON Schema called website-milestone that defines these
milestones may look like this (we put this schema into the intelligence-ai directory of
our JSON Schema registry):
{
  "$id": "https://schemas.unify101.com/intelligence-ai/website-milestone/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "enum": [
    "visit_landing_page", "set_contact_info", "set_billing_info",
    "agree_to_terms", "sign_up", "invite_friends"
  ]
}

Dataset Entries
Right now, we have JSON Schemas that individually describe every column of our
example dataset. The next step is to create a composite JSON Schema that pulls the
other schemas together to describe a complete entry of our dataset.

220 | Chapter 12: Designing Data Products Using JSON Schema



If you recall the dataset at the beginning of this chapter and its JSON representation,
each entry looks like this, where the email property is optional:
{
  "timestamp": "2023-08-30T13:30:16Z",
  "ip": "182.28.182.59",
  "email": "gator@example.com",
  "state": "CA",
  "milestone": "set_contact_info",
  "const": { "amount": 0, "currency": "USD" }
}

Here is a basic JSON Schema called signup-analytics-entry that describes such
JSON objects:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/
v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "required": [ "timestamp", "ip", "state", "milestone", "cost" ],
  "properties": {
    "timestamp": { "$ref": https://schemas.unify101.com/common/timestamp/v1.json" },
    "ip": { "$ref": "https://schemas.unify101.com/common/ipv4/v1.json" },
    "email": { "$ref": "https://schemas.unify101.com/common/email/v1.json" },
    "state": { "$ref": "https://schemas.unify101.com/common/us-state/v1.json" },
    "milestone": { 
      "$ref": "https://schemas.unify101.com/intelligence-ai/website-milestone/
v1.json" },
    "cost": { "$ref": "https://schemas.unify101.com/common/price/v1.json" }
  }
}

This JSON Schema describes an object with six properties: timestamp, ip, email,
state, milestone, and cost. Each of these properties references another JSON
Schema created in this chapter. To represent its optionality, the email property is
not present in the array of required properties.

The Dataset Schema
A dataset is a sequence of events. Therefore, we are missing one last piece: a JSON
Schema that represents an array of entries. This final schema, which describes a JSON
array whose items are signup analytics entries, will be called signup-analytics:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "array",
  "items": {
    "$ref": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/

Second Facet: Structure | 221



v1.json"
  }
}

At this point, you can ensure structural integrity of the example dataset presented at
the beginning of this chapter. One less facet to worry about!

Third Facet: Meaning
The meaning facet encompasses semantics about the data in each entry. For example,
our dataset entries contain an optional email address; not only can we structurally
validate an email string as we did during the structure facet, but we can convey that
this field represents the abstract idea of an email address, independently of the name
we chose for the concept or its property name in the entry. As another semantics
example, we could give the email entry property a user-readable description like
User’s Email Address.
JSON Schema supports these kinds of semantics through a rich set of annotation key‐
words. The process of extracting semantics from a piece of data given a schema corre‐
sponds to the annotation extraction fundamental operation described in Chapter 8.
Table 12-2 lists the JSON Schema keywords that you will likely use when implement‐
ing the meaning facet along with their vocabulary of origin. Recall from Chapter 5
that in JSON Schema, a vocabulary is a group of keywords defined together.

Table 12-2. A set of commonly used annotation keywords from the Meta Data and Format
Annotation official JSON Schema vocabularies

Keyword Vocabulary Description
title Meta Data A preferably short description about the purpose of the instance described by the

schema
description Meta Data An explanation about the purpose of the instance described by the schema
default Meta Data Used to supply a default JSON value associated with a particular schema
deprecated Meta Data Indicates that applications should refrain from using the declared property
examples Meta Data Used to provide sample JSON values associated with a particular schema for the

purpose of illustrating usage
readOnly Meta Data Indicates that the value of the instance is managed exclusively by the owning

authority and attempts by an application to modify the value of this property are
expected to be ignored or rejected by that owning authority

writeOnly Meta Data Indicates that the value is never present when the instance is retrieved from the
owning authority

format Format Annotation Define semantic information about a string instance, without performing validation

In this section, we will revise the schemas we created during the structure facet, add
meaning information, and publish new versions to the registry.

222 | Chapter 12: Designing Data Products Using JSON Schema



Timestamp
For the timestamp schema, we can declare a title, a description, and a set of examples.
Though computers will ignore it, we can add a URL to the specification that defines
these kinds of timestamps using the $comment keyword for human consumption. We
can also use the format keyword to express that this concept represents a date-time
string. You can see the list of formats recognized by JSON Schema by looking at the
documentation for the Format Annotation vocabulary.
With these changes, the version 2 of the timestamp schema may look like this. The
properties of the schema that have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/common/timestamp/v2.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$comment": "https://www.iso.org/iso-8601-date-and-time-format.html",
  "title": "ISO 8601 timestamp in Basic Format",
  "description": "The go-to date and time format at Unify101",
  "format": "date-time",
  "examples": [ "2023-08-21T13:24:49Z", "2023-08-21T13:26:33Z" ],
  "type": "string",
  "pattern": "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$"
}

The title keyword is meant to provide a concise description of
an instance that successfully validates against the given schema. We
recommend using a noun that is not prefixed with A or An, similar
to how you would name a class when writing in an object-oriented
programming language. For example: Timestamp instead of A time‐
stamp.

Once your concepts have this kind of information, you can extend the landing page
of your JSON Schema registry to present the concept titles, surface their examples,
and more.

IP Address
For the ipv4 schema, we can declare a title, a description, a set of examples, a format
specifier, and a URL to the specification that defines IP version 4 addresses, just as we
did for the timestamp schema.
With these changes, the version 2 of the ipv4 schema may look like this. The
properties of the schema that have been changed are in bold:

Third Facet: Meaning | 223



{
  "$id": "https://schemas.unify101.com/common/ipv4/v2.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$comment": "https://datatracker.ietf.org/doc/html/rfc791",
  "title": "IPv4 address",
  "description": "As defined by IETF RFC 791",
  "format": "ipv4",
  "examples": [ "192.168.0.1", "172.16.0.254" ],
  "type": "string",
  "pattern": "^(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\
\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.
(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"
}

Email
For the email schema, we can declare a title, a description, a set of examples, a format
specifier, and a URL to the specification that defines email addresses, just as we did
for the timestamp and ipv4 schemas.
With these changes, the version 2 of the email schema may look like this. The
properties of the schema that have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/common/email/v2.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$comment": "https://www.rfc-editor.org/rfc/rfc5322",
  "title": "Email address",
  "description": "As defined by IETF RFC 5322",
  "format": "email",
  "examples": [
    "user@example.com",
    "john.doe123@subdomain.domain.com"
  ],  
  "type": "string",
  "pattern": "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"
}

US State
The JSON Schema Format Annotation vocabulary does not define a format that
corresponds to a US state. For this reason, we will avoid the format keyword but still
declare a title, a description, a set of examples, and a URL to the specification that
defines US states two-letter codes.

224 | Chapter 12: Designing Data Products Using JSON Schema



The JSON Schema specification defines a list of known values
for the format keyword. While not commonly seen in practice,
the specification allows schema writers to set this keyword to an
arbitrary value as long as the consumers of the schema understand
it. In this example, we could set the format keyword to a made
up us-state JSON string. However, in this book we recommend
against using custom formats, as those are by definition nonstan‐
dardized and thus a potential source of ambiguity.

With these changes, the version 2 of the us-state schema may look like this. The
properties of the schema that have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/common/us-states/v2.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$comment": "https://webstore.ansi.org/standards/incits/incits382009",
  "title": "United States 2-letter state code",
  "description": "As defined by ANSI INCITS 38-2009",
  "examples": [ "CA", "NY" ],
    "enum": [
    "AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",
    "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",
    "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",
    "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",
    "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"
  ]
}

Currency
The additions to the currency schema are similar to the ones we added to the
us-state schema: we will declare a title, a description, a set of examples, and a URL
to the specification that defines currency three-letter codes. With these changes, the
version 2 of the currency schema may look like this.
The properties of the schema that have been changed are in bold, and the enumera‐
tion of currency codes has been summarized for brevity purposes:

Third Facet: Meaning | 225



{
  "$id": "https://schemas.unify101.com/common/currency/v2.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$comment": "https://www.iso.org/iso-4217-currency-codes.html",
  "title": "Currency 3-letter code",
  "description": "As defined by ISO 4217",
  "examples": [ "USD", "GBP", "EUR" ],
  "enum": [
    "AED", "AFN", "ALL", "AMD", "ANG", "AOA", "ARS", "AUD", "AWG", "AZN",
    ...
    "XPT", "XSU", "XTS", "XUA", "XXX", "YER", "ZAR", "ZMW", "ZWL"
  ]
}

Price
The price schema represents a composite concept: the conjunction of a currency
code with a monetary amount. For these types of schemas, we can also add human-
friendly titles to the properties it defines. In this case, we can describe what the
amount and currency properties mean. We will also update the currency reference
to import the new version of the currency concept we just created and get that
additional meaning as well.
Version 2 of the price schema may look like this. The properties of the schema that
have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/common/price/v2.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Monetary price quantity",
  "description": "The go-to price type at Unify101",
  "examples": [
    { "amount": 99.9, "currency": "EUR" },
    { "amount": 15, "currency": "USD" }
  ],
  "type": "object",
  "required": [ "amount", "currency" ],
  "properties": {
    "amount": { 
      "title": "The price amount",
      "type": "number",
      "minimum": 0 
    },
    "currency": {
      "title": "The price currency",
      "$ref": "https://schemas.unify101.com/common/currency/v2.json" 
    }
  }
}

226 | Chapter 12: Designing Data Products Using JSON Schema



Milestone
For the website-milestone schema, we can declare a title, a description, and a set of
examples just as we did for all our revised schemas so far.
The version 2 of the website-milestone schema may look like this. The properties of
the schema that have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/website-milestone/v2.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Website milestone in the Intelligence.AI sign-up form completion jour-
ney",
  "description": "Taken from Chapter 11 and 12 of our book",
  "examples": [ "set_contact_info", "invite_friends" ],
  "enum": [
    "visit_landing_page", "set_contact_info", "set_billing_info",
    "agree_to_terms", "sign_up", "invite_friends"
  ]
}

Analytics Entry
The signup-analytics-entry schema describes JSON object entries in the final
dataset. Because this schema also describes a composite type, its additions are similar
to the ones for the price schema: we will declare a title, a description, and an
example at the top level of the schema, but also add titles for every property in
the object. As before, we will upgrade our schema references to point to the newer
versions.
Version 2 of the signup-analytics-entry schema may look like this. The properties
of the schema that have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/
v2.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Analytics entry for the fictitious Intelligence.AI sign-up process",
  "description": "Taken from Chapter 11 and 12 of our book",
  "examples": [
    {
      "timestamp": "2023-08-30T13:30:16Z",
      "ip": "182.28.182.59",
      "email": "gator@example.com",
      "state": "CA",
      "milestone": "set_contact_info",
      "const": { "amount": 0, "currency": "USD" }
    }
  ],
  "type": "object",
  "required": [ "timestamp", "ip", "state", "milestone", "cost" ],

Third Facet: Meaning | 227



  "properties": {
    "timestamp": { 
      "title": "The time of the entry event",
      "$ref": "https://schemas.unify101.com/common/timestamp/v2.json" 
    },
    "ip": { 
      "title": "The IP address of the user",
      "$ref": "https://schemas.unify101.com/common/ipv4/v2.json" 
    },
    "email": { 
      "title": "The email address of the user, if known",
      "$ref": "https://schemas.unify101.com/common/email/v2.json" 
    },
    "state": { 
      "title": "The US state where the user is connecting from",
      "$ref": "https://schemas.unify101.com/common/us-state/v2.json" 
    },
    "milestone": { 
      "title": "The sign-up milestone the user completed",
      "$ref": "https://schemas.unify101.com/intelligence-ai/website-milestone/
v2.json" 
    },
    "cost": { 
      "title": "The cost of acquiring the user for completing this milestone",
      "$ref": "https://schemas.unify101.com/common/price/v2.json" 
    }
  }
}

By now, you have three out of four facets covered. You can not only validate that
your datasets conform to your concepts, but you can also extract useful metadata
out of them as needed. For example, these annotations can help you generate dataset
documentation, produce navigation user interfaces, generate rich forms for inputting
new data, and more.

Fourth Facet: Context
The final facet we will look at is context. The meaning and context facets may look
similar on the surface, as they are both related to metadata. However, whereas the
meaning facet is concerned with metadata about each entry in the dataset, context is
concerned with metadata about the dataset as a whole, such as its name and purpose.
To implement this facet, we will use the same JSON Schema keywords we used for
the meaning facet. However, instead of applying them to concept or entry schemas,
we will apply them to the JSON Schema that defines the dataset as a whole, which we
introduced at the end of the structure section (see “Second Facet: Structure” on page
215).

228 | Chapter 12: Designing Data Products Using JSON Schema



The Signup Analytics Schema
For the sake of this example, we will declare our dataset to be a static snapshot
of signup analytics taken at the end of August 2023. As such, we will declare our
dataset as read-only. We will also declare a human-readable title and description that
represent our intent. Finally, we will update the signup-analytics-entry schema
reference to point to the new version we published in the last section.
With these changes, the version 2 of the signup-analytics schema may look like
this. As before, the properties of the schema that have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics/v2.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "A dataset snapshot of signup analytics by milestone on August 2023",
  "description": "Taken from Chapter 12 of our book",
  "readOnly": true,
  "type": "array",
  "items": {
    "$ref": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/
v2.json"
  }
}

After these simple additions, you have created a data product that covers the four
facets introduced in Chapter 4.

Summary
In this chapter, we took baby steps to cover every facet of a well-designed data
product using JSON Schema. Defining every concept in detail may feel tedious at
first, but the results compound over time as more and more concepts are reused
across datasets, getting your organization closer to alignment. The authors of this
book found the process of defining hierarchies of concepts essential to surfacing
knowledge gaps, implicit constraints, and common sources of confusion.

Automated Schema Extraction
Throughout this exercise, you witnessed the power and expressiveness of JSON
Schema to cover three out of the four facets of data products introduced in Chapter 4.
These capabilities come with a learning curve; we covered fundamentals of JSON
Schema in Chapters 5 and 8. The increased interest in JSON Schema in academia and
the rise of AI will only make learning it easier.

Summary | 229



For example, we recommend checking out JSONoid (see Figure 12-2), an open
source and free-to-use project based on research at the Rochester Institute of Tech‐
nology1 to automatically infer JSON Schemas out of existing data. Compared to other
similar tools, we found that JSONoid already gets very close to what a skilled schema
writer would produce. Even more, JSONoid is working on automatically producing
large language model (LLM)-based JSON Schema annotations to automatically sup‐
port the meaning facet.

Figure 12-2. A screenshot of using the JSONoid online tool to infer a 2020-12 JSON
Schema out of a dataset of US senators.

1 Michael J. Mior, “JSONoid: Monoid-Based Enrichment for Configurable and Scalable Data-Driven Schema
Discovery,” arXiv, July 6, 2023.

230 | Chapter 12: Designing Data Products Using JSON Schema



We also found ChatGPT (especially ChatGPT-4) to be useful as a companion to aid in
schema writing. Of course, we advise you to not blindly trust the schemas produced
by any of these tools; instead, use them as a foundation and apply your existing
organizational and JSON Schema knowledge on top.

Next Steps
The attentive reader might have realized that while the official JSON Schema vocabu‐
laries perform well on the structure and meaning facet, they are not expressive enough
for the context facet. For example, Chapter 4 suggests documenting who created the
dataset, when the dataset was created, who maintains the dataset, and more. There
are no available keywords to support these use cases yet. However, this is not a
hard blocker! In the next chapter, you will learn how to tap into the JSON Schema
vocabulary system to define your own custom keywords that address any limitation
you encounter.

Summary | 231



CHAPTER 13
Extending JSON Schema

Once we accept our limits, we go beyond them.
—Albert Einstein

JSON Schema is used in a wide range of disparate use cases, from UI generation to
data serialization. Given its wide applicability, the JSON Schema organization cannot
possibly foresee every required feature. While the vocabularies that the JSON Schema
organization defines are enormously powerful, there are cases for which you need to
look beyond what’s offered out of the box. You saw a glimpse of this in Chapter 12,
when JSON Schema didn’t provide keywords for declaring some of the metadata we
wanted for the context facet of our example data product.
In Chapter 5, you learned an important skill: how to methodically understand every
JSON Schema and consult documentation for keywords and vocabularies you haven’t
seen before. This chapter builds on that to show how to define custom keywords that
can do almost anything you want.
There are two ways in which you can extend JSON Schema:

1. Exploit how JSON Schema collects annotations for unknown keywords (what we
refer to as the simple case).

2. Properly define and publish a JSON Schema vocabulary (what we call the com‐
plex case).

Although the simple case is easy to get going, it imposes limitations on how much
you can make your keywords do.

233



Simple Case: Unknown Keywords
When performing validation, JSON Schema implementations will silently ignore any
keywords they do not recognize. This fact can be easily exploited to add arbitrary
metadata to schemas without the need to define a vocabulary beforehand. For exam‐
ple, you can add a foobar property to your schemas without causing any harm. The
official JSON Schema 2020-12 dialect will not recognize this keyword as coming from
any of its imported vocabularies and will ignore it.
Continuing the topic of designing data products with JSON Schema from Chapter 12,
let’s consider the following JSON Schema that describes a set of positive integers.
It uses the made-up keywords documentation, authors, and license, which corre‐
spond to the context facet of data products:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/example-dataset/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "An example dataset of positive integers that uses unknown keywords",
  "description": "For Chapter 13 of our book",
  "readOnly": true,
  "documentation": "https://example.com/my-dataset-docs",
  "authors": [ 
    "Juan Cruz Viotti <juan@example.com>", 
    "Ron Itelman <ron@example.com>" 
  ],
  "license": "CC-BY-4.0",
  "type": "array",
  "items": {
    "type": "integer",
    "minimum": 0
  }
}

The JSON Schema 2020-12 official dialect does not know anything about keywords
called documentation, authors, or license, as they are not defined by any of the
vocabularies in use by the dialect.

Extracting Unknown Keywords as Annotations
Often, we want our systems to do something with the new keywords we have
declared. While you can write code that attempts to find occurrences of your cus‐
tom keywords across subschemas—which is hard to do correctly—there is a much
easier way. Conveniently, when JSON Schema implementations encounter unknown
keywords, they can collect them as annotations as part of the annotation extraction
process discussed in Chapter 8.

234 | Chapter 13: Extending JSON Schema



For example, the output of evaluating the preceding schema against a valid instance
with a JSON Schema implementation configured to extract annotations will contain
the following outcomes. Note that the annotation values correspond to the values we
set the unknown keywords to.
An annotation for the unknown documentation keyword:
{
  "keywordLocation": "/documentation",
  "absoluteKeywordLocation": "https://schemas.unify101.com/intelligence-ai/example-
dataset/v1.json/#/documentation",
  "instanceLocation": "",
  "annotation": "https://example.com/my-dataset-docs"
}

An annotation for the unknown authors keyword:
{
  "keywordLocation": "/authors",
  "absoluteKeywordLocation": "https://schemas.unify101.com/intelligence-ai/example-
dataset/v1.json/#/authors",
  "instanceLocation": "",
  "annotation": [ "Juan Cruz Viotti <juan@example.com>", "Ron Itelman <ron@exam-
ple.com>" ]
}

An annotation for the unknown license keyword:
{
  "keywordLocation": "/license",
  "absoluteKeywordLocation": "https://schemas.unify101.com/intelligence-ai/example-
dataset/v1.json/#/license",
  "instanceLocation": "",
  "annotation": "CC-BY-4.0"
}

A software system that generates documentation for our datasets could include logic
to search the evaluation output for these special annotations and render them as it
sees fit.

Pros and Cons of This Approach
As you can see, extending JSON Schema through unknown keyword annotations
does not require any prerequisite work. As a schema author, you can come up with
the keywords you want and start using them right away.

Simple Case: Unknown Keywords | 235



However, this approach comes with limitations; you cannot make your keywords
affect validation—at most they only result in annotations. Also, you cannot enforce
rules for the expected values of your keywords. For example, you cannot enforce that
the license keyword must only be set to a valid Software Package Data Exchange
(SPDX) license identifier.

Future versions of JSON Schema will make unknown keywords
forbidden unless they are prefixed by the x- string. For example:
"x-license". Keep this upcoming breaking change in mind when
purposely making use of unknown keywords. We are avoiding the
prefix in this book, as doing so is not a convention for the current
version of JSON Schema. Learn more on the official blog post
Custom Annotations Will Continue.

The alternative is to extend JSON Schema through the vocabulary system. Anybody,
including you, can extend JSON Schema as you see fit. In fact, the official vocabu‐
laries introduced by the JSON Schema specifications we’ve covered so far are not
special. They are based on the same extension mechanism that schema writers can
use too.

Complex Case: Authoring Vocabularies
This section explores how vocabulary specifications, meta-schemas, and implementa‐
tions are produced. We will see examples of existing third-party vocabularies and
define a context vocabulary ourselves.

The JSON Schema Vocabulary System
As you saw in Chapter 5, a vocabulary is a collection of interrelated keywords that
can be imported into a schema for use. Schema authors can create their own vocab‐
ularies to make JSON Schema do pretty much anything, with few restrictions. To
define a custom JSON Schema vocabulary, you should make use of three ingredients:
Specification

A document that unambiguously describes the intended behavior of the key‐
words defined in the new vocabulary

236 | Chapter 13: Extending JSON Schema



Meta-schema
A JSON Schema that validates the expected syntax of the keywords defined in the
new vocabulary

Implementation
A JSON Schema implementation that is extended to support the new vocabu‐
lary and make your new keywords behave in the way you described in the
specification

Each of these orthogonal ingredients has its key purpose; the specification covers the
semantics and syntax of the vocabulary, the meta-schema validates the syntax of the
vocabulary, and the implementation brings the vocabulary into reality.

The only notable restriction when defining new keywords is that
they cannot start with $ (dollar sign), like $foobar. This prefix
is reserved for the Core official vocabulary, which defines the
vocabulary system itself.

Step 1: Writing a Specification
Arguably, the most important part of a vocabulary is its specification. A specification
is meant to unambiguously describe how each of the new keywords must behave in
such a way that any developer could implement every keyword from the vocabulary
in an interoperable manner.
A vocabulary specification is written as prose, but JSON Schema does not state
how a specification must be written. You may use HTML, Markdown, LaTeX, or
any other markup language of your choice. While most vocabulary specifications
are self-published on the web or to source control providers such as GitHub (see
Figure 13-1), you can also publish vocabularies to a standards body such as the IETF.

Complex Case: Authoring Vocabularies | 237



Figure 13-1. An example specification of a third-party vocabulary that defines an
advanced mechanism for identifying uniqueness of array items, deployed to a website
powered by GitHub Pages. This vocabulary defines a single keyword called uniqueKeys.

Vocabulary identifiers
A vocabulary is uniquely identified by a URI. For example, the Applicator vocabulary
from the JSON Schema 2020-12 dialect is uniquely identified by the following URI:
https://json-schema.org/draft/2020-12/vocab/applicator.
While not mandatory by the JSON Schema specification, for user convenience we
recommend a vocabulary URI to resolve or redirect to the vocabulary specification
when visited on a web browser. The JSON Schema official vocabularies do this to
some extent: if you paste the Applicator vocabulary URI into your web browser,
you will get redirected to the JSON Schema Core specification, which defines this
vocabulary among others.

238 | Chapter 13: Extending JSON Schema



Chapter 11 advised treating schemas as immutable resources and proposed a version‐
ing strategy for deploying different versions of the same schema under different
URLs. In the same way, vocabulary URIs are expected to be stable. If you are updating
your vocabulary in any significant way, you are expected to publish a new version of
the specification at a different URI.
Consider how JSON Schema versions its own official vocabularies. The latest two
versions of the Validation official vocabulary correspond to these URIs. The version
identifiers JSON Schema embeds into the URIs are in bold:

• https://json-schema.org/draft/2020-12/vocab/validation
• https://json-schema.org/draft/2019-09/vocab/validation

The context vocabulary specification
Using HTML, write a concise specification for a vocabulary called context that
defines our documentation, authors, and license keywords, then deploy it to our
unify101.com website using Cloudflare Pages. See Figures 13-2 and 13-3 for examples.

Figure 13-2. Let’s start by presenting the title of our custom vocabulary, a brief descrip‐
tion of it, and its vocabulary URI: https://schemas.unify101.com/vocab/context/v1.

Complex Case: Authoring Vocabularies | 239



Figure 13-3. The online specification lists every keyword introduced by the vocabulary, a
brief description for each keyword, and some concise examples.

Note that each keyword section clarifies that their value must be collected as annota‐
tion. When a developer attempts to implement support for this vocabulary, they will
know what to do. If you want your keywords to perform validation on the instance,
you must explain the expected validation constraints in the specification.

240 | Chapter 13: Extending JSON Schema



Learning how to write clear, unambiguous, and correct specifica‐
tions is a skill of its own. In the world of JSON Schema, there
are no better examples than the JSON Schema specifications them‐
selves. We recommend carefully reading them to understand the
language they use and how they approach explaining complex
keywords.

Step 2: Writing a Vocabulary Meta-Schema
You might remember from Chapter 5 that JSON Schema has been designed to
describe itself; a JSON Schema that describes other JSON Schemas is called a meta-
schema. Because it can impose constraints over its children’s schemas, a meta-schema
is the perfect medium for defining the syntax of keywords—for example, declaring
that a certain keyword can only be set to numeric values.

Official vocabularies meta-schemas
Every official JSON Schema vocabulary is associated with its own meta-schema (see
Table 13-1).

Table 13-1. The JSON Schema organization defines eight official vocabularies, each with
their respective meta-schema.

Vocabulary Meta-schema
Core https://json-schema.org/draft/2020-12/meta/core
Applicator https://json-schema.org/draft/2020-12/meta/applicator
Validation https://json-schema.org/draft/2020-12/meta/validation
Meta-Data https://json-schema.org/draft/2020-12/meta/meta-data
Format Annotation https://json-schema.org/draft/2020-12/meta/format-annotation
Unevaluated https://json-schema.org/draft/2020-12/meta/unevaluated
Content https://json-schema.org/draft/2020-12/meta/content
Format Assertion https://json-schema.org/draft/2020-12/meta/format-assertion

Try visiting the meta-schema URIs from Table 13-1 in your browser to see how the
syntax of the keywords you have been using all along is defined. For example, the
relevant parts of the Validation vocabulary meta-schema concerned with defining the
type keyword are as follows:
{
  "$id": "https://json-schema.org/draft/2020-12/meta/validation",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  ...
  "properties": {
    "type": { 
      "anyOf": [
        { "$ref": "#/$defs/simpleTypes" }, 

Complex Case: Authoring Vocabularies | 241



        {
          "type": "array",
          "items": { "$ref": "#/$defs/simpleTypes" }, 
          "minItems": 1,
          "uniqueItems": true
        }
      ]
    },
    ...
  },
  "$defs": {
    ...
    "simpleTypes": { 
      "enum": [ "array", "boolean", "integer", "null", "number", "object", 
"string" ]
    },
    ...
  }
}

This schema describes an object property called type . According to the schema,
its value is either an instance of the simpleTypes embedded definition  or a unique
array of instances of the simpleTypes embedded definition . If we take a look at this
definition under the $defs keyword , it declares the 7 possible values that we have
been using all along, like "string" and "number".

Remember the interplay of meta-schemas and specifications; meta-
schemas are just about syntax, whereas specifications also cover
semantics. In this example, note that the vocabulary meta-schema
only declares the syntax of the type keyword. What happens when
the user chooses one or more of these types is beyond the scope of
the meta-schema and within the realms of the specification instead.

SPDX licenses
The license keyword in our new vocabulary is meant to declare the license of a data
product. The license identifiers you’ve seen so far, such as "Apache-2.0" and "MIT",
are defined by the (SPDX) standard.
Before we continue, and just as we did in the last chapter, let’s publish a new concept
under common/license to model license identifiers so we can use it to describe
our license keyword. The SPDX standard lists hundreds of licenses, so here is a
summarized version for readability purposes:
{
  "$id": "https://schemas.unify101.com/common/license/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "License identifier",

242 | Chapter 13: Extending JSON Schema



  "description": "As defined by the Software Package Data Exchange (SPDX)",
  "$comment": "https://github.com/spdx/license-list-data/tree/v3.20",
  "anyOf": [
    { "title": "BSD Zero Clause License", "const": "0BSD" },
    ...
    { "title": "Zope Public License 2.1", "const": "ZPL-2.1" }
  ]
}

The context vocabulary meta-schema
Now that we know how JSON Schema defines meta-schemas for each of the official
vocabularies and we have all the concepts we need, let’s attempt to write a meta-
schema for our context vocabulary. We will host this meta-schema on our registry
under vocab/context/v1.json.
This meta-schema is rather complex, but we’ll break down the most important parts
together:
{
  "$id": "https://schemas.unify101.com/vocab/context/v1.json",  
  "$schema": "https://json-schema.org/draft/2020-12/schema", 
  "$dynamicAnchor": "meta", 
  "title": "A JSON Schema vocabulary for working with the 'context' facet of data 
products",
  "description": "Taken from Chapter 13 of our book",
  "properties": {
    "documentation": { 
      "title": "This keyword is set to a URI reference that points to human-
readable documentation for the given schema",
      "type": "string",
      "format": "uri-reference",
      "examples": [ "https://example.com/my-schema-documentation" ]
    },
    "authors": { 
      "title": "This keyword is set to an non-empty array of strings that determine 
the authors of the given schema",
      "description": "The format of each author string MUST follow the following 
format 'name <email>'",
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "string",
        "pattern": "^[A-Za-z\\s]+\\s<[^>]+>$"
      },
      "examples": [
        [ "John Doe <johndoe@example.com>" ],
        [ "John Doe <johndoe@example.com>", "Jane Doe <janedoe@example.com>" ]
      ]
    },
    "license": { 
      "title": "This keyword is set to a string that denotes a valid SPDX license 

Complex Case: Authoring Vocabularies | 243



identifier",
      "$ref": "https://schemas.unify101.com/common/license/v1.json", 
      "examples": [ "MIT", "Apache-2.0" ]
    }
  }
}

Setting schema identifiers.    As with other schemas we’ve created so far, we set the URI
identifier of the meta-schema . We are using the JSON Schema 2020-12 dialect to
define the meta-schema that describes our vocabulary, so we will set the $schema
keyword accordingly .

Configuring schema extension.    Next, we set the $dynamicAnchor keyword to the special
value meta . This is a requirement that results in the meta-schema being applied
at every subschema of its instances. In other words, by setting this keyword, JSON
Schema will attempt to validate every subschema against our meta-schema. If this
keyword is not set, or set to a value other than meta, our vocabulary meta-schema will
only apply to the top-level schema and nested uses of the documentation, authors,
and license keywords will not be validated.

The Meta Dynamic Anchor
Vocabularies that define keywords that introduce subschemas, such as properties
or items, face an interesting challenge—they need to validate a subschema without
knowing which other vocabularies are present in the dialect under use.
To solve this problem, $dynamicAnchor and $dynamicRef are advanced keywords
designed to support extending recursive schemas. The former declares an extension
point that can be dynamically referenced with the latter keyword.
When describing keywords that introduce subschemas, the official meta-schemas
dynamically reference an extension point called meta so that other vocabularies can
hook into it by defining a dynamic anchor with the same name.

The remaining parts should feel familiar. We use the properties keyword to describe
our documentation , authors , and license  keywords, making use of vali‐
dation keywords (like type), annotation keywords (like title, and examples), or
referencing existing schemas (like for SPDX license identifiers ).

Step 3: Extending an Implementation
The final ingredient in designing a custom vocabulary is support for it in at least one
JSON Schema implementation. Otherwise, no existing JSON Schema implementation
will be able to process the meta-schema we wrote in the last section (see Figure 13-4).

244 | Chapter 13: Extending JSON Schema



Figure 13-4. Using an online JSON Schema validator like Hyperjump, try writing a
simple schema that references our vocabulary meta-schema. The schema will be rejected,
as the implementation won’t be aware of the vocabulary URI we declared in our
meta-schema through the use of the $vocabulary keyword.

Diversity of JSON Schema implementations
Due to its immense popularity, JSON Schema has a very vibrant ecosystem. There
are dozens of open source JSON Schema implementations for almost every popular
programming language around, ranging from Python and Rust to Lua and Elixir.
In the context of this chapter, JSON Schema implementations offer different inter‐
faces for implementing new vocabularies, if any. Some of them provide convenient
ways to hook into keyword evaluation, some allow you to pass constructors that
declaratively introduce new vocabularies, some might require you to contribute back
to them instead, and some don’t support custom vocabularies at all.
This book cannot possibly cover how to extend every implementation out there.
Instead, we will choose one that you have seen before: Hyperjump. This implementa‐
tion is the recommended one for JavaScript on Node.js and web browsers, and it
is maintained by a core member of the JSON Schema organization. If you are not
using Hyperjump, or you are not using JavaScript, please consult the documentation
of your implementation of choice for how to extend it in the same way.
The code examples we discuss here use Hyperjump v1.5.1 running on Node.js v20.5.1
on macOS Ventura 13.5.1 on a MacBook Pro M1.

Extending Hyperjump
Hyperjump provides a convenient set of self-explanatory functions called addKeyword
and defineVocabulary. The addKeyword function takes a JavaScript object with three
properties, as shown in Table 13-2.

Complex Case: Authoring Vocabularies | 245



Table 13-2. The argument properties needed to call the Hyperjump addKeyword function
Property Description
id Hyperjump requires every keyword to be associated with a unique URI in the same spirit in which JSON

Schema uses URIs to identify schemas and vocabularies. These keyword URIs can be anything you want. For
example, if our vocabulary URI is https://schemas.unify101.com/vocab/context/v1, we may set the URI of the
documentation keyword to https://schemas.unify101.com/vocab/context/v1/documentation

compile For performance reasons, this function allows you to do any precomputation that only relies on the current
schema and its parent. For example, if you are defining a keyword that takes a regular expression, you can
precompile the regular expression string into a RegExp JavaScript object. That way, you only precompile it
once, no matter how many instances you validate against the schema.

interpret This function includes the business logic for applying the keyword to a given instance and reporting back on
the evaluation results.

annotation This function takes the keyword value and returns what the resulting annotation should be.

The three keywords we want to define, documentation, authors, and license, are
annotation-only keywords, so their definitions are trivial. We do not need to precom‐
pile keywords because interpreting them always returns true, and the annotation
result is the keyword value itself:
import { addKeyword } from "@hyperjump/json-schema/experimental";

addKeyword({
  id: "https://schemas.unify101.com/vocab/context/v1/documentation",
  compile: (schema, ast, parentSchema) => schema.value,
  interpret: (implies, instance, ast, dynamicAnchors, quiet) => true, 
  annotation: (value) => value
});

addKeyword({
  id: "https://schemas.unify101.com/vocab/context/v1/authors",
  compile: (schema, ast, parentSchema) => schema.value,
  interpret: (implies, instance, ast, dynamicAnchors, quiet) => true,
  annotation: (value) => value
});

addKeyword({
  id: "https://schemas.unify101.com/vocab/context/v1/license",
  compile: (schema, ast, parentSchema) => schema.value,
  interpret: (implies, instance, ast, dynamicAnchors, quiet) => true,
  annotation: (value) => value
});

The defineVocabulary function takes a vocabulary URI and a JavaScript object
that associates the vocabulary keyword names with the keyword URIs defined using
addKeyword. For our context vocabulary, it looks like this:
import { defineVocabulary } from "@hyperjump/json-schema/experimental";
// Make sure to define the keywords here before calling defineVocabulary!
// …

246 | Chapter 13: Extending JSON Schema



defineVocabulary("https://schemas.unify101.com/vocab/context/v1", {
  documentation: "https://schemas.unify101.com/vocab/context/v1/documentation",
  authors: "https://schemas.unify101.com/vocab/context/v1/authors",
  license: "https://schemas.unify101.com/vocab/context/v1/license"
});

At this point, we can use Hyperjump to validate instances against the vocabulary
meta-schema using the validate function, which takes a URI to a schema and an
instance value as arguments. Let’s try it out with the new documentation keyword:
import { validate } from "@hyperjump/json-schema/draft-2020-12";
// Make sure to define the keywords and vocabulary here before calling validate!
// …

const schema = {
  $schema: "https://json-schema.org/draft/2020-12/schema",
  documentation: "https://example.com/my-docs"
}

const result = await validate("https://schemas.unify101.com/vocab/context/v1.json", 
  schema);
if (result.valid) {
  console.log("OK! The instance is valid!");
}

Try modifying the documentation keyword value to something that is not a string.
When doing so, the success message will not be printed.

Consuming Vocabularies
We now have a vocabulary specification and a vocabulary meta-schema, and we
taught the Hyperjump JSON Schema implementation about it. However, there is
one missing piece of the puzzle before we can use it in practice: a JSON Schema
dialect that imports our vocabulary. If you remember from Chapter 5, vocabularies
are collections of keywords, and dialects are collections of vocabularies. Right now we
have a vocabulary, but there is no dialect that uses it yet.

Defining a Dialect
Dialects are also defined using meta-schemas. The dialect we have been using all
along in this book is https://json-schema.org/draft/2020-12/schema. Compared to a
vocabulary meta-schema, a dialect meta-schema is very simple. It declares the set
of vocabularies it imports using the $vocabulary keyword and references the vocabu‐
lary or dialect meta-schemas that correspond to all the imported vocabularies.

Consuming Vocabularies | 247



Vocabularies Are Not Transitive
The vocabularies defined by a meta-schema are not transitive. In other words, you
must explicitly define the $vocabulary keyword in every meta-schema to declare
the vocabularies in use. For example, if meta-schema A defines a set of vocabularies
and you define a meta-schema B that sets its dialect to meta-schema A, instances of
meta-schema B will not inherit the vocabularies set by meta-schema A.

Let’s create a dialect that extends JSON Schema 2020-12 and adds our new vocabulary
as required. We will publish this dialect to our registry under dialect/data-product:
{
  "$id": "https://schemas.unify101.com/dialect/data-product/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema", 
  "$dynamicAnchor": "meta",
  "$vocabulary": { 
    "https://json-schema.org/draft/2020-12/vocab/core": true,
    "https://json-schema.org/draft/2020-12/vocab/applicator": true,
    "https://json-schema.org/draft/2020-12/vocab/unevaluated": true,
    "https://json-schema.org/draft/2020-12/vocab/validation": true,
    "https://json-schema.org/draft/2020-12/vocab/meta-data": true,
    "https://json-schema.org/draft/2020-12/vocab/format-annotation": true,
    "https://json-schema.org/draft/2020-12/vocab/content": true,
    "https://schemas.unify101.com/vocab/context/v1": true 
  },
  "title": "A dialect based on 2020-12 for defining Data Products",
  "allOf": [ 
    { "$ref": "https://json-schema.org/draft/2020-12/schema" },
    { "$ref": "https://schemas.unify101.com/vocab/context/v1.json" } 
  ]
}

Our dialect meta-schema is described by the JSON Schema 2020-12 dialect . It
imports the same vocabularies as the official 2020-12 dialect  and also imports the
context vocabulary we defined in this chapter . This dialect meta-schema uses the
allOf logical operator  to include both the JSON Schema 2020-12 dialect itself and
the context vocabulary meta-schema . Essentially, we are saying: take the JSON
Schema 2020-12 dialect and extend it with the context vocabulary.

Now that you know how to define dialects, you know how to
consume third-party vocabularies that other people have made.
While some of these vocabularies come with their own convenient
dialects, you will often need to intermix various vocabularies from
different sources, which you can only accomplish by defining your
own dialects.

248 | Chapter 13: Extending JSON Schema



Making Use of the Dialect
Chapter 12 defined a data product called signup-analytics. Let’s deploy a new
version to the registry that makes use of our brand-new dialect and annotation
keywords. The properties of the schema that have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics/v3.json",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v1.json", 
  "title": "A dataset snapshot of signup analytics by milestone on August 2023",
  "description": "Taken from Chapter 13 of our book",
  "documentation": "https://learning.oreilly.com/library/view/unifying-business-
data/9781098144999/", 
  "authors": [ "Juan Cruz Viotti <juan@example.com>", "Ron Itelman <ron@exam-
ple.com>" ], 
  "license": "AGPL-3.0", 
  "readOnly": true,
  "type": "array",
  "items": {
    "$ref": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/
v2.json"
  }
}

Our revised data product definition now makes use of our new dialect meta-schema
, getting access to the new context vocabulary. For this data product, we will set

the documentation keyword to the URL of this book itself , we will set the authors
keyword to the authors of this book  and set the license keyword to the GNU
Affero General Public License (AGPL) version 3 .

Example: Extracting Annotations with Hyperjump
In the section “Extending Hyperjump,” we taught the Hyperjump JavaScript JSON
Schema implementation about our context vocabulary and used it to perform some
basic validation against the vocabulary meta-schema. Now that we have a dialect
using the new vocabulary, let’s build on the code we wrote before to extract the
annotations that correspond to our new vocabulary.

Adding the dialect
One caveat to be aware of is that while Hyperjump can download vocabulary meta-
schemas over HTTP, it does not support dynamically fetching dialect meta-schemas.
Instead, users are expected to manually declare them using the addSchema function.
In this example, we will define our dialect in place:
import { addSchema } from "@hyperjump/json-schema/draft-2020-12";

// Make sure to define the keywords and vocabulary here before calling addSchema!
// …

Consuming Vocabularies | 249



addSchema({
  "$id": "https://schemas.unify101.com/dialect/data-product/v1.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$dynamicAnchor": "meta",
  "$vocabulary": {
    "https://json-schema.org/draft/2020-12/vocab/core": true,
    "https://json-schema.org/draft/2020-12/vocab/applicator": true,
    "https://json-schema.org/draft/2020-12/vocab/unevaluated": true,
    "https://json-schema.org/draft/2020-12/vocab/validation": true,
    "https://json-schema.org/draft/2020-12/vocab/meta-data": true,
    "https://json-schema.org/draft/2020-12/vocab/format-annotation": true,
    "https://json-schema.org/draft/2020-12/vocab/content": true,
    "https://schemas.unify101.com/vocab/context/v1": true
  },
  "title": "A dialect based on 2020-12 for defining Data Products",
  "allOf": [
    { "$ref": "https://json-schema.org/draft/2020-12/schema" },
    { "$ref": "https://schemas.unify101.com/vocab/context/v1.json" }
  ]
});

Getting annotations
The annotate function takes as arguments the URI of a schema and an instance
value. The result is an object that you can query using the AnnotatedInstance.anno
tation function, which takes as arguments an annotation extraction result, the name
of the annotation to extract, and the dialect of the given schema.
For example, let’s extract the annotations of our updated data product schema,
https://schemas.unify101.com/intelligence-ai/signup-analytics/v3.json, and print its
documentation annotation, which we previously set to the URL of our book:
import { annotate } from '@hyperjump/json-schema/annotations/experimental';
import * as AnnotatedInstance from "@hyperjump/json-schema/annotated-instance/exper-
imental";
// Make sure to define the keywords, vocabulary, and dialect here 
// before calling annotate!
// …

const result = await annotate(
  "https://schemas.unify101.com/intelligence-ai/signup-analytics/v3.json", []);
const documentation = AnnotatedInstance.annotation(result, "documentation",
  "https://schemas.unify101.com/dialect/data-product/v1.json");
console.log(documentation[0]);

If you run this program, you will get back https://learning.oreilly.com/library/view/
unifying-business-data/9781098144999/.
Hyperjump also provides convenient functions to iterate over every collected
annotation and more. We encourage the interested reader to consult the project
documentation.

250 | Chapter 13: Extending JSON Schema



Summary
In this chapter, you discovered how to exploit the way JSON Schema treats unknown
keywords to add ad hoc annotations (what we referred to as the simple case),
and how to properly introduce new keywords through the vocabulary system (the
complex case). Although the simple case is easy to get going, it imposes limitations
on how much you can do. So if you are thinking of extending JSON Schema, we
recommend always making use of the vocabulary system.
Defining a new vocabulary consists of a specification, a vocabulary meta-schema, and
an extended implementation. Making use of a vocabulary requires the definition of
a dialect meta-schema. We went through every step of defining a vocabulary that
makes JSON Schema more capable of covering the context facet of a data product.
Armed with this knowledge, you are no longer limited to what the official JSON
Schema vocabularies can do—you can always define your own!
In Chapter 14, we will explore how JSON Schema can be extended to support a whole
new use case: defining datasets.

Summary | 251



CHAPTER 14
Introducing JSON Unify

Great things are done by a series of small things brought together.
—Vincent Van Gogh

In previous chapters, you saw the idea of a data product introduced in Chapter 4
come to life using JSON and JSON Schema. More specifically, given an example
JSON dataset, which corresponds to the data facet of a data product, we used JSON
Schema to describe the remaining three facets: structure, meaning, and context.
The attentive reader might have realized there is still one problem. According to
Chapter 4, a data product “encapsulates both the data and its packaging into a single,
self-contained object.” However, the data product we defined in Chapter 12 and
Chapter 13 is not standalone. The data lives separately from the schema, without
any connection between them, and the schema does not contain all the information
itself; it mostly references other schemas in our schema registry that do contain the
information.
The aim of this chapter is to give you a glimpse of how JSON Schema can be
extended, once again, to fix this last set of concerns. Rather than something to use
and deploy right now, consider this chapter to be a thought-provoking experiment
about the future of data products using JSON Schema. The authors of this book are
exploring these ideas under the JSON Unify name and hope to publish an open source
specification and implementation in the future.

Introducing the Dataset Vocabulary
If the core problem is collocating schema and data, what about embedding the data
into the schema itself? As we did in Chapter 13, let’s assume we created a new JSON
Schema vocabulary called dataset whose URI is https://schemas.unify101.com/vocab/
dataset/v1. This vocabulary defines two keywords: dataset and datasetSchema. The

253



dataset keyword can be set to a JSON array whose items are described by the schema
declared using the datasetSchema keyword.
Let’s also assume we published a new version of the data-product dialect from
Chapter 13 to make use of this new vocabulary. An example data product using this
new dialect may look like this:
{
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json",
  "title": "A an example dataset of positive integers",
  "description": "Taken from Chapter 14 of our book",
  "datasetSchema": { "type": "integer" }, 
  "dataset": [ 1, 2, 3, 4, 5 ] 
}

The preceding schema uses the new dataset keyword to define a short collection
of integers . Each entry successfully validates against the dataset schema , which
describes integers.

Revisiting the Signup Analytics Example
Assuming the presence of this new dataset keyword, a revised version of the website
signup analytics data product from Chapters 12 and 13 that includes the data facet
may look like the following. The properties of the schema that have been changed are
in bold:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics/v4.json",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "title": "A dataset snapshot of signup analytics by milestone on August 2023",
  "description": "Taken from Chapter 14 of our book",
  "documentation": "https://learning.oreilly.com/library/view/unifying-business-
data/9781098144999/", 
  "authors": [ "Juan Cruz Viotti <juan@example.com>", "Ron Itelman <ron@exam-
ple.com>" ], 
  "license": "AGPL-3.0", 
  "readOnly": true,
  "datasetSchema": {
    "$ref": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/
v2.json"
  },
  "dataset": [
    {
      "timestamp": "2023-08-21T13:24:49Z",
      "ip": "84.216.114.94",
      "state": "CO",
      "milestone": "visit_landing_page",
      "cost": { "amount": 0.92, "currency": "USD" }
    },
    ...

254 | Chapter 14: Introducing JSON Unify



    {
      "timestamp": "2023-08-30T13:51:31Z",
      "ip": "115.93.200.62",
      "email": "crove@gmail.com",
      "state": "NY",
      "milestone": "sign_up",
      "cost": { "amount": 0, "currency": "USD" }
    }
  ]
}

Now we finally have the four facets of a data product (context, structure, meaning,
and data) accessible from within a single schema. If the contents of the new dataset
keyword are collected as a JSON Schema annotation, then it is trivial to use the anno‐
tation extraction operation discussed in Chapter 8 and exemplified in Chapter 13 to
read the data out of a data product schema, as shown in Figure 14-1. This way, you
obtain the data through a standard JSON Schema mechanism instead of implement‐
ing custom logic that attempts to detect the presence of a dataset keyword.

JSON Schema Bundling
For maintainability and reusability purposes, the signup-analytics schema from
Chapter 12 does not directly describe its instances. Instead, it makes use of JSON
Schema remote referencing (using the $ref keyword introduced in Chapter 5) to pull
in the schema that describes entries in the dataset: signup-analytics-entry. But in
our example, it does not stop there. The signup-analytics-entry schema references
other schemas too, like timestamp, email, and price, which may even reference other
schemas themselves, like currency, as shown in Figure 14-2.
Modern JSON Schema implementations support remote referencing and can effort‐
lessly traverse networks of schema references of arbitrary depths, often efficiently
caching them along the way. However, what if you need to package a data product
as a standalone object that does not depend on any other schemas hosted over the
Internet? For example, you might want to embed the preresolved schema in an
application for performance and reliability reasons or deploy it to an air-gapped
environment.

JSON Schema Bundling | 255



Figure 14-1. If the contents of the dataset keyword produce an annotation, feeding a
schema that uses such a keyword into a JSON Schema implementation that is capable of
extracting annotations will result in a dataset annotation with the expected results.
256 | Chapter 14: Introducing JSON Unify



Figure 14-2. Nontrivial schemas often reference other schemas. In this example, the
signup-analytics schema references the signup-analytics-entry schema, which
references the price schema, which references the currency schema.

JSON Schema Bundling | 257



The Bundling Process
JSON Schema supports this use case through a process called JSON Schema bun‐
dling. As its name implies, bundling is about merging multiple schemas into one.
While it may sound intimidating, the bundling process is incredibly simple; when
encountering a remote reference, inline the contents of such schema using the $defs
keyword, setting the keys to the corresponding schema identifiers.
For example, consider the following example schema:
{
  "$id": "https://example.com/schema-1",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "array",
  "items": { "$ref": "https://example.com/schema-2" } 
}

That schema remotely references a fictitious schema hosted at https://example.com/
schema-2 , that may look like this:
{
  "$id": "https://example.com/schema-2",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "boolean"
}

To bundle https://example.com/schema-2 into https://example.com/schema-1, we cre‐
ate a new schema that declares the $defs keyword. In it, we add two object proper‐
ties whose keys are https://example.com/schema-1 and https://example.com/schema-2,
where their values correspond to the contents of such schemas. Finally, we make the
new schema reference https://example.com/schema-1.
{
  "$id": "https://example.com/bundled-schema-1",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$ref": "https://example.com/schema-1",
  "$defs": {
    "https://example.com/schema-1": {
      "$id": "https://example.com/schema-1",
      "$schema": "https://json-schema.org/draft/2020-12/schema",
      "type": "array",
      "items": { "$ref": "https://example.com/schema-2" }
    },
    "https://example.com/schema-2": {
      "$id": "https://example.com/schema-2",
      "$schema": "https://json-schema.org/draft/2020-12/schema",
      "type": "boolean"
    }
  }
}

258 | Chapter 14: Introducing JSON Unify



When a compliant JSON Schema implementation processes this new version of the
schema, it detects https://example.com/schema-2 within itself and will not attempt to
locate it elsewhere.

Why Does JSON Schema Bundling Work?
If you are curious, the reason JSON Schema bundling works is that when a JSON
Schema implementation processes a schema, it will first traverse it in search for
identifiable schema resources that were described using keywords such as $id and
$anchor. In the case of a bundled schema, an implementation will detect the children
schemas under the $defs keyword and make the corresponding associations. Next, an
implementation will start evaluating the schema. By the time remote references are
encountered, these child schemas will already be present in the internal registry, and
no HTTP requests will be necessary.

Bundling Our Example Data Product
If we run a JSON Schema bundler on the signup-analytics data product, we will get
a large but standalone schema that looks like this (ellipses used for brevity purposes):
{
  ...
  "$ref": "https://schemas.unify101.com/intelligence-ai/signup-analytics/v4.json",
  ...
  "$defs": {
    "https://schemas.unify101.com/intelligence-ai/signup-analytics/v4.json": {
      "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/
v4.json",
      ...
      "datasetSchema": { 
        "$ref": "https://schemas.unify101.com/intelligence-ai/signup-analytics-
entry/v2.json"
      },
      "dataset": [ ... ],
    },
    "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/v2.json": {
      "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/
v2.json",
      ...
    },
    "https://schemas.unify101.com/common/timestamp/v2.json": {
      "$id": "https://schemas.unify101.com/common/timestamp/v2.json",
      ...
    },
    "https://schemas.unify101.com/common/ipv4/v2.json": {
      "$id": "https://schemas.unify101.com/common/ipv4/v2.json",
      ...
    },

JSON Schema Bundling | 259



    "https://schemas.unify101.com/common/email/v2.json": {
      "$id": "https://schemas.unify101.com/common/email/v2.json",
      ...
    },
    "https://schemas.unify101.com/common/us-states/v2.json": {
      "$id": "https://schemas.unify101.com/common/us-states/v2.json",
      ...
    },
    "https://schemas.unify101.com/intelligence-ai/website-milestone/v2.json": {
      "$id": "https://schemas.unify101.com/intelligence-ai/website-milestone/
v2.json",
      ...
    },
    "https://schemas.unify101.com/common/price/v2.json": {
      "$id": "https://schemas.unify101.com/common/price/v2.json",
      ...
    },
    "https://schemas.unify101.com/common/currency/v2.json": {
      "$id": "https://schemas.unify101.com/common/currency/v2.json",
      ...
    }
  }
}

This is a true data product that, according to our definition in Chapter 4, encapsulates
both the data and its packaging into a single, self-contained object.

Bundling Meta-Schemas
While not typically done in practice, as most modern JSON Schema implementations
are dialect and vocabulary aware—both need to be defined in the implementation
beforehand—it is possible to bundle meta-schemas. Doing so with our example data
product would result in the following schemas being bundled:

• https://schemas.unify101.com/dialect/data-product/v2.json
• https://json-schema.org/draft/2020-12/schema
• https://json-schema.org/draft/2020-12/meta/core
• https://json-schema.org/draft/2020-12/meta/applicator
• https://json-schema.org/draft/2020-12/meta/unevaluated
• https://json-schema.org/draft/2020-12/meta/validation
• https://json-schema.org/draft/2020-12/meta/meta-data
• https://json-schema.org/draft/2020-12/meta/format-annotation

260 | Chapter 14: Introducing JSON Unify



• https://json-schema.org/draft/2020-12/meta/content
• https://schemas.unify101.com/vocab/context/v1.json
• https://schemas.unify101.com/vocab/dataset/v1.json

Referencing Remote Data
We’ve seen that schemas may remotely reference other schemas. If needed, schema
users may resolve remote references in advance through the bundling process. The
same reasoning can be applied to the dataset keyword. We can define a datasetRef
keyword to declare a URI where the actual data is hosted. If an implementation that
is aware of the dataset vocabulary encounters a remote reference, it may perform an
HTTP request to fetch the data, then prefetch the remotely located data during the
bundling process and inline it as a dataset JSON array.
For example, assume that we uploaded a JSON representation of the
signup-analytics dataset from Chapter 12 at https://example.com/datasets/signup-
analytics.json that looks like this:
[
  {
    "timestamp": "2023-08-21T13:24:49Z",
    "ip": "84.216.114.94",
    "state": "CO",
    "milestone": "visit_landing_page",
    "cost": { "amount": 0.92, "currency": "USD" }
  },
  ...
  {
    "timestamp": "2023-08-30T13:51:31Z",
    "ip": "115.93.200.62",
    "email": "crove@gmail.com",
    "state": "NY",
    "milestone": "sign_up",
    "cost": { "amount": 0, "currency": "USD" }
  }
]

Using this remotely located JSON file, we may publish a new version of the signup-
analytics data product that references the dataset through its URI. The properties of
the schema that have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics/v5.json",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "title": "A dataset snapshot of signup analytics by milestone on August 2023",
  "description": "Taken from Chapter 14 of our book",
  "documentation": "https://learning.oreilly.com/library/view/unifying-business-

Referencing Remote Data | 261



data/9781098144999/", 
  "authors": [ "Juan Cruz Viotti <juan@example.com>", "Ron Itelman <ron@exam-
ple.com>" ], 
  "license": "AGPL-3.0", 
  "readOnly": true,
  "datasetSchema": {
    "$ref": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/
v2.json"
  },
  "datasetRef": "https://example.com/datasets/signup-analytics.json"
}

The Problem of Streaming JSON
While defining small datasets using JSON arrays is convenient, doing so with large
datasets may prove impractical from a performance point of view.
As a consequence of the grammar, it is hard to incrementally parse JSON arrays in
a streaming fashion. Instead, applications typically need to load and parse the entire
JSON array before being able to access individual items. For example, if you have
a dataset whose JSON representation takes 1 terabyte of storage, you might need to
load the entire 1 terabyte into memory in order to read any item from it.

Introducing JSON Lines
While some JSON parsers attempt to implement some level of clever streaming
support, there is an easier way: JSON Lines. JSON Lines (JSONL) is a basic collection
format whose underlying idea is simple—each line of a JSONL document is a com‐
plete minified JSON document that can be parsed on its own.
For example, we may publish a JSONL variant of the https://example.com/datasets/
signup-analytics.json dataset we published in this section at https://example.com/data‐
sets/signup-analytics.jsonl, which may look like this:
{"timestamp":"2023-08-21T13:24:49Z","ip":"84.216.114.94","state":"CO","mile-
stone":"visit_landing_page","cost":{"amount":0.92,"currency":"USD"}},
...
{"time-
stamp":"2023-08-30T13:51:31Z","ip":"115.93.200.62","email":"crove@gmail.com","state"
:"NY","milestone":"sign_up","cost":{"amount":0,"currency":"USD"}}

In comparison to parsing an entire JSON array, a JSONL implementation can parse
one entry at a time or even skip certain entries.

Due to its streaming characteristics, JSONL is popular in AI. For
example, the popular ChatGPT model is fine-tuned using JSONL
input.

262 | Chapter 14: Introducing JSON Unify



We can let the datasetRef keyword remotely reference a JSONL document to obtain
efficient streaming capabilities. With this new capability, let’s publish a new version of
the signup-analytics data product that points to a JSONL dataset. The properties of
the schema that have been changed are in bold:
{
  "$id": "https://schemas.unify101.com/intelligence-ai/signup-analytics/v6.json",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "title": "A dataset snapshot of signup analytics by milestone on August 2023",
  "description": "Taken from Chapter 14 of our book",
  "documentation": "https://learning.oreilly.com/library/view/unifying-business-
data/9781098144999/", 
  "authors": [ "Juan Cruz Viotti <juan@example.com>", "Ron Itelman <ron@exam-
ple.com>" ], 
  "license": "AGPL-3.0", 
  "readOnly": true,
  "datasetSchema": {
    "$ref": "https://schemas.unify101.com/intelligence-ai/signup-analytics-entry/
v2.json"
  },
  "datasetRef": "https://example.com/datasets/signup-analytics.jsonl"
}

Note that when performing JSON Schema bundling, the JSONL input would be
converted into a JSON array, losing its streaming capabilities in exchange for self-
containment.

Extracting Meaning
The signup-analytics data product we have been using so far produces a set of
JSON Schema annotations that correspond to the meaning facet introduced in Chap‐
ter 4. Chapter 8 talked about how the annotation extraction operation builds upon
JSON Schema standard output formats. For ergonomic reasons, we can postprocess
the standard output format to tune it to be better suited for reading entries from a
data product. For example, each entry may produce a JSON object that looks like this:
{ "data": { ... }, "annotations": { ... } }

The data property contains the entry JSON value and the annotations property
contains every annotation extracted along the way, which mirrors the structure of the
corresponding data property.

A Simple Example
For example, consider the following data product of either integers  or Booleans ,
where each case declares a different title:

Extracting Meaning | 263



{
  "$id": "https://example.com/integers-or-booleans",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "datasetSchema": { 
    "anyOf": [
      { "title": "This entry is an integer", "type": "integer" }, 
      { "title": "This entry is a boolean", "type": "boolean" } 
    ]
  },
  "dataset": [ 1, false, true, 4, 5 ]
}

Reading the first element of the dataset, the integer 1, and evaluating it against
the datasetSchema property would result in the following output. Note that the
corresponding title keyword is collected as an annotation:
{ 
  "data": 1,
  "annotations": { "title": "This entry is an integer" }
}

Reading the second element of the dataset, the false Boolean, would result in the
following output:
{ 
  "data": false,
  "annotations": { "title": "This entry is a boolean" }
}

Using Logic Operators
Due to the expressiveness of JSON Schema, annotations can be as complex as the
schema writer desires. One interesting and powerful possibility, often not available
when using alternative technology, is to conditionally annotate dataset entries using
logic operators based on their content.
For example, consider a fictitious data product that describes a set of integers 
where we state that even numbers  are deprecated :
{
  "$id": "https://example.com/conditional-annotations",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "datasetSchema": { 
    "title": "This entry is an integer",
    "type": "integer", 
    "if": { "multipleOf": 2 }, 
    "then": { "deprecated": true } 
  },
  "dataset": [ 1, 2, 3, 4, 5 ]
}

264 | Chapter 14: Introducing JSON Unify



Reading an odd entry of the dataset, such as the integer 1, would result in the
following output:
{ 
  "data": 1,
  "annotations": { "title": "This entry is an integer" }
}

However, if the entry is an even number, such as the integer 2, then the deprecated
annotation will be collected:
{ 
  "data": 2,
  "annotations": {
    "title": "This entry is an integer",
    "deprecated": true
  }
}

The Signup Analytics Example
The amount of extracted meaning depends on the complexity of the data structures
you are describing and the amount of annotation keywords you used when describ‐
ing them.
Now consider the signup-analytics data product we have been using so far. When
encountering the following entry:
{
  "timestamp": "2023-08-21T13:24:49Z",
  "ip": "84.216.114.94",
  "state": "CO",
  "milestone": "visit_landing_page",
  "cost": { "amount": 0.92, "currency": "USD" }
}

The result would contain plenty of rich metadata from every schema referenced by
the data product:
{
  "data": { 
    "timestamp": "2023-08-21T13:24:49Z",
    "ip": "84.216.114.94",
    "state": "CO",
    "milestone": "visit_landing_page",
    "cost": { "amount": 0.92, "currency": "USD" }
  },
  "annotations": { 
    "timestamp": {
      "title": [ "The time of the entry event", "ISO 8601 timestamp in Basic For-
mat" ],
      "description": "The go-to date and time format at Unify101",

Extracting Meaning | 265



      "format": "date-time"
    },
    "ip": {
      "title": [ "The IP address of the user", "IPv4 address" ],
      "description": "As defined by IETF RFC 791",
      "format": "ipv4"
    },
    "state": {
      "title": [ "The US state where the user is connecting from", 
        "United States 2-letter state code" ],
      "description": "As defined by ANSI INCITS 38-2009"
    },
    "milestone": {
      "title": [ "The sign-up milestone the user completed", 
        "Website milestone in the Intelligence.AI sign-up form completion jour-
ney" ],
      "description": "Taken from Chapter 11 and 12 of our book"
    },
    "cost": {
      "title": [ "The cost of acquiring the user for completing this milestone", 
        "Monetary price quantity" ],
      "description": "The go-to price type at Unify101",
      "amount": {
        "title": "The price amount"
      },
      "currency": {
        "title": [ "The price currency", "Currency 3-letter code" ],
        "description": "As defined by ISO 4217"
      }
    }
  }
}

Dataset Lineage
A dataset is often used to support more than one use case, and each of these use cases
might consume different subsets of it in different ways. For example, the website for
the signup-analytics dataset we have been playing with may be used to aggregate
and render a success spectrum visualization like the one in Chapter 11 (using the ip
and milestone properties), but it may also be used to power an analytics report that
groups unique visitors by their geographical location (consuming the ip property to
perform IP reverse lookups).

266 | Chapter 14: Introducing JSON Unify



For practical reasons, the systems that implement these use cases often preprocess or
fork the corresponding datasets to better suit their needs. For example, extracting and
deduplicating IP addresses from a large enough website analytics dataset may be too
slow to do every time the report is generated. Therefore, a data scientist may perform
the task once, create a new dataset of unique IP addresses, store it somewhere else,
and only consume this preprocessed dataset from then onward.
Although the strategy of preprocessing or forking datasets provides great immediate
wins, it does not come without pitfalls. In our experience, more often than not, the
lineage of a dataset is not preserved. For example, a modified dataset might have
been created from another dataset at a certain point of time, but the new dataset
might not record what the original dataset was or when the new dataset was created.
Consequently, it may become difficult to track down and verify the data’s source of
truth.
To solve the dataset manipulation problem without compromising on lineage, what
if a data product could point to another, extending it as needed? In the “Referencing
Remote Data” section, we added a datasetRef keyword to point to a remote JSON
array or JSONL document. We can extend this keyword to let it point to a full-blown
schema that also defines the dataset or datasetRef keywords.

Filtering
One common reason for modifying an existing dataset is for filtering purposes. For
example, consider a simple dataset consisting of integers:
{
  "$id": "https://example.com/integers",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "datasetSchema": { "type": "integer" },
  "dataset": [ 1, 2, 3, 4, 5 ]
}

We can support a datasetFilter keyword that provides a schema for filtering input
data. With it, we can create a new dataset that points at the integer datasets , only
keeping the entries that consist of even integers :
{
  "$id": "https://example.com/even-integers",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "datasetFilter": { "multipleOf": "2" }, 
  "datasetRef": "https://example.com/integers" 
}

Dataset Lineage | 267



This way, we get the dataset we want (in this case, even integers), while keeping
a reference to the original dataset. Furthermore, changes to the upstream dataset
will be automatically reflected when reevaluating the dataset that applies the filtering
operation.

Transforming
A natural extension of the dataset vocabulary is to support transformation of entries.
An excellent language for expressing these transformation rules is JSON-e, a template
engine for JSON where the templates are written in JSON itself. It was originally
developed to support Mozilla’s CI/CD processes, but it’s now a standalone open
source project.

Another popular and powerful JSON query and transformation
language you might find useful is JSONata. We prefer JSON-e since
JSONata expressions do not use JSON constructs.

Covering JSON-e to its full extent is outside the scope of this book, but to give you a
taste of how it looks, consider the following template that multiplies every element of
an array of integers by two:
{
  "$map": [ 1, 2, 3, 4, 5 ], 
  "each(x)": { "$eval": "x * 2" } 
}

In this example, we declare a JSON array using the $map operator . By doing so, we
can set the each(x) property  to a subtemplate that will be evaluated against each
item of the array.
Evaluating the previous JSON-e template results in the following array:
[ 2, 4, 6, 8, 10 ]

Because JSON-e is JSON, we can elegantly embed arbitrary JSON-e transformation
templates in our dataset schemas using a keyword we may call datasetTransform.
For example, consider the following dataset of IP and email addresses:
{
  "$id": "https://example.com/ip-and-email",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "datasetSchema": { 
    "type": "object",
    "properties": {
      "ip": { "$ref": "https://schemas.unify101.com/common/ipv4/v2.json" },
      "email": { "$ref": "https://schemas.unify101.com/common/email/v2.json" }

268 | Chapter 14: Introducing JSON Unify



    }
  },
  "dataset": [ 
    { "ip": "167.16.122.17", "email": "example1@gmail.com" },
    { "ip": "203.11.113.42", "email": "example2@hotmail.com" },
    { "ip": "172.16.254.11", "email": "example3@protonmail.com" }
  ]
}

We can define a new dataset that only extracts the IP addresses , ignoring the email
properties, as follows:
{
  "$id": "https://example.com/only-ip",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "datasetSchema": { 
    "type": "array",
    "items": { "$ref": "https://schemas.unify101.com/common/ipv4/v2.json" }
  },
  "datasetTransform": { "$eval": "dataset.ip" }, 
  "datasetRef": "https://example.com/ip-and-email"
}

Evaluating the previous dataset would result in the following JSON array:
[ "167.16.122.17", "203.11.113.42", "172.16.254.11" ]

As an example, we can also combine datasetFilter and datasetTransform to
extract the IP addresses of Gmail users only:
{
  "$id": "https://example.com/gmail-ips",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "datasetSchema": { 
    "type": "array",
    "items": { "$ref": "https://schemas.unify101.com/common/ipv4/v2.json" }
  },
  "datasetFilter": {
    "properties": { "email": { "pattern": "@gmail\\.com$" } }
  },
  "datasetTransform": { "$eval": "dataset.ip" },
  "datasetRef": "https://example.com/ip-and-email"
}

Aggregation
An aggregation operation consists of combining dataset entries into a smaller set.
For example, you may want to combine a dataset of integers into their total sum. In
big data, one programming model for aggregating datasets—popularized by Apache
Hadoop—is MapReduce. Figure 14-3 shows a representation of this programming
model.

Dataset Lineage | 269



Figure 14-3. MapReduce transforms the entries of a dataset into their desired form (the
map operation), and then combines the results (the reduce operation).

The map operation is already covered by our datasetTransform keyword, so let’s
introduce a datasetReduce keyword that takes a JSON-e expression to combine two
entries of the dataset. Mapping a dataset of integers to multiply them by two and
adding the results may look like this:
{
  "$id": "https://example.com/multiple-and-add",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "datasetSchema": { "type": "integer" },
  "datasetTransform": { "$eval": "dataset * 2" },
  "datasetReduce": { "$eval": "dataset + accumulator" },
  "dataset": [ 1, 2, 3, 4, 5 ]
}

The result of evaluating this dataset is the integer 30.
Often, we want to reduce entries based on certain grouping characteristics, such as
a user identifier. We can simplify this use case by introducing a new datasetGroup
keyword that declares the grouping characteristic as a JSON-e expression.
Consider the following dataset that groups and reduces a dataset whose entries
consist of a user string property and an arbitrary data integer property:
{
  "$id": "https://example.com/group-by-user",
  "$schema": "https://schemas.unify101.com/dialect/data-product/v2.json", 
  "datasetSchema": { 
    "type": "object",
    "properties": {
      "user": { "type": "string" },
      "data": { "type": [ "array", "integer" ] }
    }
  },
  "datasetGroup": { "$eval": "dataset.user" }, 
  "datasetReduce": { 
    "user": { "$eval": "dataset.user" },

270 | Chapter 14: Introducing JSON Unify



    "data": { 
      "$flatten": [ 
        { "$eval": "accumulator.data" },
        { "$eval": "dataset.data" }
      ] 
    }
  },
  "dataset": [ 
    { "user": "foo", "data": 1 },
    { "user": "foo", "data": 2 },
    { "user": "bar", "data": 3 }
  ]
}

In this example, we group by the content of the user property . For each group, we
combine its entries by concatenating the values in their data properties  as a JSON
array. The result of evaluating this dataset is:
[
  { "user": "foo", "data": [ 1, 2 ] },
  { "user": "bar", "data": 3 }
]

If you want to learn more about the MapReduce data programming
model, we suggest consulting MapReduce Design Patterns by Don‐
ald Miner and Adam Shook (O’Reilly, 2012).

Summary
To cover the data facet of data products, this chapter introduced the ideas behind
JSON Unify, an extension to JSON Schema. With an implementation that brings
this vocabulary to life, a schema writer may directly associate a dataset with a data
product schema, consume datasets while extracting metadata from their entries, and
extend existing data products while keeping data lineage intact.
We hope that this chapter gave you a glimpse of the power of JSON Schema and
inspires you to extend it in creative ways to solve the challenges you will encounter in
your data transformation journey.
In Chapter 15, you will learn about designing unified intelligence and how to
combine the power of JSON Schema with a conceptual design for core unifying
principles.

Summary | 271



CHAPTER 15
Principles of Designing Intelligence

The most beautiful thing we can experience is the mysterious. It is the source of all true art
and science.

—Albert Einstein

From the moment our ancestors first gazed upon the cosmic constellations shimmer‐
ing in the night sky to now—the age of AI, where data scientists look at constellations
of connected data points in vast graphs of information—humanity’s story is one of
insatiable curiosity. The value of venturing past boundaries, from the known into the
unknown and uncharted, lies in forging new pathways of understanding and new
paradigms of thinking that enable us to better shape the reality we wish to create.
Congratulations, you have made it to the end of the unifying methodology for data
strategy and data management. You might be asking, “What’s next?” The answer is
that now that you, the data champion, have unified your organization or aspects of
your organization’s data, you are ready to learn the universal principles that serve
as the fundamental building blocks for designing systems—be they computational,
organizational, or otherwise—that can be combined in ways that exhibit properties
ascribed to intelligence itself.

Your Unifying Journey So Far
As you learned in this book, unifying views your organization from a zoomed-out
perspective as a holistic entity made of connected networks, then zooms in on the
most granular and fundamental unit, the concept.
Chapter 4 discussed concept-first design, which aims to remove ambiguity and knowl‐
edge gaps around concepts by asking nontechnical people to describe the meaning
and logic of the language they use in a spreadsheet so that the concept can be
easily translated into a computational format. It is the concept that traverses between

273



human and machine language, and the goal of concept-first design is to align the
meaning, context, and structure of the concept across those two domains.
The process of going back and forth between human and machine definitions,
seeking greater clarity, accuracy, and alignment, was introduced in Chapter 6 as
harmonization. Once concepts are sufficiently aligned, the next step is synchroniza‐
tion (introduced in Chapter 7) the act of connecting and scaling concepts across
processes, rules, and decision making while continuing to maintain alignment.
Data products, introduced in Chapter 4, create a structure for organizing data to
implement this paradigm using four facets: context, meaning, structure, and the data
itself.

A Constellation of Deeper Principles Guides Unifying
What are the philosophical principles that have been guiding this methodology? In
this chapter, we will unpack the ways of thinking, or principles, by which data scientists
and AI researchers go about breaking down complex problems in order to create and
apply machine learning solutions that exhibit properties of what is commonly called
intelligence. These principles are meant to be laid out as an assortment of colors in
your palette to choose from as you design more intelligent systems. Because of this,
we call them principles of designing intelligence.
Throughout human history, constellations have served as invaluable navigational
tools for ancient mariners, seasonal guides for farmers, and even as cultural narra‐
tives that connected communities. Just as ancient people didn’t need to be astrono‐
mers to benefit from constellations, you don’t need to be a data scientist to implement
these principles or understand the beauty of their illuminating light. They’re universal
touchpoints that help organizations align and prosper.
If you’re wondering how to innovate and enhance your system’s intelligence, these
principles can act as guides to help you break down complex problems into more
manageable parts, thereby fostering team alignment and facilitating compelling
storytelling.
In this chapter, you will learn the following principles of designing intelligence:
Alignment

Advocates for harmonizing organizational goals and data to enhance decision-
making and mitigate risks.

Information
Stresses the importance of structuring and quantifying data to understand the
relationship of uncertainty to information.

274 | Chapter 15: Principles of Designing Intelligence



Learning
Identifies error correction as an essential aspect of intelligence, applicable to both
human and machine learning contexts.

Integrated simplicity
Recommends breaking down complex tasks into simpler elements for more
efficient problem solving.

Continuums
Emphasizes the value of converting complex variables into easy-to-understand
metrics through quantification and calibration.

State transitions
Clarifies complex systems by categorizing them into distinct states and defining
transitions, aiding problem solving and predictive analytics.

Decidability
Discusses criteria for how to make effective decisions by balancing critical
inquiry with actionable steps.

Heuristics
Evaluates the role and ethical implications of using mental shortcuts in decision
making.

Mastery
Encourages a balanced approach to learning, underlining the importance of
self-awareness and wisdom in the quest for expertise.

Wisdom
Promotes system designs that facilitate continuous learning, thereby enhancing
an organization’s collective intelligence over time.

1. The Principle of Alignment
“Hidden Threats to Organizations: A Modern Parallel” on page xiii touched on the
crippling effects of organizational misalignment, whose root causes are ambiguity,
knowledge gaps, and blind spots. Imagine data as the circulatory system of an organi‐
zation; misaligned data can disseminate throughout the structure, starting as minor
inaccuracies but escalating into major systemic failures.

Transforming the Abstract to Concrete
The alignment principle underscores the necessity of converting abstract ideas into
concrete formats using concepts, our fundamental building blocks of communicating
information. This transformation across mediums and contexts must be accessible
and comprehensible to both humans and machines.

1. The Principle of Alignment | 275



Take the concept of red, for example. In data, you can represent this color in various
ways. Using RGB (Red, Green, Blue) syntax, the color can be numerically displayed as
255,0,0 or simply termed as red. These varying ways to describe concepts are known
as data representations, but the concept itself, red, has a singular, clear meaning.
In organizations, humans and machines are continuously transforming concepts. The
term for this is transduction, which is also a biological phenomenon happening in
you right now as you read this. As this text enters your eyes (in the form of light),
specialized cells interact with those photons, and proteins convert (transduce) the
products of those interactions into electrical signals, the unified language our brains
can interpret and act upon.

What You See Can Kill You, and the Same Is True in Data
At the pivotal moment of transduction, the quality of information can either be
preserved or compromised. When abstract ideas feel like they make sense in our
minds, it is often because of the assumptions we use to compensate for ambiguity and
knowledge gaps. Alignment is the harmonization of concepts and synchronization of
processes using concepts to have accurate and error-free transduction of information
in representations.
Using a medical analogy, think of transduction as the point where “infection” can
occur. Just as a poorly sterilized medical instrument can introduce bacteria into
a sterile environment, poor transduction can introduce errors into an otherwise
coherent system, as shown in Figure 15-1.
Alignment in organizations can be likened to a well coordinated symphony. Tactics
like harmonization and synchronization are key and require a common reference,
like a metronome or musical scale. We identified tools for aligning concepts are
described in Chapters 6 and 7. We recommend using JSON Schema to minimize data
misalignment. But even without technical expertise, you can apply these principles
using simple spreadsheets and whiteboards, as we discussed starting in Chapter 4.
Once your data is aligned, you can further optimize your processes. Shifting from a
diffuse approach to a laser-focused one can significantly improve your effectiveness,
as we discussed in Chapters 9 and 10.

276 | Chapter 15: Principles of Designing Intelligence



Figure 15-1. This visualization embodies the complexities and challenges of problem
solving. At the outset, the declaration “I understand the problem!” suggests confidence.
However, ensuing steps—represented by a square, a circle, and a triangle, each leading
to their specific solutions—converge on a shared “Errors” junction. This bottleneck
represents confusion and misunderstandings about why the proposed solutions aren’t
working, symbolized by a star, a hexagon, and a cloud. The narrative is cyclic; until the
stakeholders are aligned, no matter how much effort the three stakeholders put in, they
will never reach an efficient and effective solution.

1. The Principle of Alignment | 277



2. The Principle of Information
Claude Shannon is often referred to as the “father of information theory.” Shan‐
non’s groundbreaking work has transformed our understanding of communication,
information, and probability. His seminal 1948 paper, A Mathematical Theory of
Communication, revolutionized the way we think about information by providing a
quantitative model for communication.
In this paper, Shannon proposed that information could be quantified using mathe‐
matical formulas and introduced key concepts like the bit, or binary digit (true or
false), as a basic unit of information. He explained how to measure the information
content of a message and how to encode it most efficiently.
Let’s explore this idea with a simple example: a guessing game where you’re asked
to guess a number between 1 and 100. Someone else knows the number, and your
goal is to identify it by asking as few questions as possible. One efficient way to
find the correct number is to employ true/false questions, or binary search, such that
you divide the remaining set of possible numbers in half each time. For example,
your first question might be, “Is the number greater than 50?” No matter the answer,
you’ve already eliminated half of the possibilities.

Understanding Uncertainty
Before you start asking questions in the guessing game, there’s a high degree of
entropy, or uncertainty, because the correct number could be any one of 100 possi‐
bilities. As you ask questions and receive answers, you reduce this entropy until it
becomes zero—indicating you’ve discovered the correct number.
Let’s examine this in more detail. If you had to guess a number between 1 and 100
and phrased your yes/no questions so as to split the search range in half, as shown
in Figure 15-2, you are mathematically guaranteed to remove all uncertainty in 7
questions (or less). Let’s examine how this works.
One of Shannon’s major achievements was to prove that as long as you can reduce
information to binary structure, you can quantify exactly how much information is in
any message (the total amount of entropy). What makes his contribution remarkable
is that it is universally applicable to any communication, regardless of the informa‐
tion’s meaning to a sender or receiver or the medium in which it was sent.

278 | Chapter 15: Principles of Designing Intelligence



Figure 15-2. Any number selected between 1 and 100 can be deduced with yes/no
or true/false questions in a deterministic manner by dividing the search space in half
with each question. The total number of required questions to guarantee removing any
uncertainty is the amount of information—or, as Shannon called it, entropy. “Binary
search” is the technical name for this deterministic process, and the term coined by
Shannon for this process is “reduction of entropy.”

Here’s a quick explanation of the mathematics. The term log base 2 essentially asks,
“To what power must we raise 2 to get a certain number, n?” This is particularly
relevant when our information is in a binary format. In binary, 1 bit of information
has two possible states: true or false. Thus, the log base 2 of 100 is approximately 6.64,
which we round up to 7. In the context of our number guessing game, this means
that we are guaranteed to deduce any number between 1 and 100 with seven true/false
questions if we keep cutting the list in half with each question. From Shannon’s per‐
spective on information theory, the entropy (or the measure of uncertainty) resides in
the entire set of possible values. For our example, this would be the numbers 1–100.
Now let’s explore how that relates to the unifying core principle that ambiguity,
knowledge gaps, and blind spots are the root problem that create misalignment in
organizations:

2. The Principle of Information | 279



• All ambiguity has been removed, since the goal, the question, the answer, and
numbers don’t have dual meanings.

• We have a known unknown—the number we need to find.
• All blind spots have been removed, we have no ambiguity, and we know we have

a knowledge gap—the known unknown, and we even know how much effort we
need to complete our goal (seven steps).

Reducing questions to binary (true/false) possible values forces you
to remove a lot of ambiguity, knowledge gaps, and blind spots
when dealing with concepts and designing processes. It is precisely
for this reason that success spectrums—explained in Chapter 10—
structure goals in ordered steps, with binary conditions at each
state that must be true in order to progress to the next state.

3. The Principle of Learning
Failure is the key to success; each mistake teaches us something.

—Morihei Ueshiba, The Art of Peace (Shambhala, 1992)

Both in the realms of artificial and human intelligence, the capacity for swift error
correction is often seen as a hallmark of higher intelligence. This is not just about
avoiding mistakes; it’s about a system’s—or person’s—ability to learn and adapt from
them. Whether it’s a data algorithm tweaking its parameters after a false prediction
or an individual openly acknowledging a mistake to make an informed decision
next time, the speed and effectiveness of this learning loop are key indicators of
intelligence.
In the context of organizations and large-scale systems—what we might call the
macro world—this principle takes on added significance. Capturing data on errors
is not just a tactic in data science; when applied from a strategic and managerial
perspective, it enables organizations to analyze errors, minimize uncertainty, and
optimize decision making.

Defining Learning
But what is learning, exactly, and how might we describe it in a universal way? It
turns out that learning, like many other properties ascribed to intelligence, can be
computationally described in universal terms, as shown in Figure 15-3.

280 | Chapter 15: Principles of Designing Intelligence



Figure 15-3. Even the most abstract questions and ideas—such as “What is learning?”—
in mathematical formulas can be simplified in visuals, as this illustration shows. Cap‐
turing data about errors over time is the simplest and most effective way to measure
learning. It is applicable to humans, machines, teams, and even entire organizations.
The trick is reaching alignment on another question: “What is an error?”

Learning is defined by the ability of a student or machine’s algorithm to reduce errors
over time. When does learning start? Imagine a child who doesn’t know the rules
of chess. At first, they make moves that don’t make sense. Over time, they learn the
rules and make fewer errors. And the faster they (or any learner such as an AI)
can reduce errors over time, the more efficient the learning rate. When educational
material, or learning experiences, are given to a learner, we can measure the efficacy
of that content in maximizing the learning rate. For machine learning, this might be
done through better data, models, or algorithms; for humans, it might be through
different exercises and communication methods. For those interested in collective
intelligence, this might be creating user experiences which create feedback loops from
user interactions that can be used to create better training data.

Although various forms of intelligence have been identified in
psychology—such as spatial and musical intelligence—this book
focuses on a generalized and simple principle of measuring the
learning efficiency that is applicable to both human and machine
learning, with a focus on measuring learning across collaborative
networks in organizations and examining ways that such learning
may be computationally modeled.
There are many, many more dimensions to measuring intelligence,
learning, and so on, some of which are explored further in this
chapter.

3. The Principle of Learning | 281



Defining Errors
In order to automate or implement learning, we must have a way of defining and
measuring errors. Following this principles-first approach continues our core unify‐
ing practice of seeking to identify and minimize misalignment; we are reducing
ambiguity, knowledge gaps, and blind spots about how people understand errors and
how they are measured. The act itself of having these conversations greatly reduces
opportunities for misalignment. Remember:

Error = Prediction – Suggestion

An error occurs when the outcome diverges from our prediction.

Another term for prediction might be expectation, and another
term for error might be surprise. When thinking about how to align
on errors, thinking in this context can spur alignment opportuni‐
ties on what each team member’s expectations are and how they
set them. Will errors be captured somewhere? Would a spreadsheet
work for now? It is far better to have these conversations quickly
and easily without needing to introduce technical complexity.

4. The Principle of Integrated Simplicity
I didn’t have time to write a short letter, so I wrote a long one instead.

—Blaise Pascal, mathematician in the 1600s; quote often attributed to Mark Twain,
American author

The well-known adage “to kill two birds with one stone” is a metaphor for achieving
multiple goals with a single, thoughtful action. Imagine not only felling two birds but
also knocking down a bee’s nest to harvest honey; this epitomizes elegant efficiency,
the art of accomplishing more with less in a graceful manner.
Simplicity is an art form crafted from four key elements: complexity reduction, decom‐
posability, compressibility, and memoization. To bring these abstract concepts to life,
the next section uses the example of a chef preparing a dish from a recipe. What
strategies might the chef employ?

Complexity Reduction
The recipe lists myriad ingredients and intricate steps, yet the chef masterfully distills
this complexity into fewer, essential actions without compromising flavor. This mir‐
rors the philosophical tenet of Occam’s razor and its computational representation
Kolmogorov complexity, both of which advocate for using the simplest solution when
multiple options exist.

282 | Chapter 15: Principles of Designing Intelligence



Decomposition
Perhaps the chef lacks certain specialized ingredients. No problem; they skillfully
deconstruct what’s available into fundamental components, storing any extras for
future culinary adventures. For instance, rendered duck fat from one dish is a versa‐
tile base for other recipes.

Compression
The chef ’s focus is on reusability and efficiency. Picture them slicing an onion:
first halving it, then stacking the layers before making incisions. One stroke yields
multiple cubes, maximizing output with minimal effort.

Memoization
Memoization is a programming technique used to optimize computational efficiency
by storing the results of expensive function calls and returning the cached result
when inputs reoccur. In nontechnical terms, when a problem is solved, the problem
and solution are stored so that they can be reused rather than solving the problem
again.
In essence, memoization serves as a memory mechanism that remembers past calcu‐
lations to avoid redundant work. This would be like our chef putting up posters
of how to best solve problems, so that everyone has access to the same quick and
accurate solutions.

Integrating in Communication Networks
The crux of integrated simplicity lies in discerning which strategies of complexity
reduction, decomposability, compression, and memoization to employ for maximum
impact, recognizing the nodes of interconnection among these elements and weaving
them together to unlock unprecedented value.
In the context of communication networks within an organization, memoization
becomes a collective intelligence strategy. Just as computer algorithms store prior
results for quick retrieval, an organizational network can maintain a “memory bank”
of solved problems and effective solutions.
By identifying interconnected nodes, you can break down larger, more intricate
problems into smaller, more digestible tasks. Through the strategic application of
complexity reduction, decomposability, and compression, you can craft solutions
that are not only streamlined but also extraordinarily effective at solving a range of
interconnected challenges.

4. The Principle of Integrated Simplicity | 283



5. The Principle of Continuums
Measure what is measurable, and make measurable what is not so.

—Galileo Galilei, 17th century, known as the father of observational astronomy

In business, quantification is more than just a means of measurement; it is the
cornerstone for decision making. When we quantify something, we distill its com‐
plexity into a single, easily understandable dimension. Whether it’s the durability of a
product or a customer satisfaction score, these one-dimensional measures provide a
common language for evaluation.
Let’s explore the utility of continuums as a framework for evaluating complex vari‐
ables. A continuum quantifies and calibrates these variables on a universally under‐
standable scale which creates a shared language for evaluation. Think of it as setting
the boundaries from the worst conceivable outcome to the pinnacle of excellence,
much like zero degrees Kelvin is the theoretical lowest possible temperature.
A continuum provides a baseline that allows for a more nuanced understanding of
where a particular measurement falls: whether it’s closer to good or bad, efficient or
inefficient. When a measurement is plotted on a continuum, you can observe trends
or patterns more transparently, leading to more effective decision making.

Making Things Measurable
The true brilliance of breaking down complex variables into measurable dimensions
comes from computational analysis. Once variables like customer satisfaction, prod‐
uct quality, and employee performance are translated into numerical data points,
they become compatible with rigorous computational methods, including machine
learning algorithms. Not only can you assess the current state of affairs, but you can
also predict future outcomes, providing an invaluable asset for strategic planning.

The Dangers of Misusing Measurements
What gets measured gets managed.

—Peter Drucker, an important figure in management consulting

Measurement is a double-edged sword in business. Managers sometimes measure
things that aren’t important because the act of measuring becomes evidence of their
managing! In other words, if they know how their performance is rated, they may
skew goals, decisions, or priorities to keep their performance metrics up. Be careful
and open-eyed about why something is measured and how measurements will be
used.

284 | Chapter 15: Principles of Designing Intelligence



The key to effective measurement lies in selecting the right metrics to measure and
asking the question, “What are the KPIs of your KPIs?” Creating and managing KPIs
is resource intensive, and their value diminishes if they don’t serve a continuous
purpose, which underscores the need to approach measurability as a heuristic: a
guideline that aids in converting abstract ideas into quantifiable metrics.

For a deeper dive into measuring what truly matters, review the
problem with problems, a concept explained back in Chapter 1,
and how success is relative to defining the measure in Chapter 10,
which covers success spectrums.

A Continuum Example for a Control Strategy Problem
Let’s use an example where a data champion has been asked to examine a data man‐
agement problem. Some teams are creating little to no documentation for the datasets
they create; others are creating their own semantic definitions for data columns;
others are trying to use datasets from other teams, unaware that the same terms have
different meaning. These differing methods result in teams across departments being
unable to share insights or learn from each other’s efforts—or worse, creating reports
that whose insights appear to be data driven but are not.
This situation might be described as a systems control strategy, which is a fancy way
of architecting processes to minimize errors and risks. How might a data champion
begin to understand, let alone tackle, this problem?
One strategy might be to create a continuum in order to understand the situation
relativistically. On one end of the continuum, the data champion places the current
situation, complete decentralized control—where there are no centralized controls and
everyone makes their own decisions or controls for data management.
On the other end of the spectrum, the data champion places full centralized control,
where every data management decision is controlled by a hierarchical data manage‐
ment authority. This approach, shown in Figure 15-4, forces our data champion to
remove ambiguity, knowledge gaps, and blind spots in conceptualizing the problem
and creates a shared artifact that can be used in conversations with leaders.
The data champion begins to ponder the issues with full centralized control; taxono‐
mists become bottlenecks, leading to teams going behind their backs to create data,
and not everyone has the time or resources required to learn the complex data
management tools used in the enterprise-level solutions that full centralized control
requires.

5. The Principle of Continuums | 285



Figure 15-4. An example of a continuum approach to begin to convert abstract ideas
into something quantifiable. In this example, the continuum is of data control strategies.
On the left side is the extreme of full autonomy, where anyone is able to create any
data definition, and on the other lies full control by a centralized, top-down, hierarchical
taxonomy manager. This visualization might be useful for beginning to ask, “Where is
your organization on this spectrum?” and “How might we measure this?”

Think about how you would design the continuum. Does it represent the inefficiency
of each control strategy? Perhaps it relates to how many people are creating high-
quality data with proper data management practices. Or perhaps the continuum
represents the number of people who have a preference. There are many ways to
design it, and each continuum represents a singular dimension of the duality between
these two paradigms.
A continuum representing the different types of problems each has might be useful.
For example, the decentralized approach might be characterized by chaos, poor
quality, and no standardization, whereas the centralized approach might have bottle‐
necks, limited innovation, and people breaking the rules to define what they need.
Conceptualizing these continuums gives you a more measurable way to attack the
problem.

The principle of continuums first appeared in Chapter 10, when
you learned about success spectrums.

6. The Principle of State Transitions
Understanding systems—whether computational, organizational, or even systems
of individual behavior—requires a deep dive into state transitions. This principle
describes the use of a state machine to distill complex problems into recognizable
states and clearly defined transitions between those states.

286 | Chapter 15: Principles of Designing Intelligence



The sheer complexity of today’s challenges can be overwhelming. Adopting the lens
of a state transition system simplifies these complexities by breaking them down into
manageable states and transitions. The benefit is twofold; you get to identify where
you are (the current state) and what it takes to get to where you want to be (the target
state).

A Simple State Machine
Figure 15-5 illustrates how a thermometer’s sensor triggers a heater’s state transitions.

Figure 15-5. A simple state machine contains a finite number of nodes in different states
—in this case, the heater being ON or OFF—and the set of conditions that must be true
in order to move to the next state.

Take an organizational workflow as an example. Visualize it as a sequence of states—
inception, development, review, and completion. What actions or conditions must
be satisfied to move from one state to another? Is it managerial approval? Is it the
completion of a test? Defining these success conditions can eliminate ambiguity and
increase efficiency.

Simplifying State Transitions
In the world of data science, predictive modeling can offer insights into the proba‐
bility of moving between any two states. (While this is a simplified way to think
about designing ways to algorithmically think of state transitions, it is helpful as a
principle.) You need the current state and an action as inputs, and a way to output
new states:

State transition = f(Current state, Action) → New state

In this formula, f is the function that encapsulates the rules and conditions for
transitions between states.
A more complex version is one in which a system or a user can take an action.
This allows for adaptive and personalized experiences and allows for manual and
automated processes. Consider this model:

State Transition = f(Current state, User action, System action) → New state

6. The Principle of State Transitions | 287



In this model:
Current state

This is the state of a system as it currently exists.
User action

Encapsulates any actions or input provided by the user, such as the user manually
turns on the heater.

System action
Encapsulates actions taken by the system in response to the current state and user
action. This could be something as simple as: if the user turns on the heater, ask
the user if they want it to automatically shut off in a couple of hours.

By applying the principle of state transitions, you ensure every stakeholder under‐
stands the what, why, and how of change, making problem solving transparent and
effective.

7. The Principle of Decidability
Life is a series of choices, much like a chess game or a self-driving car navigating
through traffic. Making the right decision requires more than just gut feeling—
it requires a structured approach. Enter the principle of decidability, inspired by
the field of reinforcement learning, an approach to designing artificial intelligence
systems.

What Is Decidability?
Decidability isn’t about creating complicated mathematical models; it’s about using
three clear-cut questions to guide you:

• What problem-solving strategy best improves decision making?
• How can you determine if you’re ready to make an informed decision?
• How can you eliminate ambiguity and knowledge gaps?

It’s entirely OK for team members not to have answers and not want to use a formal
mathematical framework for decision making. However, this principle is incredibly
important to having conversations that serve our root principle: increase alignment
by removing ambiguity, knowledge gaps, and blind spots. Providing a framework for
determining people’s approaches to decision making can spur conversations about
people’s beliefs regarding the likelihood of outcomes and the impacts of decisions.

288 | Chapter 15: Principles of Designing Intelligence



Formal decidability is a mathematical concept that is mostly used
in academic settings to discuss the limitations of algorithms and
computing systems. In unifying, the principle of decidability is
geared toward decision-making processes in organizational con‐
texts. Here, decidability refers to a framework or model designed to
enhance the quality, reliability, and ease of organizational decision
making.

Two Key Approaches to Problem Solving
Understanding the nature of your decisions is crucial. Drawing from AI’s reinforce‐
ment learning, you should ask:

• Do I have the right information? Without the right data, even the smartest minds
can falter. In the quest for better information, we engage in what’s known as
exploratory problem solving.

• How can I best use information? Striking a balance between exploration—trying
new things—and exploitation—sticking with what works—is critical. Too much
of either leads to inefficiency.

Making Informed Decisions
When humans navigate the world, we often use maps or GPS. In AI, one such
navigation tool is the Markov decision process (MDP), which combines the state
transition principle with probabilistic thinking.
The following are important terms in MDPs:
States

Like GPS locations representing decision points
Actions

The routes you can take between states
Transition probabilities

The odds of going from one state to another
Rewards

Benefits or penalties associated with each action
Figure 15-6 demonstrates a way to visualize an MDP decision-making scenario given
the following : a person wakes up and has to decide between watching TV or exercis‐
ing, considering probabilities and rewards. While the example shows mathematical
values for rewards and probabilities of taking actions, it is entirely valid and useful to
make things simple for team members to have important conversations: Is outcome
(state) X likely or not? Why do you think that way? What are options (action paths) that

7. The Principle of Decidability  | 289



can be taken in any one state? What are the benefits or negative consequences of any
state?

Figure 15-6. A simple MDP visualization. The person is in a resting state (S1) with a
reward of 0 and has an 80% chance of watching TV (S2) with a reward of –1, or a 20%
chance of exercising (S3) with a reward of +2. Both states 2 and 3 have a 100% chance of
resting afterward. P stands for probability, R stands for reward, and S stands for state.

Real-World Decidability to Reduce Misalignment in Teams
Imagine you’re a CEO. Your perspective is unavoidably skewed, often high level
and clouded by politics. Decision making is further complicated if your team isn’t
equipped to interpret data.
Decidability reduces misalignment by asking your teams to explain the following in
plain language:

• Have we defined processes, states, and value (rewards) in straightforward terms?
• Do we know the probabilities of different outcomes? Can we make educated

guesses?
• Is there a unified decision-making framework, or are we just winging it?

A practical approach is to use an MDP as a common language for decision making.
MDPs help everyone visualize the problem and the possible outcomes of various
decisions. If team members struggle to articulate their version of an MDP, it usually
indicates gaps in knowledge or understanding. This lack of clarity can be a red flag,
suggesting that the team is not yet ready to make an informed decision.

8. The Principle of Heuristics
Heuristics gained prominence through the pioneering work of psychologists Amos
Tversky and Daniel Kahneman. These mental shortcuts enable swift decision making

290 | Chapter 15: Principles of Designing Intelligence



in complex scenarios. Although highly efficient, they carry the risk of cognitive biases
and decision-making errors.
Here are some examples of heuristics that are dangerous in business and leadership:
Availability heuristic

Frequent exposure to a product can create an inflated perception of its quality,
leading consumers to make suboptimal purchasing decisions.

Sunk cost fallacy
Commonly observed in business and research settings, this heuristic encourages
continued investment in failing projects due to the weight given to already-
incurred costs.

Anchoring heuristic
The reliance on initial information can disproportionately influence subsequent
judgments and decisions.

Confirmation bias
This cognitive bias prioritizes information that supports preexisting beliefs while
neglecting or dismissing evidence to the contrary.

Intergroup bias
This bias manifests when actions are evaluated differently depending on the
group to which the actor belongs, leading to unfair assessments or discrimina‐
tory behavior.

Heuristics and biases, though closely related, serve different roles
in cognitive psychology. A heuristic is a mental shortcut or rule of
thumb that facilitates quick decision making and problem solving.
It’s a mechanism our minds use to simplify complex situations and
make them manageable.
On the other hand, a bias is a systematic error or skewed perspec‐
tive that arises from the use of heuristics. While heuristics aim to
enhance efficiency, they can sometimes lead to biases—deviations
from rational or objective judgment. In essence, heuristics are the
cognitive tools, and biases are the potential pitfalls or errors that
can occur when using those tools.

Awareness and Ethical Considerations
Understanding the ethical implications of using heuristics is paramount. While some
industries like educational software aim to harness heuristics to facilitate learning,
other domains such as marketing, advertising, and social media often exploit these
cognitive shortcuts to manipulate public perception.

8. The Principle of Heuristics | 291



Take, for example, the like feature on social media platforms. This feature taps into
the social proof heuristic, which suggests that people are more likely to engage with
a post if it has already received a significant number of likes or approval from
others. While the feature’s initial intent may have been to encourage positive social
interaction, it has also been criticized for fostering addictive behaviors, reinforcing
confirmation bias, and creating echo chambers. In this case, understanding the eth‐
ical considerations is crucial, as the design choice has clear implications for user
behavior and well-being.
No one is immune to biases introduced by heuristics—not even experts in fields
like science and psychology. These mental shortcuts are deeply embedded in our
cognitive processes, serving as efficient tools for navigating complex environments.
However, this efficiency comes at the cost of potential biases and ethical pitfalls. Thus,
it’s imperative to be aware of how heuristics may shape our perspectives and influence
our actions, and to thoroughly examine and consider the ethical implications of
heuristics before using them in systems.

Connection to Decision Making in Business
Biases influenced by heuristics can distort both personal and professional judgments,
often leading to flawed or even detrimental decisions. Understanding the relationship
between heuristics and biases enables us to aim to make more objective and groun‐
ded choices.
In organizational settings, biases can exacerbate discord. A team collectively suc‐
cumbing to the same bias amplifies its effect, while varying biases among team
members can result in conflicts as each interprets data through their own skewed
lens. Inertia and heuristics are often to blame for glaringly inefficient processes or
decisions within an organization. They create a kind of decision-making autopilot
that hampers innovation and optimal outcomes.
The challenge lies in our innate tendency to rationalize decisions after the fact,
defending choices made without conscious deliberation. Adopting data-driven
approaches can mitigate these vulnerabilities, fostering more objective and unified
decision making.

9. The Principle of Mastery
In the realms of cognitive psychology and education, Bloom’s taxonomy, shown in
Figure 15-7, serves as a seminal framework that categorizes knowledge into hierarchi‐
cal levels, from the basic act of remembering information to the advanced ability to
create new knowledge.

292 | Chapter 15: Principles of Designing Intelligence



Figure 15-7. Bloom’s taxonomy serves as a hierarchical framework that outlines vari‐
ous levels of cognitive mastery essential for effective educational or learning design.
Beginning with the basic skill of “Remember,” the taxonomy scales upward through
increasingly complex cognitive abilities, culminating in the pinnacle of “Create.”

Levels of Mastery in Knowledge
The hierarchy of Bloom’s taxonomy provides a structure for both evaluating a
learner’s current level of mastery and designing targeted educational experiences to
advance them to higher levels.
Here are the definitions of each level:
Remember

This initial level focuses on the simple recall of information. While foundational,
it is the most rudimentary form of learning.

Understand
This level involves comprehending and interpreting the information learned,
making the knowledge more meaningful.

Apply
Here, learners are capable of utilizing the understood information in new situa‐
tions, showcasing a functional mastery of the material.

9. The Principle of Mastery | 293



Analyze
This level entails dissecting complex sets of information to understand their
underlying structure, signifying a higher level of cognitive engagement.

Evaluate
More advanced still, this stage requires critical thinking skills to assess the valid‐
ity of arguments and even to evaluate others’ level of understanding, thereby
demonstrating depth in cognitive ability.

Create
Considered the zenith of Bloom’s taxonomy, this stage goes beyond all previous
levels. Here, learners can synthesize new forms of knowledge, innovate, and
express novel ways of seeing and experiencing what they have learned.

When a learner can demonstrate knowledge of a topic across these levels without
error, they have achieved mastery.

Strategies for Mastery
The goal of this principle is to create a clear pathway to attain mastery by seeking to
eliminate any ambiguity, knowledge gaps, and blind spots about any knowledge or
skill.
The following are techniques for pursuing mastery using the example of music (the
creation and experiencing of which both require knowledge):
Slowing down

Reduce information over time to reduce complexity.
Intentional practice

It is not enough to practice; you have to practice playing the correct note at the
right time, or you are practicing incorrectly.

Immediate feedback
Immediate feedback allows for quick corrections and continuous improvement.

Granularity
Paradoxically, as you attain higher levels of mastery, levels of granularity get
deeper and smaller, until you reach your limits of conceptual building blocks.

Automaticity
Aim to play a solo without needing to look; practice relaxing completely so that it
is automatic and effortless.

Breaking down actions into singular steps
Breaking down the music into single notes and chords over time is similar to our
principles of continuums and state transitions.

294 | Chapter 15: Principles of Designing Intelligence



Universal guides
Learning to listen and synchronize one’s efforts to a trusted third-party reference,
such as a metronome, is essential for accuracy.

Foresight
The ability to foresee, plan, and even improvise multiple steps ahead in a given
process. This allows you to navigate complex situations with a heightened sense
of control and adaptability.

Modalities
The ability to translate your skills and knowledge across various forms, such as
visualizing musical notes as geometric shapes or adapting your musical ability to
different instruments and vocal techniques.

All of these strategies can be designed into your user experiences, architected in
data structures, and programmatically implemented in applications. For example,
decomposition from the principle of integrated simplicity is highly related to breaking
down actions into singular steps, and immediate feedback can be a design strategy.
Intentional practice relates to success spectrums (Chapter 10) and the concept compass
(Chapter 6).
In Chapter 16, which discusses four pillars of intelligence in organizations, you’ll
discover how to apply these principles of mastery to the design and architecture
of intelligent systems. These principles are scalable; they can enhance learning on
individual and organizational scales.

10. The Principle of Wisdom
The beginning of wisdom is the definition of terms.

—Socrates

The principle of wisdom brings together all the other principles for designing intelli‐
gence. Wisdom is more than an individual pursuit; it is the collective perspective
of the entire organization, treating the organization itself as an intelligent learning
entity made up of intelligent learning entities (humans and machines collaborating
together).
In unifying, the definition of wisdom is the ability to self-optimize the relationship
between knowledge and reasoning capabilities across all levels and connections in an
organization.
Wisdom is how an organization develops the self-awareness to recognize limitations
and knowledge gaps and map strategies for self-optimization. In other words, wis‐
dom seeks ways to develop mastery for the collective organization and all of its
components. Wisdom propels us to seek further learning, continually updating our

10. The Principle of Wisdom | 295



understanding and adapting to new challenges. It is not just about solving problems
efficiently; it is about understanding what problems ought to be solved and why.
In the face of organizational inefficiencies—often driven by politics, budgetary con‐
straints, or siloed perspectives—this principle advocates for intelligence-first decisions.
Given the enormity of decisions and the volume information flow in organizations,
what’s needed is often wisdom at scale.
Integral to this principle are continuous feedback mechanisms that add a layer of
meta-learning, allowing systems to adapt and evolve. Feedback loops collate data
across processes, fostering dynamic adaptability that’s essential for navigating com‐
plex and uncertain landscapes.

Like the principle of alignment, the principle of wisdom, which
offers the capability of interoception for organizations, comes from
the unifying methodology, and will be explored in more depth in
Chapter 16.

Summary
This chapter has provided an in-depth exploration of 10 indispensable principles for
the design of intelligent systems. These principles traverse a wide array of topics,
ranging from the alignment of organizational data management to the strategic use
of information theory for enhanced decision making. We scrutinized the vital role
of error correction as an intelligence metric and investigated tactical approaches to
problem simplification.
Moreover, this chapter wove together insights from both human and machine intel‐
ligence to identify universal principles. In the realm of machines, we explored strate‐
gies gleaned from Claude Shannon’s information theory; for humans, we described
principles of mastery (drawn from educational psychology) and the nuanced use of
heuristics (from cognitive psychology).
The principle of wisdom, is the connective tissue between all of these diverse princi‐
ples. It underscores the importance of collective mastery and lifelong learning as
crucial elements in intelligent organizational decision making. As we move forward,
we will expand the notion of mastery to include both user experiences and predictive
systems, using the framework of Bloom’s taxonomy as a guide.
Chapter 16 explores the next phase: the development of unified intelligence within
your organization. If the principles of designing intelligence are your North Star
in this unifying journey, then achieving unified intelligence is your next critical
milestone.

296 | Chapter 15: Principles of Designing Intelligence



CHAPTER 16
Toward Unified Intelligence

Chapter 15 introduced 10 essential principles for designing intelligence to equip data
champions for the next phase of their journey: going beyond data management into
the realm of actionable intelligence strategies. In this final chapter, we elevate the
discourse by exploring the key approaches that organizations can adopt to seamlessly
weave artificial intelligence (AI) and foundational principles of designing intelligence
into their operational fabric.
In this chapter, we demystify three cornerstone approaches to intelligence that are
redefining modern enterprise strategies, each offering a unique blend of artificial and
human elements:
Functional artificial intelligence

A pragmatic approach focusing on problem solving through the targeted deploy‐
ment of AI or ML. This strategy homes in on specific operational functions,
driving measurable gains in efficiency.

Collective intelligence
This strategy often employs autonomous AI agents like drones or groups within
organizations, and has parallels to swarm intelligence.

Collaborative intelligence
Capitalizes on human-guided choices and machine-generated data insights, cul‐
minating in a self-perpetuating cycle of improvement for both individual deci‐
sion making and organizational efficiency.

297



The subsequent sections of this chapter will guide you through the application of
a unified framework to these intelligence strategies, integrating them with the 10
principles of intelligent design introduced in Chapter 15:
Unified intelligence

An ecosystem approach of collective and collaborative intelligence—to remove
ambiguity, knowledge gaps, and blind spots; learn how to anticipate users’ knowl‐
edge needs; automate and augment processes in the search for the ultimate
advantage in efficiency

Codifying principles of intelligence
Examines how to codify the 10 principles into applications and integrate them
into unified intelligence strategy

Functional Artificial Intelligence
AI promises unprecedented opportunities for innovation and efficiency, but accord‐
ing to a Gartner study, an astonishing 80% of AI projects never see the light of day.
Among those that do, profitability is roughly a coin toss (60%). This glaring gap
between expectation and reality signals a pressing need to rethink our approach to AI
in business settings.

Your AI Is Only as Good as Your Data
As you’ve read our book, you’re already familiar with this indispensable truth: quality
data is the backbone of any successful AI initiative. Ambiguity, knowledge gaps, and
blind spots can doom an AI project from the outset. Thus, if you’re undertaking an
AI project, it’s crucial to preemptively tackle these issues to ensure your venture’s
success:
Unpack every assumption

Understand that the data your organization has accumulated is not the starting
point; it’s a result of past decisions made by different teams and leaders. These
decisions carry with them a host of assumptions that must be unpacked.

Start with knowledge
What information do you need for which objective, and how do you measure
success?

Use a data product approach
Data products are introduced in Chapter 4, and using the four facets of data
products gives everyone a specific playbook for identifying necessary informa‐
tion about your data: context, meaning, and structure.

298 | Chapter 16: Toward Unified Intelligence



Beware Illusions Within Vetting Processes
While executives might think they’re engaging in thorough vetting processes while
making decisions within the confines of allocated budgets and resources, this
approach can be misleading. Vetting strategies concerning talent and vendor part‐
nerships can be complex, unduly influenced by leaders and team members’ internal
politics, biases, and preexisting relationships. Typical corporate strategies such as pro‐
curement teams can also have these challenges, which creates bottlenecks. Moreover,
these decisions are often made based on a misunderstanding of the actual challenges
or objectives at hand, leading to misalignment between strategy and execution.
Embracing a unifying approach provides a deep understanding of the intricate rela‐
tionship between data and business value. While no process is immune to flawed
decision making, a unifying strategy aims to make the evaluation as objective as
possible. Ignoring data hygiene from the get-go sets you on a path to failure.

Question Assumptions
Here’s a real-life example from one of the authors. Ron was working on a project
involving a huge, multimillion dollar digital transformation of a department that
provided support for trouble tickets. The department was a pure cost center to the
CFO with little business value.
Using the unifying approach, questioning assumptions, and tracing back questions
with a knowledge framework and data product approach, it was discovered that
about 60% of the trouble tickets were because of business logic errors and bugs on the
website! Instead of taking years and millions of dollars to develop an AI to implement
robotic process automation (RPA) to reduce headcount expenses—a downstream
strategy—fixing the website was a project that could be accomplished with lightning
speed and minimal cost in comparison.

Collective Intelligence
Picture a squadron of drones flying in perfect harmony, as illustrated in Figure 16-1.
While each drone operates autonomously through AI, they all work toward a com‐
mon goal guided by a centralized system and directed by human oversight.

Collective Intelligence | 299



Figure 16-1. Drones in a collective intelligence paradigm have enough autonomy to
manage their own controls, but their goals are controlled via centralized system(s) and
an intelligence hub access API, which are managed by humans.

When humans vote collectively to make decisions, that is also known as collective
intelligence. You may have heard of the phrase “the wisdom of the crowd,” which is a
phenomenon where the aggregate opinion or decision of a group tends to be more
accurate than the opinion of an individual within that group. This concept can be
seen in action in contexts such as public opinion polling, prediction markets, and
even game shows. For example, imagine participating in a game where people try to
guess the number of marbles in a jar. While you might correctly believe that although
individual guesses can vary widely, and are often wrong, the average guess of the
group often comes remarkably close to the actual number! Collective wisdom can
outperform individual judgment.
But collective intelligence is not confined to human populations or robots. As previ‐
ously mentioned, AI agents operating in a networked environment can showcase
collective behaviors. For instance, AI algorithms in the stock market can evaluate
massive amounts of data quickly and predict potential trends by working in tandem.
Such collective operations allow these algorithms to provide more nuanced and
sophisticated insights than if they were operating independently.

300 | Chapter 16: Toward Unified Intelligence



Collaborative Intelligence
Collaborative intelligence bridges the gap between human intuition and machine
precision at scale. In today’s world, where technology and human expertise are
increasingly interwoven and data complexity is skyrocketing, the value of collabora‐
tive intelligence cannot be overstated.
Figure 16-2 shows an example of a collaborative intelligence paradigm. A user
prompts a system to automate a task: scanning emails to produce reports. Various
AI agents execute this, educating humans in the process. Simultaneously, these AI
agents relay their discoveries to a central hub, promoting collective learning. While
collective intelligence emphasizes group dynamics, collaborative intelligence under‐
scores the symbiotic relationship between humans and machines.

Figure 16-2. A user’s request, channeled through an API, where an intelligence hub
can use AI agents or any ML recommender system to generate the required report.
Concurrently, the agents relay the request and their insights to a centralized intelligence
hub.

Collaborative Intelligence | 301



Machine teaching refers to designing applications aimed at fast-tracking ML. At
the heart of collaborative intelligence lies the continuous feedback cycle, wherein
machines adapt based on human choices and preferences. The key to building collab‐
orative intelligence is maximizing the amount of data generated that is preoptimized
for machine learning, which often can be greatly aided by design-related, goal-setting
choices (as discussed in Chapter 10 with success spectrums).
Ideally, a machine’s predictive capabilities are continuously refined by human exper‐
tise, and an organization’s workforce is continuously getting better tools and recom‐
mendations to aid in decision making.

Unified Intelligence
When data is unified (as you learned to do in Chapters 1–14), magic happens.
Powerful new capabilities unlock: collective and collaborative intelligence that have
integrated unifying’s data hygiene practices and its principles of designing intelligence
in their DNA.
This ecosystem of intelligence still follows the basic practices of removing ambiguity,
knowledge gaps, and blind spots, all of which benefit—and benefit from—the ability
to augment and automate the creation, curation, and dissemination of knowledge,
whether from the holistic collective view of the organization (zooming out) or the
perspective of an individual in a specific context (zooming in). This ability is based in
the principles of wisdom and mastery from Chapter 15.
Unified intelligence has a few key advantages—collaborative learning networks, per‐
sonalized knowledge, and scale-free causality—that enable an organization can con‐
tinually self-optimize at all scales, both internally and externally.

Collaborative Learning Networks
In a collaborative learning network, human and machine intelligences come together
to learn from each other. These networks are multinodal, with each node being a
participant (either human or machine) in the learning process. Through ML algo‐
rithms and human insights, knowledge is accumulated, shared, and applied across the
network. This approach accelerates learning and adaptation and democratizes access
to knowledge; insights gained at one node can be disseminated to others, enriching
the entire network and fostering innovation.

302 | Chapter 16: Toward Unified Intelligence



For example, consider a city where each government department and agency has its
own unique data standards, making it difficult for them to learn from one another
effectively. Contrast this with another city where all agencies and nonprofit organiza‐
tions use a unified data model. In the second scenario, entities can seamlessly share
insights and information, enabling collective, organization-wide learning.

Personalized Knowledge
Personalized knowledge enhances collaborative intelligence with the principle of mas‐
tery from Chapter 15. Its focus is predicting and preventing mistakes, like a tutor
guiding a student. This requires an intelligent system able to identify knowledge gaps,
implement intervention strategies, and learn precisely what works for whom in which
situation and why. The term for this in education is personalized learning. (For a
refresher on a more formal definition of knowledge gaps and interventions within a
knowledge framework, please review Chapter 10.)
KPIs

Metrics assessing user progression efficiency and effectiveness.
System state

Decisions by the system to optimize user experience, such as A/B test variations.
User state

Archiving of user’s immediate actions for future personalized engagements.
Objectives

Goals for each stage in a user’s journey and the overarching process aim.
Success criteria

Computational definitions for successful user state transitions.
Value

Evaluates and represents stages in user journeys to the value of organizational
aims.

As an example, consider Figure 16-3, where a user is about to take an action in
a defined action space, as explained in the context of Markov decision processes
(MDPs). (See Chapter 15’s discussion of the principle of decidability for a refresher on
MDPs and the principle of state transitions.)

Unified Intelligence | 303



Figure 16-3. A sophisticated decision-making process within a collaborative intelligence
system. The process is a series of interconnected stages, each playing a vital role in
determining user actions and system recommendations.

The following is a list of the new elements in this state transition visualization:
Observer

This AI agent captures the user’s potential actions at Decision point 2.
Decision sherpa

Predicts the most optimal state for the user based on the observed data and uses
its understanding to influence the user toward the optimal state. Think of it as a
sherpa helping move the user to the right branch of the tree.

Nudge
The system then generates recommendations, depicted here by the light bulb
icon. Recommendations are tailored interventions meant to guide the user
toward the best decision. The efficacy and relevance of these nudges are crucial,
as they can directly influence the user’s decision making.

Intelligence Hub API
All of the interactions are sent via an API to an intelligence hub to undergo
evaluation. Here, the system seeks answers to pivotal questions: Was the recom‐
mendation followed? Did it result in a better outcome? Was the nudge effective?

304 | Chapter 16: Toward Unified Intelligence



Was the recommendation accurate? This feedback loop ensures that the system is
continuously learning and refining its predictions and recommendations.

Sherpas are renowned for their mountaineering expertise and their
intimate knowledge of the local terrain in the high Himalayan
regions of Nepal. The term sherpa has evolved to generally denote
a guide or an expert adept at navigating challenging paths, particu‐
larly within mountaineering contexts.
A decision sherpa is a process or algorithm designed to optimize
decision making by guiding or influencing traversal through deci‐
sion trees or other structured processes that enhance decision
making in fields like data science or management. Such a sherpa
embeds knowledge directly into the decision-making process.

Anticipatory Design: Personalization and Digital Twins
Digital twins serve as virtual replicas of physical systems, capturing real-time data that
can be used for monitoring, analysis, and computational influence. With the power
to simulate real-world scenarios and interactions, digital twins amplify the potential
of predictive systems by encapsulating user behaviors, preferences, and interactions,
leading to more tailored and precise user experiences.
To leverage the power of digital twins, adhere to the structured knowledge format.
Detail goals, system states, user states, efficacy metrics, and more. Each interaction
with a digital twin is unique, allowing you to amass a repository of behavioral data.
Over time, and with enough data, these patterns can be harnessed for predictive
insights and precision personalization.
The key advantage of personalization and digital twins is the ability to create antici‐
patory design experiences. An example of this would be anticipating what a user is
searching for and ranking the different questions they might ask in order to augment
searches.
This technique can be applied to any type of task completion, as long as the system
understands the goal, the action space, and the possible system states so that it can
learn how to self-optimize organizational efficiencies.

Unified Intelligence | 305



Codifying Principles of Intelligence
Talk is cheap. Show me the code.

—Linus Torvalds

In the journey to unifying intelligence in your organization, the foundational princi‐
ple from Chapter 15 is alignment, with each principle adding an additional layer,
much like the pyramid in Bloom’s taxonomy in the principle of mastery. At the apex,
is the principle of wisdom.
The uniqueness of this principles-centric approach is its direct codification into
applications, transforming unifying into a tangible, interactive experience. The uni‐
fying methodology, being both a strategic and a technical framework, seamlessly
integrates into user experience design, data management, and holistic intelligence
strategies.
Figure 16-4 demonstrates how unifying’s techniques can be integrated into the user
experience or codified. It features Flow, an example application from Intelligence.AI,
which serves as a process mapping tool. Introduced in “Hidden Threats to Organ‐
izations: A Modern Parallel” on page xiii, concepts act as the fundamental units
for communicating information in the unifying methodology. These concepts can
exist in two forms: as data optimized for machine readability, and as knowledge
tailored for human understanding. To manage these concepts programmatically in
user experiences, we now introduce the concept codex, as a strategy for embedding
unifying annotations into any of your applications.
The concept codex allows users in the Flow app to annotate any process UI element
with a rating (from very low to very high) from attributes defined in the alignment
principle (Chapter 15). This means that if something in a process has ambiguity or
knowledge gaps, it can be tagged, rated, and given an annotated description. If several
processes across teams have ambiguities, knowledge gaps, or other attributes such as
pain point levels, they can be automatically detected and analyzed, and can provide
insights.
This approach can be replicated for any of the 10 principles of designing intelligence,
and when implementing a unified intelligence strategy, codifying these principles in a
concept codex can be extremely powerful, because it means unifying can be scaled up
from a single data champion to a holistic and omnipresent intelligence for the entire
organization.

306 | Chapter 16: Toward Unified Intelligence



Figure 16-4. The Flow application. On its right side is the concept codex, which is
designed to integrate the 10 principles of intelligence design—highlighted at the bottom
left—into the user interface. A key feature is its ability to capture user feedback, such as
ratings and descriptive annotations.

Codifying Principles of Intelligence | 307



Continuous Human–Machine Learning Loops
A concept codex initially allows users to describe and annotate processes. Over
time, it can also incorporate unifying techniques into the user experience. This
strategy enables a virtuous cycle of continuous human–machine learning; machines
offer predictive recommendations to help humans improve, and humans provide
feedback through the user interface that teaches the machine how to refine its
recommendations.
Here are the three phases of designing and implementing learning loops, using the
example from Figure 16-4, where we can annotate any ambiguity or knowledge gaps
in a process map:

1. Training phase. Initially, an ML algorithm uses data to train a model. Once the
model is trained, the learning algorithm’s work is essentially done for that version
of the model.

2. Operational phase. The trained model then identifies ambiguities and knowledge
gaps, offering recommendations to resolve them. Although we sometimes refer
to this as a recommendation algorithm, it’s actually the output of the trained
model.

3. Retraining phase. User feedback on these recommendations serves as new data.
This data is used to retrain the model, starting another training phase. The model
is updated, and a new operational phase of more accurate recommendations
begins.

For instance, if the machine says, “I found ambiguity in your definition of the word
revenue. Is this what you meant?” with Yes/No buttons for feedback, and the user
clicks Yes, both parties learn something valuable. The human learns their original
definition was ambiguous and how to correct it, effectively being taught by the
machine. Conversely, if the human clicks either Yes or No, the machine learns the
quality of its recommendation. If further data shows that the recommendation led
to a meaningful event, such as a sale or high-performing marketing campaign, that
enriches the data for future training cycles.

Applying Wisdom in Practice
When designing intelligent systems, you can choose how much to invest in the
following:
Algorithmic complexity

This involves developing advanced models and algorithms, generally requiring
investment in high-performance computing resources and specialized personnel
like data scientists.

308 | Chapter 16: Toward Unified Intelligence



Framework-based knowledge design
This concentrates on clarifying objectives, comprehending the scenarios at hand,
and utilizing precise, quality data to inform decision making. The aim is to
reduce ambiguity and improve the quality of domain-specific knowledge.

Wisdom is the self-awareness an organization exercises to self-optimize its approach
to blending algorithmic complexity and knowledge engineering when designing
intelligent systems. This balance is crucial because—per the No Free Lunch Theorem—
no algorithm is universally optimal; effective AI applications often require a blend of
advanced reasoning algorithms and high-quality, domain-specific knowledge.
Having both algorithmic sophistication and knowledge engineering is the super‐
power you want to give your organization in a unified intelligence ecosystem. In this
approach, reasoning and knowledge are inversely proportional. The more valuable
your knowledge, the less reasoning power you need, and vice versa.
Utilizing this approach has led to tangible successes. For instance, Juan garnered
multiple awards at the University of Oxford for breakthroughs in the realm of data
serialization (binary encoding) by creating a technology called JSON BinPack, which
stands for JSON binary packing, guided by the principles of data hygiene elucidated in
this book.
What sets JSON BinPack apart from its competitors is its capability for comprehen‐
sively describing data for the purpose of serialization. This depth of understanding
allows JSON BinPack’s static analysis to make more informed and accurate decisions,
consistently outperforming all considered alternatives across a range of use cases.
Simply put, JSON BinPack succeeds because it operates on a richer dataset, reducing
the need for guesswork.

Conceptual Zoomability
As introduced in the Preface (see Figure P-1), zoomability showcases various layers
of granularity, and unifying requires understanding alignment across knowledge
domains such as business, data, and code, as well as how they align across hierarchi‐
cal levels, viewing the organization from a holistic perspective. These levels include:
Data product level

Concepts are the discrete, unique, and foundational units of information
described in JSON Schema. Designed for both human and machine comprehen‐
sion, they underpin data products.

UX level
This level represents the human-computer interaction space, driving the organi‐
zation’s inputs and outputs.

Codifying Principles of Intelligence | 309



Organizational networks level
Symbolizing the intricate web of people, policies, and technology, this level
anchors the broader perspective.

To leverage a concept codex in your applications, the zoomability feature is indispen‐
sable. It allows users to effortlessly traverse through varying user experience layers,
much like zooming in and out on a digital map.
Figure 16-5 depicts a 100% zoom on a document or user experience: the input/output
layers where content is delivered and input is accepted. Diving deeper, more refined
layers are code, data, and concepts. Zooming out connects UX elements to broader
realms, spanning from knowledge, processes, and governance of the business.

Figure 16-5. Conceptual zoomability in the concept codex allows navigation through
hierarchical layers. Zoom out to navigate to broader domains like the success spectrum
(Chapter 10) and the CLEAN governance framework (Chapter 7). Zooming in reveals
the granular details of a concept’s JSON Schema representation.

Recursive Design
Conceptual zoomability is an incredibly versatile and powerful way to approach
designing and architecting systems. Imagine it like a digital map. You can zoom in to
focus on tiny details or zoom out to understand the bigger picture.
This ability to toggle between layers introduces a remarkable feature: self-similarity
across data, processes, and design. In simpler terms, self-similarity is a pattern that
recurs across varying scales. For instance, consider the branching patterns of trees,
from their trunks out to their smaller limbs. Or think about the vortex shape of
water draining in a sink—this same shape is echoed in large-scale phenomena like

310 | Chapter 16: Toward Unified Intelligence



hurricanes. It’s worth noting that visual instances of self-similarity, such as these, are
often termed fractal patterns.
This recursive property can be intentionally placed by using the concept codex
approach to manage data products, or process maps in an application, as shown
in Figure 16-4. This makes the conceptual zoomability have the property of recursive‐
ness, which is self-repeating patterns. In popular language terminology, these are
often called “fractals.”

Wisdom Graphs: Connecting Concepts, Actions,
and Outcomes

How you do anything is how you do everything.
—Martha Beck

Let’s revisit our definition of wisdom from Chapter 15:
The ability to self-optimize the relationship between knowledge and reasoning capabili‐
ties across all levels and connections in an organization.

What do we really mean when we talk about knowledge? Definitions differ across
fields, from organizing facts in databases to solving complex problems in education.
In recent decades, knowledge graphs have aimed to encode semantic, relational
knowledge through a networked “things, not strings” approach. Yet even knowledge
graphs focus on factual understanding—the bottom tiers of the mastery pyramid
from Chapter 15.
How might we expand both the depth and breadth of digitized knowledge? If knowl‐
edge graphs take a “things not strings” approach, a wisdom graph takes a “holistic,
relativistic, and synergistic” viewpoint:
Holistic knowledge

This connects technical systems and human perspectives into an integrated
whole, like networks instead of isolated points.

Relativistic knowledge
It grounds truths within shared frames of reference, enabling universal measures
while honoring multiple vantage points, like how mountain heights are defined
in relation to sea level.

Synergistic knowledge
This refers to measurable improvements in efficiencies and value of how knowl‐
edge is combined. In other words, having lots of well-organized data that isn’t
creating beneficial outcomes is like organizing chairs on the Titanic—it’s a cost

Wisdom Graphs: Connecting Concepts, Actions, and Outcomes | 311



center that drains people’s time and budgets, and introduces complexity and
processes to maintain.

Cognitive Primitives: Standardizing Cognitive Experience Design
In order to connect concepts, actions, and outcomes across various systems, we need
to establish a standardized format that aims to optimize information for designing
intelligent systems. This involves using data products designed for holistic, relativis‐
tic, and synergistic purposes.
Consider the following nine cognitive primitives, which are fundamental and simple
representations used to capture perspectives from both humans and AI agents in
various scenarios, including internal (employee) and external (customer) experiences:

• Who. The person or AI agent using the system
• Wish. The desired future state or objective
• Where. The position in a process to achieve an objective
• Why. The decision-making factors in a situation
• Which. The action taken versus potential alternatives
• When. Temporal information about events
• What. Data, knowledge, or interaction at a given moment
• Wow. The impact and value of an experience
• Warranty. Elements related to trust, provenance, and safety

For instance, in a healthcare application, the who could be a patient or a healthcare
provider, with the wish representing their health goals. The where might involve
the stage of treatment they are in, while why and which could encompass treatment
choices based on medical history and current health status. When and what provide
context and data for these decisions, wow reflects the effectiveness of treatment,
and warranty ensures the safety and trustworthiness of the process. This real-world
application demonstrates how these cognitive primitives can be practically employed
to enhance the user experience in a critical sector like healthcare.
This modular, scale-free approach of the wisdom graph is what enables holistic
knowledge and captures the relativistic (unique) perspectives of members of the
network. The specific nine cognitive principles set the user experience and data
required to optimize for creating human and learning experiences in a synergistic way,
as shown in Figure 16-6.

312 | Chapter 16: Toward Unified Intelligence



Figure 16-6. The nine cognitive primitives, shown on top as a ring, are a complete unit
that acts as a type of nexus, bringing together UX design and data perspectives to form
an optimal shape to create cognitive experiences for collaborative learning networks. This
pattern can be described as a lattice, symmetrical rings that can be extended with equal
properties.

Wisdom Graphs: Connecting Concepts, Actions, and Outcomes | 313



The Value of Unifying
I learned that it does not matter if you are a light…if you are not going to illuminate
another’s road.

—Walt Disney

The ultimate advantage an organization can possess is the ability to instantly trans‐
form every action taken by an employee into a learning opportunity for the entire
community—be it customers, colleagues, or collaborative learning networks. This
turns each goal, decision, and outcome into building blocks of collective wisdom
that can be distributed to individuals, empowering them to achieve their goals more
effectively and efficiently than ever before.
Unifying is going to enable your organization to optimize this balance between
complexity and knowledge more effectively because the expensive and often overly
complex knowledge management processes that exist today have to be managed by
specialized, centralized, and highly controlled small teams with complex enterprise
and expensive licenses.

Prioritize Knowledge Before AI
There exists an inverse proportionality between reasoning and knowledge. When
there are ambiguity, knowledge gaps, and blind spots between rich domain knowl‐
edge and clear organizational goals, organizations commonly attempt to resolve
this via increased algorithmic complexity and vice versa, following the adage “to a
hammer, everything looks like a nail.”
Most organizations default to overcompensating for a lack of quality datasets by
investing in complex systems and specialized teams. This often leads to high failure
rates in AI projects.
Though knowledge engineering often yields quicker, more reliable results than raw
computational power, it’s not without its challenges. Organizing knowledge in a
downstream fashion—after accumulating a mass of disordered data—is highly ineffi‐
cient. The real power lies in establishing knowledge frameworks first and enforcing
data governance from the get-go. Refer back to Figure 3-2 for a review of the chal‐
lenges in the cycle of upstream and downstream data being continuously transformed
and reused.
The question, therefore, is what is the most efficient and effective way to codify your
organization’s knowledge? As you’ve read the book, you already know the answer, and
it most likely challenges the current strategies your organization uses to manage data
and knowledge.

314 | Chapter 16: Toward Unified Intelligence



A Tale of Simple Knowledge Versus Complex Intelligence
Picture a sprawling lake teeming with fish. On one end, there’s a luxurious boat
outfitted with cutting-edge technology. Its owner is wealthy and surrounded by a
crew of highly educated individuals. They’ve spent years learning the science of
fishing, aquatic ecology, and even ML algorithms to predict where the fish might be.
On the other side of the lake is a lone fisherman in a modest boat, equipped with just
a fishing rod and a small net. What he lacks in academic credentials and sophisticated
tools he makes up for with a simple yet invaluable piece of local knowledge: at sunset,
the most prized fish in the lake gather at a particular spot.
As the sun dips below the horizon, it’s the lone fisherman who reels in the day’s best
catch, not the team with their advanced degrees and high-tech equipment.
In this tale, knowing when and where to fish outperforms even the most advanced
algorithmic complexities. Sometimes the right piece of knowledge can indeed be
more powerful than a mountain of data or an army of experts.

Follow the Principle of Integrated Simplicity
Unifying gives you the knowledge you need to design and engineer the most valuable
knowledge for your organization in the simplest ways and with the lowest algorithmic
complexity.
By adopting the technologies recommended in this book, you will provide your
technical teams with the most straightforward and accessible approach to implemen‐
tation, leveraging JSON—the most widely used data format—and JSON Schema, the
industry standard for schema languages.
We hope you find the value of unifying is not only in the transformative experience
we are confident your organization and teams will have, but also in its simplicity and
ease of implementation.
Our final insight for you, our data champion, is this: it is wisdom that enables you
to shift from solving problems to ensuring they never exist in the first place. It is
through unifying your business, data, and code that you gain the wisdom to create
wisdom for your organization.

Summary
The paradigm of unified intelligence ushers in unparalleled possibilities. By foster‐
ing collaborative networks, prioritizing personalized knowledge, and leveraging the
prowess of digital twins, organizations can redefine the paradigms of decision mak‐
ing, personalization, and prediction.

Summary | 315



At the time of the writing of this book, unified intelligence is an
upcoming inevitability at the vanguard of the possible. We hope
you carefully consider a unified intelligence approach, and that this
chapter helped clarify any of the confusing terminology in various
intelligence strategies.

Going Beyond This Book
Thank you for entrusting us to guide you through this exploration. As we close this
final chapter, we are humbled by your engagement and excited for the groundbreak‐
ing work you’re destined to undertake—work that will weave data and collaboration
into harmonized innovation efforts.
We won’t stop here. To further support your journey, we will be sharing a treasure
trove of free tools and educational materials about unifying at Intelligence.AI.
Please consider this an open invitation to connect; you’re always welcome to reach
out to us at hello@intelligence.ai for questions, further insights, and workshops to
learn more about unifying.
In closing, we can’t wait to witness the remarkable steps you’ll take to reshape and
better our world through what you have learned in unifying. We look forward to
learning how unifying is a resource for you to achieve the extraordinary.

316 | Chapter 16: Toward Unified Intelligence



Index

Symbols facilitating assessments of conceptual
" (double quote), 27 alignment across technical/nontechnical
/ (backslash), 28 teams, 67-69
[ ] (square brackets), 28 “go slow to go smooth, go smooth to go
{ } (curly braces), 29 fast” approach to aligning innovation

efforts, 69-70
A illuminating misalignment with a concept

compass, 104-110
acceleration as journey, not destination, 43-48

pathways of (see pathways of acceleration) orchestrating at organizational scale, xix-xx
tools for (see process maps; success spec‐ planning stages, 44

trums) principle of, 275-276
action space, 303 quality of data at moment of transduction,
aggregation, dataset lineage and, 269-271 276
Agile reducing ambiguity via symmetry between

benefits of a unifying data strategy for, concepts and JSON Schema, 102
11-15 strategies for setting up teams for success,

combining with waterfall and Unifying, 13 46-48
defined, 11-15 synchronization (see synchronization)
defining being Agile, 12 transforming abstract to concrete, 275
improper use of, 9 unifying versus, 52-54
origins, 11 validating concepts, 111-113
primary characteristics in relation to data, ambiguity

12 assumptions and, 102
velocity versus productivity, 13 defined, 102, 155

AI (see artificial intelligence) influence on decisions and progress toward
alignment, 99-114 goals, 155-156

ambiguity and assumptions as inimical to, as inimical to alignment, 101
100-102 map projection example, 156-157

data-centric innovation and, 43-48 process maps for revealing, 159
defined, 52 reducing via symmetry between concepts
effective alignment as goal, 45-46 and JSON Schema, 102
evaluating from a holistic perspective, 43-45 revealing by enriching process maps with

annotations, 162-163

317



visualizing/removing in processes, 160-163 B
anchoring heuristic, 291 backslash (/), 28
annotating process maps, 162-163 Barabási, Albert-László, on networks, 129
annotation extraction, 144-151 Beck, Martha, on wisdom, 311

defined, 6 being Agile, defined, 12
deprecations use case, 145-146 belief scoring, 112-113
example, 144 Belsky, Scott, on ideas, 191
extracting unknown keywords as annota‐ biases, xii

tions, 234-236 (see also heuristics)
format keyword and, 150 as constraint on human behavior, xii
with Hyperjump, 249-250 heuristics versus, 291
online validator for, 151 big data, good data versus, 16
revealing ambiguity by enriching process Binary JSON (BSON), 35

maps with annotations, 162-163 blind spots
runtime extraction, 146-148 defined, 156
runtime performance affected by, 148 influence on decisions and progress toward
standard output formats, 148-150 goals, 155-156

annotation keywords, 144 removing, 174-176
anticipatory design Bloom’s taxonomy, 292-294

digital twins, 305 Boolean schemas, 139-140
personalization, 305 Booleans, JSON, 27

Applicator vocabulary, JSON Schema, 84-86 bottom-up progressions, 4
Aristotle (Google team study project), 46 BSON (Binary JSON), 35
arrays bundling, JSON Schema, 255-260

converting dataset to JSON array, 215 basics, 255
JSON, 28-29 bundling meta-schemas, 260

artificial intelligence (AI) bundling process, 258-259
aligning problem-solving strategies and data bundling signup-analytics data product

with, xvi-xvii example, 259-260
as collective intelligence, 300 business logic
functional AI, 298-299 CLEAN data governance and, 124
illusions within vetting process, 299 harmonizing perspectives with concept
prioritizing knowledge before AI, 314 compass, 105-106
quality data and, 298
questioning assumptions, 299
“Your AI Is Only as Good as Your Data” C

axiom, xiv CDO (chief data officer), roles and responsibili‐
assess phase ties of, 40

for concept-first design, 65-67 centralized control, 285
“go slow to go smooth, go smooth to go ChatGPT, 231

fast” approach to aligning innovation chief data officer (CDO), roles and responsibili‐
efforts, 69-70 ties of, 40

assessments (knowledge assessments), 166 CLEAN (Collaborative Learning Networks)
assumptions data governance

defined, 102 activity, 124
as inimical to alignment, 102 business logic, 124
questioning, 299 collaboration, 122

availability heuristic, 291 four facets of data products and CLEAN,
126

318 | Index



four horsemen of data death, 127-129 validating, 111-113
fundamentals, 120-125 validating with belief scoring, 112-113
knowledge creation, 123 validating with counterfactuals, 111
in practice, 125 conceptual schemas, power of, 67

Cloudflare Pages conceptual zoomability, 309-311
creating a custom domain in, 206 confirmation bias, 291
deploying schema registry to, 195-200 constraints language, JSON Schema as, 137-139
setting up a CNAME DNS record, 208 consuming vocabularies, 247-250

CNAME DNS record, 208 defining a dialect, 247
cognitive primitives, 312 extracting annotations with Hyperjump,
collaboration, CLEAN data governance and, 249-250

122 making use of dialects, 249
collaborative intelligence, 301-302 context vocabulary specification, 239-241
collaborative learning networks, 302 continuums, principle of, 284-286
collaborative networks, 129-130 control strategy problem, 285-286
collective intelligence, 299 dangers of misusing measurements, 284
communication networks, integrated simplicity making things measurable, 284

in, 283 Core vocabulary, 82
complexity reduction, 282 counterfactuals, 111
compliance issues, in business logic, 124 cover your ass (CYA) culture, 13
compression, 283 Crockford, Douglas, 21
concept compass curly braces ({ }), 29

harmonized concepts, 109-110 customers, problems described by, 180
harmonizing business logic perspectives CYA (cover your ass) culture, 13

(the “what”), 105-106
harmonizing JSON Schema (the “how”), D

108-109 data
harmonizing operational tasks (the “way”), aligning AI and problem-solving strategies

106-107 with, xvi-xvii
illuminating misalignment with, 104-110 quality at moment of transduction, 276

concept-first design, 57-71 raw data versus data product, 59
assessing concepts, 65-67 data cascades, 16
basics, 57-58 data champions (data product managers)
benefits of, 10 and data transformations, 40-42
coffee packaging example, 59-60 and data-centric innovation, 39-55
defined, 9 unifying as responsibility of, 54
facilitating assessments of conceptual data consumer, defined, 50

alignment across technical/nontechnical data customer, defined, 50
teams, 67-69 data distributors, defined, 51

four facets of a data product, 60-62 data driven (term), 58
getting started with, 63-64 data hygiene, 65
“go slow to go smooth, go smooth to go challenges that compromise, xiii

fast” approach to aligning innovation goals of, 154
efforts, 69-70 data managers, roles/responsibilities of, 41

unification blueprint, 64-65 data problems, 3-6
concepts data producers, defined, 51

as foundation of unifying methodology, xiv data product
reducing ambiguity via symmetry between CLEAN methodology and, 126

concepts and JSON Schema, 102

Index | 319



concept-first design for (see concept-first heterogeneous data structures, 140-141
design) JSON Schema and, 134-143

four facets of, 60-62 online validator for, 135
raw data versus, 59 data-centric innovation, 39-55

data product design using JSON Schema, alignment as journey, not destination, 43-48
213-231 data champions as key to data transforma‐
automated schema extraction, 229 tions, 40-42
context facet, 228 data product managers and, 42
data facet, 214 incorporating a product management mind‐

CSV dataset example, 214 set, 48-52
JSON row representation, 215 unifying versus aligning, 52-54

meaning facet, 222-228 data-driven breakthroughs, quest for, 1-11
analytics-entry schema, 227 data problems, 3-6
currency schema, 225 multiple/conflicting North Stars, 2
email schema, 224 understanding which are the data right
IP address schema, 223 problems to solve, 7-9
price schema, 226 unifying concepts as key to innovation, 9-11
timestamp schema, 223 data-driven organizations
US state data, 224 becoming data centric, 16-17
website-milestone schema, 227 bottlenecks preventing teams from becom‐

signup analytics schema, 229 ing data driven, 17-18
structure facet, 215-222 understanding the phrase “being data

application-specific concepts, 220 driven”, 15-18
cost and currency, 218-220 dataset keyword, 253, 261, 267
dataset schema, 220 dataset lineage
email addresses, 217 aggregation, 269-271
general-purpose concepts, 215-220 filtering, 267
IP address, 217 JSON Unify and, 266-271
timestamp, 217 transforming entries, 268-271
US state codes, 218 dataset schema, 220

data product management, 187 dataset vocabulary, 253-255
(see also product management; product datasetRef keyword, 261, 263, 267

managers) decentralized control, 285
data quality, in AI, 298 decidability, principle of, 288-289
data strategies (see unifying data strategy, need decidability defined, 288

for) making informed decisions, 289
data strategists real-world decidability to reduce misalign‐

product managers’ KPI use compared to, 43 ment in teams, 290
roles and responsibilities, 41, 43 two key approaches to problem solving, 289
strategies for setting up teams for success, decision sherpas, 305

46-48 decomposition, 283
data transformations, data champions as key to, $defs keyword, 258

40-42 Deming, W. Edwards, on data, 102
data validation demons

Boolean schemas, 139-140 and thought processes for innovation, 179
JSON Schema as a constraints language, defined, 179

137-139 imagining magical possibilities with, 181
example, 136-137 real-life example leading to a unified data
format keyword, 142-143 product model, 184-186

320 | Index



deprecated keyword, 145-146 exploration for new knowledge, xvi
deprecations, 145-146 extending JSON Schema, 233-251
design (see specific forms of design, e.g., authoring vocabularies, 236-247

concept-first design) consuming vocabularies, 247-250
designing intelligence, 273-296 unknown keywords, 234-236

defined, xiv
overview of principles, 274-275 F
principle of alignment, 275-276 “faster horses” anecdote, 180
principle of continuums, 284-286 filtering, dataset lineage and, 267
principle of decidability, 288-289 Flow app, 306
principle of heuristics, 290-292 format keyword, 142-143, 150
principle of information, 278-280 four horsemen of data death, 127-129
principle of integrated simplicity, 282-283 ignorance, 128
principle of learning, 280-282 indecisiveness, 128
principle of mastery, 292-295 shortsightedness, 128
principle of state transitions, 286-288 siloed incentives, 128
principle of wisdom, 295 fractal patterns, 311
unifying journey recap, 273 Friedland, Bernard, on feedback, 12

dialects, JSON Schema Fuller, Buckminster, on models and change, 120
defining a dialect, 247 functional artificial intelligence, 298-299
definition of term, 75
determining with $schema keyword, 78-79
extracting annotations with Hyperjump, G

249-250 Galilei, Galileo, on measuring, 284
making use of, 249 Galton board, 129

digital twins, 305 General Data Protection Regulation (GDPR),
Disney, Walt, on illumination, 314 124
DNS (Domain Name System) configuration, GitHub repository setup for schema registry,

208 192-195
double quote ("), 27 creating a repository, 192
downstream data journey, 51 uploading first schema, 193-195
Drucker, Peter, on measurement and manage‐ goals, measuring progress toward, 157

ment, 284 good data, big data versus, 16
$dynamicAnchor keyword, 244 Google, Aristotle team study project at, 46

granularity, levels of, xiv
E graph database, 119

graphs, 117-118
effective alignment, 45-46 (see also knowledge graphs; wisdom graphs)
efficiency, effectiveness versus, 15 as visual language of networks, 117-118
Einstein, Albert

on accepting limits, 233
on experiencing the mysterious, 273 H
on solving problems, 133 heuristics, 290-292

email addresses, 217 awareness and ethical considerations, 291
entropy (uncertainty), 278-280 basics, 290-291
enum keyword, 92 biases versus, 291
errors, defining in learning context, 282 connection to decision making in business,
ethical issues in heuristics, 291 292
exploitation of existing knowledge, xvi HIPPO (highest paid person’s opinion), 15

Index | 321



Hoffman, Reid, on teams versus solo operators, IP addresses, 217
45

holistic alignment evaluation, 43-45 J
holistic knowledge, 311 JavaScript, 21
HTTP JSON, 21-38

header configuration, 200-202 alternative representations, 34-36
hosting schemas over, 191 arrays, 28-29

human–machine learning loops, 308 basics, 21-25
hygiene, xii binary alternatives, 35
Hyperjump, 135, 142 Booleans, 27

extending, 245-247 brief history, 21-22
extracting annotations with, 249-250 in concept-first design, 68

data types, 26-31
I document creation, 36
ignorance, as cause of data death, 128 grammar learning resources, 31
illusion of communication, 101 grammar overview, 26-36
imagining magical possibilities, 179-181 JSON data model versus, 36
incentives, siloed, 128 minification, 32-33
indecisiveness, as cause of data death, 128 null constant, 31
information, principle of, 278-280 number data type, 27
innovation objects, 29-30

“go slow to go smooth, go smooth to go prettifying documents, 33
fast” approach to aligning efforts, 69-70 Schema (see JSON Schema)

process maps revealing opportunities for, simple example, 23
163 strings, 27

unifying concepts as key to, 9-11 textual alternatives, 34
using demons and magical thinking for, 179 viewing and authoring tools, 24

instances, defined, 74 JSON BinPack (JSON binary packing), 309
integrated simplicity, principle of, 282-283 JSON data model, JSON versus, 36

complexity reduction, 282 JSON documents, 23
compression, 283 JSON Hero, 24
decomposition, 283 JSON Lines (JSONL), 262
integrating in communication networks, JSON Schema, 73-97

283 basics, 74-75
memoization, 283 Boolean schemas, 139-140
unified intelligence and, 315 brief history, 75

intelligence (see unified intelligence) building blocks, 75-76
Intelligence.AI Coffee Beans (fictitious com‐ bundling, 255-260

pany used as example), 18-19 in concept-first design, 68
generating a web form, 95 as a constraints language, 137-139
JSON document creation for, 36 data product design using (see data product
JSON Schema for addling new catalog list‐ design using JSON Schema)

ings, 91-95 deploying a schema registry, 191-211
product entry, 36 determining dialect with $schema keyword,
store order, 37 78-79
writing a schema, 91-95 dialects (see dialects, JSON Schema)

intergroup bias, 291 extending (see extending JSON Schema)
interventions, defined, 166 format keyword, 142-143

322 | Index



harmonizing via concept compass, 108-109 knowledge experience format, 169-170
heterogeneous data structures, 140-141 knowledge frameworks, 166-171
Intelligence.AI project example, 91-95 knowledge experiences and pathways,
meta-schemas, 76 167-169
primary purposes, 108 from structured knowledge to computa‐
reading/understanding, 76-86 tional knowledge, 171
as recursive data structure, 86-90 as tool for designing knowledge experien‐
reducing ambiguity via symmetry between ces, 169-170

concepts and JSON Schema, 102 knowledge gaps, 166
referencing schemas, 87-90 defined, 155
schema definition, 74 influence on decisions and progress toward
thinking in schemas, 151 goals, 155-156
understanding Schema keywords, 82-86 knowledge graphs
validating the structure of data, 134-143 aligning knowledge for companies new to,
vocabularies (see vocabularies, JSON 120

Schema) challenges with, 119
writing a schema, 91-95 as networks of entities, 118-120

JSON Unify, 253-271 simple, 119
dataset lineage, 266-271 knowledge objectives, 166
dataset vocabulary, 253-255 known unknowns, 175
extracting meaning, 263-265 Kolmogorov complexity, 282
JSON Schema bundling, 255-260 KPIs (key performance indicators)
referencing remote data, 261-263 defining for success measures/metrics, 172,

JSON-e, 268-269 178-179
json-everything online validator, 151 in knowledge experience design, 168
JSON5, 34 OKRs versus, 52
JSONoid, 230

L
K Lao Tzu, on not changing direction, 13
Kahneman, Daniel, 290 learning
key performance indicators (see KPIs) defining, 280-281
keyword namespacing, 81 defining errors, 282
keywords, JSON Schema principle of, 280-282

Applicator vocabulary, 84-86 learning loops, 308
extracting unknown keywords as annota‐ legal issues, business logic and, 124

tions, 234-236 Leonardo da Vinci, on practice without knowl‐
top-level keywords, 82-83 edge, 104
understanding Schema keywords, 82-86 license keyword, 242
unknown keywords, 234-236 local referencing, 88
validation vocabulary, 83-84

knowledge M
levels of mastery in, 293 machine teaching, 302
prioritizing before AI, 314 MacLaughlin, Steve, on the effect of bad data
simple knowledge versus complex intelli‐ on AI, 134

gence, 315 magical thinking (imagining magical possibili‐
knowledge creation, CLEAN data governance ties), 179-181

and, 123 map projections, 156
knowledge experience design, 167-170 MapReduce, 269-270

Index | 323



Markov decision process (MDP), 289, 303 Newtonsoft, 142
mastery, principle of, 292-295 “next best states”, 173

levels of mastery in knowledge, 293 nonexistent values, null values versus, 31
strategies for mastery, 294-295 North Stars, 2

Maxwell, James Clerk, 180 nudges/nudging, 183
Maxwell’s demon, 180 null constant, JSON, 31
MDP (Markov decision process), 289, 303 null values, nonexistent values versus, 31
Meadows, Donella H., on systems, 115
meaning facet of data product design, 222-228 O

analytics-entry schema, 227 objectives and key results (see OKRs)
currency schema, 225 objectives, in knowledge experience design, 168
email schema, 224 objects, JSON, 29-30
extracting meaning with JSON Unify, Occam’s razor, 282

263-265 OK JSON, 25
IP address schema, 223 OKRs (objectives and key results)
price schema, 226 business goals and, 167
timestamp schema, 223 definition of elements in, 178
US state data, 224 KPIs versus, 52
website-milestone schema, 227 online JSON Schema validator, 135, 151

memoization, 283 (see also Hyperjump)
meta-schemas, 76 operational tasks, harmonizing, 106-107

bundling, 260 optional vocabularies, 80
defining dialects with, 247 output formats (JSON Schema), 148-150
inspecting, 79
vocabulary (see vocabulary meta-schema)

metadata, annotation keywords and, 144 P
metrics packaging, 59-60

defining KPIs for success measures/metrics, pain points, in problem landscapes, 182-183
178-179 Pascal, Blaise, on simplicity, 282

for product management success, 52 Pasteur, Louis, xii
minification pathways of acceleration, illuminating, 153-164

JSON, 32-33 basics, 153-154
prettifying JSON documents, 33 defining decisions and steps with process

minimum viable data (MVD), 15 maps, 158-159
misalignment how ambiguity/knowledge gaps/blind spots

illuminating with a concept compass, influence decisions and progress toward
104-110 goals, 155-156

real-world decidability to reduce misalign‐ how process maps reveal ambiguity, 159
ment in teams, 290 mapping pathways of process/progress,

multiperspective design, 176-178 157-160
MVD (minimum viable data), 15 measuring progress toward goals, 157

visualizing/removing ambiguity in pro‐
N cesses, 160-163

pathways, defined, 166
namespacing, 81 personalization, in anticipatory design, 305
needs of users, defining, 49-50 personalized knowledge, 303-305
Netscape, role in JSON’s origins, 21 prettifying JSON documents, 33
networks, thinking in (see thinking in net‐ problem landscapes, 182-183

works) data problems, 3-6

324 | Index



quantifying pain points in, 182-183 thinking in schemas, 151
real-life example leading to a unified data $schema keyword

product model, 184-186 declaring URI with, 76
problem-solving strategies, xvi-xvii determining dialect with, 78-79, 91
process maps schema registry

defining decisions and steps with, 158-159 adding a custom domain, 205-209
enriching with annotations, 158-159 checking results, 209
innovation opportunities revealed by, 163 creating a domain in Cloudflare Pages,
revealing ambiguity with, 159 206
visualizing/removing ambiguity with, setting up a CNAME DNS record, 208

160-163 best practices, 210
product management configuring HTTP headers, 200-202

defining and measuring success, 52 checking results, 202
defining product features, 50-52 declaring custom headers on Cloudflare
defining users’ needs, 49-50 Pages, 201-202
incorporating a product management mind‐ inspecting current HTTP headers, 201

set, 48-52 creating a landing page, 204-205
product managers deploying, 191-211

data strategists’ KPI use compared to, 43 deploying to Cloudflare Pages, 195-200
roles and responsibilities, 41 GitHub repository setup, 192-195
skills required of, 48 hosting schemas over HTTP, 191

product packaging, 59-60 treating schemas as immutable, 210
productivity, velocity versus, 13 versioning, 210
progress, mapping, 172 self-similarity, 310
properties, objects and, 29 Semmelweis, Ignaz, xi-xii

Shannon, Claude, 278-280
Q Shaw, George Bernard, on communication, 101
quantification, decision making and, 284 sherpas, 305

shortsightedness, as cause of data death, 128

R signup-analytics (data product example), 221
analytics-entry schema, 227

raw data, data product versus, 59 bundling, 259-260
recursive data structure, 86-90 dataset entries, 221
Reed, Brian, on design, 213 dataset schema, 220
reinforcement learning, xvi dataset vocabulary, 254
relativistic knowledge, 311 extracting annotations, 250
remote referencing, 90 extracting meaning with JSON Unify,

JSON Lines, 262 263-265
JSON Unify, 261-263 new dialect and annotation keywords, 249
streaming JSON, 262 publishing JSONL variant, 262

ripple effects, xii referencing remote data, 261-263
road maps, for success spectrum, 176-178 signup analytics schema, 229
runtime annotation extraction, 146-148 siloed incentives, 128

simplicity, integrated, 282-283
S Sinek, Simon
schema, 67 on champions, 40

(see also JSON Schema) on understanding people, 17
defined, 74 social proof heuristic, 292
power of conceptual schemas, 67 SPDX (Software Package Data Exchange), 242

Index | 325



specification (JSON Schema vocabulary sys‐ power of design in collaborative networks,
tem), 237-241 129-130

spectrums of success (see success spectrums) thinking in networks, 116-118
square brackets ([ ]), 28 synergistic knowledge, 311
state transitions, principle of, 286-288 system state, in knowledge experience design,

simple state machine, 287 168
simplifying state transitions, 287 systems control strategy, 285

streaming, remote referencing and, 262
strings, JSON, 27 T
subschemas, JSON Schema teams

local referencing, 88 bottlenecks preventing teams from becom‐
remote referencing, 90 ing data driven, 17-18
schema duplication, 87 characteristics of successful teams, 47

success criteria facilitating assessments of conceptual
defining and measuring, 52 alignment across technical/nontechnical
for knowledge experience design, 168 teams, 67-69

success spectrums, 165-189 real-world decidability to reduce misalign‐
basics, 172-183 ment in, 290
circular nature of unifying, 188 strategies for success, 46-48
data product management, 187 thinking in networks, 116-118
defining KPIs for success measures/metrics, athletes versus artists, 116

178-179 basics, 116-118
embracing multiperspective design and graphs as visual language of networks,

road maps, 176-178 117-118
imagining magical possibilities, 181 top-down progressions, 4
knowledge frameworks and, 166-171 top-level keywords, 82-83
listening to customers’ problems versus cus‐ Torvalds, Linus, on codifying intelligence, 306

tomers’ solutions, 180 transduction, data quality at moment of, 276
mapping progress and value, 172 transforming entries, extending dataset vocabu‐
nudging and, 183 lary to support, 268-269
problem landscapes, 182-183 tribal knowledge, 102
real-life problem landscape and demon Tversky, Amos, 290

example leading to a unified data prod‐
uct model, 184-186

removing blind spots, 174-176 U
using demons and magical thinking for Ueshiba, Morihei, on mistakes, 280

innovation, 179 uncertainty, 278-279
visualizing/adding “next best states”, 173 unified intelligence, 297-316

sunk cost fallacy, 291 anticipatory design, 305
synchronization, 115-131 applying wisdom in practice, 308

CLEAN data governance fundamentals, basics, 302
120-125 codifying the principles of, 306-311

CLEAN data governance in practice, 125 collaborative intelligence, 301-302
four facets of data products and CLEAN, collaborative learning networks, 302

126 collective intelligence, 299
four horsemen of data death, 127-129 conceptual zoomability, 309-311
knowledge graphs as networks of entities, continuous human–machine learning loops,

118-120 308
functional artificial intelligence, 298-299

326 | Index



integrated simplicity, 315 validator, online (see Hyperjump; online JSON
personalized knowledge, 303-305 Schema validator)
prioritizing knowledge before AI, 314 value
simple knowledge versus complex intelli‐ as element of knowledge experience design,

gence, 315 169
value of unifying, 314-315 mapping, 172
wisdom graphs, 311-313 Van Gogh, Vincent, on accomplishing great

unifying, 273 things, 253
aligning versus, 52-54 velocity, productivity versus, 13
assess phase of concept-first design, 65-67 versioning of schemas, 210
benefits of, 70 Visual Studio Code, 25
circular nature of, 188 vocabularies, JSON Schema, 75
combining with Agile or waterfall method‐ Applicator vocabulary, 84-86

ology, 13 authoring, 236-247
concept-first design unification blueprint, consuming vocabularies, 247-250

64-65 defined, 75
deeper principles guiding, 273 defining a custom vocabulary, 236
defined, 53 defining a dialect, 247
facilitating assessments of conceptual determining, 79-80

alignment across technical/nontechnical extending an implementation, 244-247
teams, 67-69 extracting annotations with Hyperjump,

origin story of, xviii 249-250
unifying concepts, as key to innovation, 9-11 inspecting meta-schemas, 79
unifying data strategy keyword namespacing, 81

benefits for Agile, 11-15 as not transitive, 248
defining a unifying data strategy approach, optional vocabularies, 80

14-15 reference documentation for, 81
Intelligence.AI Coffee Beans project, 18-19 understanding, 81
need for, 1-20 validation vocabulary, 83-84
quest for data-driven breakthroughs, 1-11 vocabulary system, 236
understanding the phrase “being data writing a meta-schema, 241-244

driven”, 15-18 context vocabulary meta-schema, 243
unknown keywords, 234-236 official vocabularies meta-schemas, 241
unknown unknowns, 156, 175 SPDX licenses, 242

(see also blind spots) writing a specification, 237-241
upstream data journey, 51 context vocabulary specification,
user experience with data product, 60 239-241
user state, 168 vocabulary identifiers, 238
users’ needs, defining, 49-50 $vocabulary keyword, 80

vocabulary meta-schema, writing, 241-244
V context vocabulary meta-schema, 243
validating concepts, 111-113 official vocabularies meta-schemas, 241

with belief scoring, 112-113 SPDX licenses, 242
with counterfactuals, 111

validation keywords, 140-141 W
validation of data (see data validation) waterfall
validation vocabulary, JSON Schema, 83-84 combining Agile with, 13
validation, defined, 6 defined, 11

Index | 327



web forms, 95 Wittgenstein, Ludwig, on limits of language, 73
Weinberger, David, on metadata, 144
whitespace, minification of, 32-33 Y
wisdom YAML format, 35

applying in practice, 308
defined, 309, 311
principle of, 295 Z

wisdom graphs, 311-313 zoomability, xiii, 187, 309-311

328 | Index



About the Authors
Ron Itelman is cofounder of Intelligence.AI, which aims to help organizations align
their business and data strategies for a scientific approach to innovation. Ron previ‐
ously worked on Personalized Learning & Analytics and Computational Psychomet‐
rics in education, and Collaborative Intelligence in Life Sciences and BioPharma.
Juan Cruz Viotti is a computer scientist with ample experience in IoT and systems
programming. He graduated from the University of Oxford, and his research led
to the proposal of an award-winning novel data serialization technology: JSON Bin‐
Pack. He currently researches data interchange technologies at his own research lab:
Sourcemeta, leads the Desktop Engineering team at Postman, and collaborates in
the JSON Schema organization. Juan previously served as the Engineering Lead at
Balena.



Colophon
The animal on the cover of Unifying Business, Data, and Code is a jackdaw. Closely
related to ravens, jays, crows, rooks, and magpies, jackdaws are the smallest of the
Corvidae family (commonly known as the crow or corvid family). There are two
species of jackdaw: the western jackdaw (Coloeus monedula) and the Daurian jackdaw
(Coloeus dauuricus).
Western jackdaws (also known as Eurasian or European jackdaws) are found across
Europe, western Asia, and North Africa. Duarian jackdaws are native to eastern
Asia (China and eastern Siberia to Japan). They live in a variety of habitats, such as
farmland, urban and suburban settings, woodlands, coastal cliffs, and grasslands.
Both species look similar when they are young and dominated by glossy black feath‐
ers. As they reach adulthood, western jackdaws tend to grow gray plumage around
the nape of their necks and have light irises. Daurian jackdaws develop creamy,
white plumage that extends from their lower parts to up around their necks and
have darker irises. The western species is slightly larger (13 to 15 inches long) than
Daurian birds (13 inches long).
Jackdaws are highly intelligent, gregarious, and vocal birds. They tend to have com‐
plex social structures and spend the majority of their day scavenging on the ground
for food. They are opportunistic omnivores that eat a variety of foods, including—but
not limited to—insects, farm grains and cultivated fruits, chick and eggs of other
birds, and carrion.
While jackdaws are considered to be of least concern on endangered species lists,
many of the animals on O’Reilly covers are endangered; all of them are important to
the world.
The cover illustration is by Karen Montgomery, based on an antique line engraving
from British Birds. The series design is by Edie Freedman, Ellie Volckhausen, and
Karen Montgomery. The cover fonts are Gilroy Semibold and Guardian Sans. The
text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the
code font is Dalton Maag’s Ubuntu Mono.



Learn from experts.  
Become one yourself.
Books | Live online courses   
Instant answers | Virtual events 
Videos | Interactive learning

Get started at oreilly.com. 
©2023 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc.  175  7x9.1975}}
{{Data Governance 
 The Definitive Guide
People, Processes, and Tools to Operationalize 
Data Trustworthiness

Evren Eryurek, Uri Gilad,  
Valliappa Lakshmanan,  

Anita Kibunguchy-Grant  
& Jessi Ashdown






Praise for Data Governance: The Definitive Guide

We live in a digital world. Whether we realize it or not, we are living in the throes of one
of the biggest economic and social revolutions since the Industrial Revolution. It’s about

transforming traditional business processes—many of them previously nondigital or
manual—into processes that will fundamentally change how we live, how we operate our

businesses, and how we deliver value to our customers. Data defines, informs, and
predicts—it is used to penetrate new markets, control costs, drive revenues, manage risk,

and help us discover the world around us. But to realize these benefits, data must be
properly managed and stewarded. Data Governance: The Definitive Guide walks you

through the many facets of data management and data governance—people, process and
tools, data ownership, data quality, data protection, privacy and security—and does it in a

way that is practical and easy to follow. A must-read for the data professional!
—John Bottega, president of the EDM Council

Enterprises are increasingly evolving as insight-driven businesses, putting pressure on
data to satisfy new use cases and business ecosystems. Add to this business complexity,

market disruption, and demand for speed, and data governance is front and center to
make data trusted, secure, and relevant. This is not your grandfather’s slow and

bureaucratic data governance either. This book shares the secrets into how modern data
governance ensures data is the cornerstone to your business resilience, elasticity, speed,

and growth opportunity and not an afterthought.
—Michele Goetz, vice president/principal analyst–business

insights at Forrester



Data governance has evolved from a discipline focused on cost and compliance to one
that propels organizations to grow and innovate. Today’s data governance solutions can

benefit from technological advances that establish a continuous, autonomous, and
virtuous cycle. This in turn becomes an ecosystem—a community in which data is used

for good, and doing the right thing is also the easy thing. Executives looking to use data as
an asset and deliver positive business outcomes need to rethink governance’s role and

adopt the modern and transformative approach Data Governance: The Definitive Guide
provides.

—Jim Cushman, CPO of Collibra



Data Governance:
The Definitive Guide

People, Processes, and Tools to Operationalize
Data Trustworthiness

Evren Eryurek, Uri Gilad, Valliappa Lakshmanan,
Anita Kibunguchy-Grant, and Jessi Ashdown

Beijing Boston Farnham Sebastopol Tokyo



Data Governance: The Definitive Guide
by Evren Eryurek, Uri Gilad, Valliappa Lakshmanan, Anita Kibunguchy-Grant, and Jessi Ashdown
Copyright © 2021 Uri Gilad, Jessi Ashdown, Valliappa Lakshmanan, Evren Eryurek, and Anita
Kibunguchy-Grant. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are
also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional
sales department: 800-998-9938 or corporate@oreilly.com.

Acquisitions Editor: Jessica Haberman Indexer: WordCo Indexing Services, Inc.
Development Editor: Gary O’Brien Interior Designer: David Futato
Production Editor: Kate Galloway Cover Designer: Karen Montgomery
Copyeditor: Piper Editorial Consulting, LLC Illustrator: Kate Dullea
Proofreader: Arthur Johnson

March 2021:  First Edition

Revision History for the First Edition
2021-03-08: First Release

See http://oreilly.com/catalog/errata.csp?isbn=9781492063490 for release details.

The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Data Governance: The Definitive Guide,
the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.
The views expressed in this work are those of the authors, and do not represent the publisher’s views.
While the publisher and the authors have used good faith efforts to ensure that the information and
instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility
for errors or omissions, including without limitation responsibility for damages resulting from the use of
or reliance on this work. Use of the information and instructions contained in this work is at your own
risk. If any code samples or other technology this work contains or describes is subject to open source
licenses or the intellectual property rights of others, it is your responsibility to ensure that your use
thereof complies with such licenses and/or rights.

978-1-492-06349-0
[LSI]



Table of Contents

Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi

1. What Is Data Governance?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1
What Data Governance Involves                                                                                     2

Holistic Approach to Data Governance                                                                      4
Enhancing Trust in Data                                                                                               5
Classification and Access Control                                                                                6
Data Governance Versus Data Enablement and Data Security                               8

Why Data Governance Is Becoming More Important                                                 9
The Size of Data Is Growing                                                                                         9
The Number of People Working and/or Viewing the Data Has Grown

Exponentially                                                                                                             10
Methods of Data Collection Have Advanced                                                           10
More Kinds of Data (Including More Sensitive Data) Are Now

Being Collected                                                                                                         13
The Use Cases for Data Have Expanded                                                                   14
New Regulations and Laws Around the Treatment of Data                                   16
Ethical Concerns Around the Use of Data                                                               16

Examples of Data Governance in Action                                                                     17
Managing Discoverability, Security, and Accountability                                        18
Improving Data Quality                                                                                              19

The Business Value of Data Governance                                                                      23
Fostering Innovation                                                                                                   23
The Tension Between Data Governance and Democratizing Data Analysis       24
Manage Risk (Theft, Misuse, Data Corruption)                                                      25
Regulatory Compliance                                                                                               26
Considerations for Organizations as They Think About Data Governance       28

Why Data Governance Is Easier in the Public Cloud                                                 30

v



Location                                                                                                                         31
Reduced Surface Area                                                                                                  32
Ephemeral Compute                                                                                                    32
Serverless and Powerful                                                                                               32
Labeled Resources                                                                                                        33
Security in a Hybrid World                                                                                         34

Summary                                                                                                                           34

2. Ingredients of Data Governance: Tools. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37
The Enterprise Dictionary                                                                                              37

Data Classes                                                                                                                   38
Data Classes and Policies                                                                                            40
Data Classification and Organization                                                                       43
Data Cataloging and Metadata Management                                                           44
Data Assessment and Profiling                                                                                  45
Data Quality                                                                                                                  46
Lineage Tracking                                                                                                          46
Key Management and Encryption                                                                             47
Data Retention and Data Deletion                                                                             50
Workflow Management for Data Acquisition                                                          52
IAM—Identity and Access Management                                                                  52
User Authorization and Access Management                                                          54

Summary                                                                                                                           55

3. Ingredients of Data Governance: People and Processes. . . . . . . . . . . . . . . . . . . . . . . . . . .  57
The People: Roles, Responsibilities, and Hats                                                              57

User Hats Defined                                                                                                        58
Data Enrichment and Its Importance                                                                        65

The Process: Diverse Companies, Diverse Needs and Approaches to Data
Governance                                                                                                                   65
Legacy                                                                                                                             66
Cloud Native/Digital Only                                                                                          67
Retail                                                                                                                              67
Highly Regulated                                                                                                          69
Small Companies                                                                                                          71
Large Companies                                                                                                          72

People and Process Together: Considerations, Issues, and Some Successful
Strategies                                                                                                                        73
Considerations and Issues                                                                                           74
Processes and Strategies with Varying Success                                                        77

Summary                                                                                                                           84

vi | Table of Contents



4. Data Governance over a Data Life Cycle. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  85
What Is a Data Life Cycle?                                                                                              85
Phases of a Data Life Cycle                                                                                             86

Data Creation                                                                                                                87
Data Processing                                                                                                            88
Data Storage                                                                                                                  88
Data Usage                                                                                                                     88
Data Archiving                                                                                                              89
Data Destruction                                                                                                          89

Data Life Cycle Management                                                                                         90
Data Management Plan                                                                                               90

Applying Governance over the Data Life Cycle                                                          92
Data Governance Framework                                                                                    92
Data Governance in Practice                                                                                      94
Example of How Data Moves Through a Data Platform                                        97

Operationalizing Data Governance                                                                            100
What Is a Data Governance Policy?                                                                         101
Importance of a Data Governance Policy                                                               102
Developing a Data Governance Policy                                                                    103
Data Governance Policy Structure                                                                           103
Roles and Responsibilities                                                                                         105
Step-by-Step Guidance                                                                                              106
Considerations for Governance Across a Data Life Cycle                                   108

Summary                                                                                                                         111

5. Improving Data Quality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  113
What Is Data Quality?                                                                                                   113
Why Is Data Quality Important?                                                                                 114

Data Quality in Big Data Analytics                                                                          116
Data Quality in AI/ML Models                                                                                117

Why Is Data Quality a Part of a Data Governance Program?                                 121
Techniques for Data Quality                                                                                        121

Scorecard                                                                                                                     123
Prioritization                                                                                                               123
Annotation                                                                                                                  123
Profiling                                                                                                                       124

Summary                                                                                                                         131

6. Governance of Data in Flight. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  133
Data Transformations                                                                                                   133
Lineage                                                                                                                            134

Why Lineage Is Useful                                                                                               135

Table of Contents | vii



How to Collect Lineage                                                                                             135
Types of Lineage                                                                                                         136
The Fourth Dimension                                                                                              138
How to Govern Data in Flight                                                                                  139

Policy Management, Simulation, Monitoring, Change Management                    141
Audit, Compliance                                                                                                         141
Summary                                                                                                                         142

7. Data Protection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  143
Planning Protection                                                                                                       143

Lineage and Quality                                                                                                   144
Level of Protection                                                                                                     145
Classification                                                                                                               146

Data Protection in the Cloud                                                                                       146
Multi-Tenancy                                                                                                            147
Security Surface                                                                                                          147
Virtual Machine Security                                                                                          148

Physical Security                                                                                                            149
Network Security                                                                                                        151
Security in Transit                                                                                                      151

Data Exfiltration                                                                                                             153
Virtual Private Cloud Service Controls (VPC-SC)                                               155
Secure Code                                                                                                                156
Zero-Trust Model                                                                                                       157

Identity and Access Management                                                                                158
Authentication                                                                                                            158
Authorization                                                                                                              159
Policies                                                                                                                         160
Data Loss Prevention                                                                                                 161
Encryption                                                                                                                  162
Differential Privacy                                                                                                    164
Access Transparency                                                                                                  165

Keeping Data Protection Agile                                                                                    165
Security Health Analytics                                                                                          165
Data Lineage                                                                                                               166
Event Threat Detection                                                                                             167

Data Protection Best Practices                                                                                     167
Separated Network Designs                                                                                      168
Physical Security                                                                                                         168
Portable Device Encryption and Policy                                                                  170
Data Deletion Process                                                                                                170

Summary                                                                                                                         173

viii | Table of Contents



8. Monitoring. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  175
What Is Monitoring?                                                                                                     175
Why Perform Monitoring?                                                                                           176
What Should You Monitor?                                                                                         179

Data Quality Monitoring                                                                                          179
Data Lineage Monitoring                                                                                          180
Compliance Monitoring                                                                                            182
Program Performance Monitoring                                                                          183
Security Monitoring                                                                                                   185

What Is a Monitoring System?                                                                                     187
Analysis in Real Time                                                                                                187
System Alerts                                                                                                              187
Notifications                                                                                                                187
Reporting/Analytics                                                                                                   188
Graphic Visualization                                                                                                188
Customization                                                                                                            188

Monitoring Criteria                                                                                                       189
Important Reminders for Monitoring                                                                        190
Summary                                                                                                                         191

9. Building a Culture of Data Privacy and Security. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  193
Data Culture: What It Is and Why It’s Important                                                      193
Starting at the Top—Benefits of Data Governance to the Business                       194

Analytics and the Bottom Line                                                                                195
Company Persona and Perception                                                                          195

Intention, Training, and Communications                                                                196
A Data Culture Needs to Be Intentional                                                                 197
Training: Who Needs to Know What                                                                      197

Beyond Data Literacy                                                                                                    200
Motivation and Its Cascading Effects                                                                      200

Maintaining Agility                                                                                                       201
Requirements, Regulations, and Compliance                                                        202
The Importance of Data Structure                                                                           202
Scaling the Governance Process Up and Down                                                     203

Interplay with Legal and Security                                                                                203
Staying on Top of Regulations                                                                                  204
Communication                                                                                                         204
Interplay in Action                                                                                                     204
Agility Is Still Key                                                                                                       205

Incident Handling                                                                                                          205
When “Everyone” Is Responsible, No One Is Responsible                                  205

Importance of Transparency                                                                                        206

Table of Contents | ix



What It Means to Be Transparent                                                                            206
Building Internal Trust                                                                                              206
Building External Trust                                                                                             207
Setting an Example                                                                                                     208

Summary                                                                                                                         208

A. Google’s Internal Data Governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  209

B. Additional Resources. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  217

Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  219

x | Table of Contents



Preface

In recent years, the ease of moving to the cloud has motivated and energized a fast-
growing community of data consumers to collect, capture, store, and analyze data for
insights and decision making. For a number of reasons, as adoption of cloud comput‐
ing continues to grow, information management stakeholders have questions about
the potential risks involved in managing their data in the cloud. Evren faced such
questions for the first time when he worked in healthcare and had to put in place the
processes and technologies to govern data. Now at Google Cloud, Uri and Lak also
answer these questions nearly every week and dispense advice on getting value from
data, breaking down data silos, preserving anonymity, protecting sensitive informa‐
tion, and improving the trustworthiness of data.
We noticed that GDPR was what precipitated a sea change in customers’ behavior.
Some customers even deleted their data, thinking it was the right thing to do. That
reaction, more than any other, prompted us to write this book capturing the advice
we have provided over the years to Google Cloud customers. If data is the new cur‐
rency, we do not want enterprises to be scared of it. If the data is locked away or is not
trustworthy, it is of no value.
We all pride ourselves on helping Google Cloud customers get value for their techni‐
cal expenditures. Data is a huge investment, and we felt obligated to provide our cus‐
tomers with the best way to get value from it.
Customers’ questions usually involve one of three risk factors:
Securing the data

Storing data in a public cloud infrastructure might concern large enterprises that
typically deploy their systems on-premises and expect tight security. With a sig‐
nificant number of security threats and breaches in the news, organizations are
concerned that they might be the next victim. These factors contribute to risk
management concerns for protecting against unauthorized access to or exposure
of sensitive data, ranging from personally identifiable information (PII) to corpo‐
rate confidential information, trade secrets, or intellectual property.

xi



Regulations and compliance
There is a growing set of regulations, including the California Consumer Privacy
Act (CCPA), the European Union’s General Data Protection Regulation (GDPR),
and industry-specific standards such as global Legal Entity Identifier (LEI) num‐
bers in the financial industry and ACORD data standards in the insurance indus‐
try. Compliance teams responsible for adhering to these regulations and
standards may have concerns about oversight and control of data stored in the
cloud.

Visibility and control
Data management professionals and data consumers sometimes lack visibility
into their own data landscape: which data assets are available, where those assets
are located and how and if they can be used, and who has access to the data and
whether they should have access to it. This uncertainty limits their ability to fur‐
ther leverage their own data to improve productivity or drive business value.

These risk factors clearly highlight the need for increased data assessment, cataloging
of metadata, access control management, data quality, and information security as
core data governance competencies that the cloud provider should not only provide
but also continuously upgrade in a transparent way. In essence, addressing these risks
without abandoning the benefits provided by cloud computing has elevated the
importance of not only understanding data governance in the cloud, but also know‐
ing what is important. Good data governance can inspire customer trust and lead to
vast improvements in customer experience.

Why Your Business Needs Data Governance in the Cloud
As your business generates more data and moves it into the cloud, the dynamics of
data management change in a number of fundamental ways. Organizations should
take note of the following:
Risk management

There are concerns about potential exposure of sensitive information to unau‐
thorized individuals or systems, security breaches, or known personnel accessing
data under the wrong circumstances. Organizations are looking to minimize this
risk, so additional forms of protection (such as encryption) to obfuscate the data
object’s embedded information are required to safeguard the data should a sys‐
tem breach occur. In addition, other tools are required in order to support access
management, identify sensitive data assets, and create a policy around their
protection.

Data proliferation
The speed at which businesses create, update, and stream their data assets has
increased, and while cloud-based platforms are capable of handling increased

xii | Preface



data velocity, volume, and variety, it is important to introduce controls and
mechanisms to rapidly validate the quality aspects of high-bandwidth data
streams.

Data management
The need to adopt externally produced data sources and data streams (including
paid feeds from third parties) means that you should be prepared not to trust all
external data sources. You may need to introduce tools that document data line‐
age, classification, and metadata to help your employees (data consumers, in par‐
ticular) to determine data usability based on their knowledge of how the data
assets were produced.

Discovery (and data awareness)
Moving data into any kind of data lake (cloud-based or on-premises) runs the
risk of losing track of which data assets have been moved, the characteristics of
their content, and details about their metadata. The ability, therefore, to assess
data asset content and sensitivity (no matter where the data is) becomes very
important.

Privacy and compliance
Regulatory compliance demands auditable and measurable standards and proce‐
dures that ensure compliance with internal data policies as well as external gov‐
ernment regulations. Migrating data to the cloud means that organizations need
tools to enforce, monitor, and report compliance, as well as ensure that the right
people and services have access and permissions to the right data.

Framework and Best Practices for Data Governance
in the Cloud
Given the changing dynamics of data management, how should organizations think
about data governance in the cloud, and why is it important? According to
TechTarget, data governance is

the overall management of the availability, usability, integrity, and security of data used
in an enterprise. A sound data governance program includes a governing body or
council, a defined set of procedures and a plan to execute those procedures.1

Simply put, data governance encompasses the ways that people, processes and tech‐
nology can work together to enable auditable compliance with defined and agreed-
upon data policies.

1 Craig Stedman and Jack Vaughan, “What Is Data Governance and Why Does It Matter?” TechTarget, Decem‐
ber 2019. This article was updated in February 2020; the current version no longer includes this quote.

Preface | xiii



Data Governance Framework
Enterprises need to think about data governance comprehensively, from data intake
and ingestion to cataloging, persistence, retention, storage management, sharing,
archiving, backup, recovery, loss prevention, disposition, and removal and deletion:
Data discovery and assessment

Cloud-based environments often offer an economical option for creating and
managing data lakes, but the risk remains for ungoverned migration of data
assets. This risk represents a potential loss of knowledge of what data assets are in
the data lake, what information is contained within each object, and where those
data objects originated from. A best practice for data governance in the cloud is
data discovery and assessment in order to know what data assets you have. The
data discovery and assessment process is used to identify data assets within the
cloud environment, and to trace and record each data asset’s origin and lineage,
what transformations have been applied, and object metadata. (Often this meta‐
data describes the demographic details, such as the name of the creator, the size
of the object, the number of records if it is a structured data object, or when it
was last updated.)

Data classification and organization
Properly evaluating a data asset and scanning the content of its different
attributes can help categorize the data asset for subsequent organization. This
process can also infer whether the object contains sensitive data and, if so, clas‐
sify it in terms of the level of data sensitivity, such as personal and private data,
confidential data, or intellectual property. To implement data governance in the
cloud, you’ll need to profile and classify sensitive data to determine which gover‐
nance policies and procedures apply to the data.

Data cataloging and metadata management
Once your data assets are assessed and classified, it is crucial that you document
your learnings so that your communities of data consumers have visibility into
your organization’s data landscape. You need to maintain a data catalog that con‐
tains structural metadata, data object metadata, and the assessment of levels of
sensitivity in relation to the governance directives (such as compliance with one
or more data privacy regulations). The data catalog not only allows data consum‐
ers to view this information but can also serve as part of a reverse index for
search and discovery, both by phrase and (given the right ontologies) by concept.
It is also important to understand the format of structured and semi-structured
data objects and allow your systems to handle these data types differently, as
necessary.

xiv | Preface



Data quality management
Different data consumers may have different data quality requirements, so it’s
important to provide a means of documenting data quality expectations as well as
techniques and tools for supporting the data validation and monitoring process.
Data quality management processes include creating controls for validation,
enabling quality monitoring and reporting, supporting the triage process for
assessing the level of incident severity, enabling root cause analysis and recom‐
mendation of remedies to data issues, and data incident tracking. The right pro‐
cesses for data quality management will provide measurably trustworthy data for
analysis.

Data access management
There are two aspects of governance for data access. The first aspect is the provi‐
sioning of access to available assets. It’s important to provide data services that
allow data consumers to access their data, and fortunately, most cloud platforms
provide methods for developing data services. The second aspect is prevention of
improper or unauthorized access. It’s important to define identities, groups, and
roles and assign access rights to establish a level of managed access. This best
practice involves managing access services as well as interoperating with the
cloud provider’s identity and access management (IAM) services by defining
roles, specifying access rights, and managing and allocating access keys to ensure
that only authorized and authenticated individuals and systems are able to access
data assets according to defined rules.

Auditing
Organizations must be able to assess their systems to make sure that they are
working as designed. Monitoring, auditing, and tracking (who did what and
when and with what information) helps security teams gather data, identify
threats, and act on those threats before they result in business damage or loss. It’s
important to perform regular audits to check the effectiveness of controls in
order to quickly mitigate threats and evaluate overall security health.

Data protection
Despite the efforts of information technology security groups to establish perim‐
eter security as a way to prevent unauthorized individuals from accessing data,
perimeter security is not and never has been sufficient for protecting sensitive
data. While you might be successful in preventing someone from breaking into
your system, you are not protected from an insider security breach or even from
exfiltration (data theft). It’s important to institute additional methods of data pro‐
tection—including encryption at rest, encryption in transit, data masking, and
permanent deletion—to ensure that exposed data cannot be read.

Preface | xv



Operationalizing Data Governance in Your Organization
Technology certainly helps support the data governance principles presented in the
preceding section, but data governance goes beyond the selection and implementa‐
tion of products and tools. The success of a data governance program depends on a
combination of:

• People to build the business case, develop the operating model, and take on
appropriate roles

• Processes that operationalize policy development, implementation, and
enforcement

• Technology used to facilitate the ways that people execute those processes

The following steps are critical in planning, launching, and supporting a data gover‐
nance program:

1. Build the business case. Establish the business case by identifying critical busi‐
ness drivers to justify the effort and investment associated with data governance.
Outline perceived data risks (such as the storage of data on cloud-based plat‐
forms) and indicate how data governance helps the organization mitigate those
risks.

2. Document guiding principles. Assert core principles associated with governance
and oversight of enterprise data. Document those principles in a data governance
charter to present to senior management.

3. Get management buy-in. Engage data governance champions and get buy-in
from the key senior stakeholders. Present your business case and guiding princi‐
ples to C-level management for approval.

4. Develop an operating model. Once you have management approval, define the
data governance roles and responsibilities, and then describe the processes and
procedures for the data governance council and data stewardship teams who will
define processes for defining and implementing policies as well as reviewing and
remediating identified data issues.

5. Establish a framework for accountability. Establish a framework for assigning
custodianship and responsibility for critical data domains. Make sure there is vis‐
ibility to the “data owners” across the data landscape. Provide a methodology to
ensure that everyone is accountable for contributing to data usability.

6. Develop taxonomies and ontologies. There may be a number of governance
directives associated with data classification, organization, and—in the case of
sensitive information—data protection. To enable your data consumers to com‐
ply with those directives, there must be a clear definition of the categories (for
organizational structure) and classifications (for assessing data sensitivity).

xvi | Preface



7. Assemble the right technology stack. Once you’ve assigned data governance
roles to your staff and defined and approved your processes and procedures, you
should assemble a suite of tools that facilitate ongoing validation of compliance
with data policies and accurate compliance reporting.

8. Establish education and training. Raise awareness of the value of data gover‐
nance by developing educational materials highlighting data governance practi‐
ces and procedures, and the use of supporting technology. Plan for regular
training sessions to reinforce good data governance practices.

The Business Benefits of Robust Data Governance
Data security, data protection, data accessibility and usability, data quality, and other
aspects of data governance will continue to emerge and grow as critical priorities for
organizations. And as more organizations migrate their data assets to the cloud, the
need for auditable practices for ensuring data utility will also continue to grow. To
address these directives, businesses should frame their data governance practices
around three key components:

• A framework that enables people to define, agree to, and enforce data policies
• Effective processes for control, oversight, and stewardship over all data assets

across on-premises systems, cloud storage, and data warehouse platforms
• The right tools and technologies for operationalizing data policy compliance

With this framework in mind, an effective data governance strategy and operating
model provides a path for organizations to establish control and maintain visibility
into their data assets, providing a competitive advantage over their peers. Organiza‐
tions will likely reap immense benefits as they promote a data-driven culture within
their organizations—specifically:
Improved decision making

Better data discovery means that users can find the data they need when they
need it, which makes them more efficient. Data-driven decision making plays a
huge role in improving business planning within an organization.

Better risk management
A good data governance operating model helps organizations audit their pro‐
cesses more easily so that they reduce the risk of fines, increase customer trust,
and improve operations. Downtime can be minimized while productivity still
grows.

Preface | xvii



Regulatory compliance
Increasing governmental regulation has made it even more important for organi‐
zations to establish data governance practices. With a good data governance
framework, organizations can embrace the changing regulatory environment
instead of simply reacting to it.

As you migrate more of your data to the cloud, data governance provides a level of
protection against data misuse. At the same time, auditable compliance with defined
data policies helps demonstrate to your customers that you protect their private
information, alleviating their concerns about information risks.

Who Is This Book For?
The current growth in data is unprecedented and, when coupled with increased regu‐
lations and fines, has meant that organizations are forced to look into their data gov‐
ernance plans to make sure that they do not become the next statistic. Therefore,
every organization will need to establish an understanding of the data it collects, the
liability and regulation associated with that data, and who has access to it. This book
is for you if you want to know what that entails, the risks to be aware of, and the con‐
siderations to keep in mind.
This book is for anyone who needs to implement the processes or technology that
enables data to become trustworthy. This book covers the ways that people, processes,
and technology can work together to enable auditable compliance with defined and
agreed-upon data policies.
The benefits of data governance are multifaceted, ranging from legal and regulatory
compliance to better risk management and the ability to drive top-line revenue and
cost savings by creating new products and services. Read this book to learn how to
establish control and maintain visibility into your data assets, which will provide you
with a competitive advantage over your peers.

Conventions Used in This Book
The following typographical conventions are used in this book:
Italic

Indicates new terms, URLs, email addresses, filenames, and file extensions.
Constant width

Used for program listings, as well as within paragraphs to refer to program ele‐
ments such as variable or function names, databases, data types, environment
variables, statements, and keywords.

xviii | Preface



This element signifies a tip or suggestion.

This element signifies a general note.

This element indicates a warning or caution.

O’Reilly Online Learning
For more than 40 years, O’Reilly Media has provided technol‐
ogy and business training, knowledge, and insight to help
companies succeed.

Our unique network of experts and innovators share their knowledge and expertise
through books, articles, and our online learning platform. O’Reilly’s online learning
platform gives you on-demand access to live training courses, in-depth learning
paths, interactive coding environments, and a vast collection of text and video from
O’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.

How to Contact Us
Please address comments and questions concerning this book to the publisher:

O’Reilly Media, Inc.
1005 Gravenstein Highway North
Sebastopol, CA 95472
800-998-9938 (in the United States or Canada)
707-829-0515 (international or local)
707-829-0104 (fax)

We have a web page for this book, where we list errata, examples, and any additional
information. You can access this page at https://oreil.ly/data-governance-TDG.

Preface | xix



Email bookquestions@oreilly.com to comment or ask technical questions about this
book.
For news and information about our books and courses, visit http://oreilly.com.
Find us on Facebook: http://facebook.com/oreilly
Follow us on Twitter: http://twitter.com/oreillymedia
Watch us on YouTube: http://www.youtube.com/oreillymedia

Acknowledgments
Thank you to our respective families, teammates, and managers. Gary O’Brien, our
editor at O’Reilly, was a force of nature—this book would not exist without his con‐
stant prodding and invaluable advice. Thanks also to our technical reviewers for their
invaluable suggestions.

xx | Preface



CHAPTER 1
What Is Data Governance?

Data governance is, first and foremost, a data management function to ensure the
quality, integrity, security, and usability of the data collected by an organization. Data
governance needs to be in place from the time a factoid of data is collected or gener‐
ated until the point in time at which that data is destroyed or archived. Along the way
in this full life cycle of the data, data governance focuses on making the data available
to all stakeholders in a form that they can readily access. In addition, it must be one
they can use in a manner that generates the desired business outcomes (insights, anal‐
ysis) and conforms to regulatory standards, if relevant. These regulatory standards
are often an intersection of industry (e.g., healthcare), government (e.g., privacy), and
company (e.g., nonpartisan) rules and codes of behavior. Moreover, data governance
needs to ensure that the stakeholders get a high-quality integrated view of all the data
within the enterprise. There are many facets to high-quality data—the data needs to
be correct, up to date, and consistent. Finally, data governance needs to be in place to
ensure that the data is secure, by which we mean that:

• It is accessed only by permitted users in permitted ways
• It is auditable, meaning all accesses, including changes, are logged
• It is compliant with regulations

The purpose of data governance is to enhance trust in the data. Trustworthy data is
necessary to enable users to employ enterprise data to support decision making, risk
assessment, and management using key performance indicators (KPIs). Using data,
you can increase confidence in the decision-making process by showing supporting
evidence. The principles of data governance are the same regardless of the size of the
enterprise or the quantity of data. However, data governance practitioners will make
choices with regard to tools and implementation based on practical considerations
driven by the environment within which they operate.

1



What Data Governance Involves
The advent of big data analytics, powered by the ease of moving to the cloud and the
ever-increasing capability and capacity of compute power, has motivated and ener‐
gized a fast-growing community of data consumers to collect, store, and analyze data
for insights and decision making. Nearly every computer application these days is
informed by business data. It is not surprising, therefore, that new ideas inevitably
involve the analysis of existing data in new ways, as well as the collection of new data‐
sets, whether through new systems or by purchase from external vendors. Does your
organization have a mechanism to vet new data analysis techniques and ensure that
any data collected is stored securely, that the data collected is of high quality, and that
the resulting capabilities accrue to your brand value? While it’s tempting to look only
toward the future power and possibilities of data collection and big data analytics,
data governance is a very real and very important consideration that cannot be
ignored. In 2017, Harvard Business Review reported that more than 70% of employees
have access to data they should not.1 This is not to say that companies should adopt a
defensive posture; it’s only to illustrate the importance of governance to prevent data
breaches and improper use of the data. Well-governed data can lead to measurable
benefits for an organization.

Spotify Creates Discover Weekly
As an example of how well-governed data can lead to measurable benefits to an orga‐
nization and how the availability of data can completely change an entire industry,
consider the Spotify Discover Weekly feature. In the early 2010s, the way most people
listened to music was to purchase physical/digital albums and rip them to create cus‐
tom playlists. These playlists, consisting of songs you owned, was what you listened
to.
There was also a large and thriving illegal music-sharing ecosystem that consisted of
pirated singles that could be added to your playlists. In an effort to get the pirated
music system under control, music labels allowed the sales of digital singles. As the
size of people’s digital libraries grew, and as internet connections became more relia‐
ble, consumers became willing to keep their purchased tracks online and stream them
to their audio devices. Music labels were also willing to “rent” out music when it was
streamed. Instead of selling the song, the music labels would be paid each time the
song was played.

1 Leandro DalleMule and Thomas H. Davenport, “What’s Your Data Strategy?” Harvard Business Review (May–
June 2017): 112–121.

2 | Chapter 1: What Is Data Governance?



This was how Spotify (which is now the world’s largest music streaming service) got
started. It’s worth noting that Spotify owes its very existence to data governance. It got
started as a way for music labels to get paid for their work—music piracy was deci‐
mating the music industry. Spotify’s entire business model was built around tracking
the songs users played and reimbursing artists for those songs. The ability to prove
that its handling of the data was trustworthy is the reason that Spotify became a viable
music service in the first place.
The fact that Spotify was keeping tabs on which songs users played meant that it had
data on what people listened to. Thus it was now possible for Spotify to recommend
new songs to its listeners. Such recommendation algorithms key off of three things:

• Find other songs by the artists you listen to, or songs in the same genre (e.g.,
1940s jazz). This is called content-based recommendation.

• Find users who like the same songs you do and recommend the songs that those
users like. This is called collaborative filtering.

• Use models that analyze the raw audio files of songs you like and recommend
songs that are similar. The raw audio captures many inherent features, such as
the beat. If you tend to like music with a fast beat and with repeating tonal
phrases, the algorithm can recommend other songs with a similar structure. This
is called similarity matching.

At that point, Edward Newett, an engineer at Spotify, had an interesting idea: instead
of recommending songs one at a time, what if Spotify created a playlist of recommen‐
dations? And so, every Monday, Spotify would recommend what they thought each
individual user would like. This was called Discover Weekly.
Discover Weekly was a huge hit—within a year after its launch, more than 40 million
people had used the service and streamed nearly five billion tracks. The deep person‐
alization had worked. The music sounded familiar but was still novel. The service
allowed music lovers to discover new titles, and new artists to find audiences, and it
gave Spotify’s customers an event to look forward to every week.
Spotify was able to use its recommendation algorithm to provide artists and music
labels with insights about fans’ preferences. It could leverage the recommendation
algorithm to expand users’ music preferences and introduce novel bands. This extra
knowledge and marketing ability has allowed Spotify to negotiate with music publish‐
ers from a position of strength.
None of this would have been possible if Spotify had not assured its users that the
information about their listening habits was being used in a responsible way to
improve their own music-listening experience. European regulators are very protec‐
tive of the privacy of EU citizens. Spotify, being based in Europe, could not have got‐
ten its recommendation systems off the ground if it had not proven that it had robust
privacy controls in place, and that it was ensuring that data scientists could devise
algorithms but not breach data that could be tied to individuals.

What Data Governance Involves | 3



Discover Weekly illustrates how data, properly governed, can create a well-loved
brand and change the power dynamic in an entire industry. Spotify extended its rec‐
ommendations with Spotify Wrapped, where listeners everywhere get a deep dive into
their most memorable listening moments of the year. This is a great way to have peo‐
ple remember and share their most listened-to songs and artists (see Figure 1-1).

Figure 1-1. Anita Kibunguchy-Grant’s 2020 Wrapped playlist

Holistic Approach to Data Governance
Several years ago, when smartphones with GPS sensors were becoming ubiquitous,
one of the authors of this book was working on machine learning algorithms to pre‐
dict the occurence of hail. Machine learning requires labeled data—something that
was in short supply at the temporal and spatial resolution the research team needed.
Our team hit on the idea of creating a mobile application that would allow citizen sci‐
entists to report hail at their location.2 This was our first encounter with making
choices about what data to collect—until then, we had mostly been at the receiving

2 This application is the Meteorological Phenomena Identification Near the Ground (mPING) Project, devel‐
oped through a partnership between NSSL, the University of Oklahoma, and the Cooperative Institute for
Mesoscale Meteorological Studies.

4 | Chapter 1: What Is Data Governance?



end of whatever data the National Weather Service was collecting. Considering the
rudimentary state of information security tools in an academic setting, we decided to
forego all personally identifying information and make the reporting totally anony‐
mous, even though this meant that certain types of reported information became
somewhat unreliable. Even this anonymous data brought tremendous benefits—we
started to evaluate hail algorithms at greater resolutions, and this improved the qual‐
ity of our forecasts. This new dataset allowed us to calibrate existing datasets, thus
enhancing the data quality of other datasets as well. The benefits went beyond data
quality and started to accrue toward trustworthiness—involvement of citizen scien‐
tists was novel enough that National Public Radio carried a story about the project,
emphasizing the anonymous nature of the data collection.3 The data governance lens
had allowed us to carefully think about which report data to collect, improve the
quality of enterprise data, enhance the quality of forecasts produced by the National
Weather Service, and even contribute to the overall brand of our weather enterprise.
This combination of effects—regulatory compliance, better data quality, new business
opportunities, and enhanced trustworthiness—was the result of a holistic approach to
data governance.
Fast-forward a few years, and now, at Google Cloud, we are all part of a team that
builds technology for scalable cloud data warehouses and data lakes. One of the
recurring concerns that our enterprise customers have is around what best practices
and policies they should put in place to manage the classification, discovery, availabil‐
ity, accessibility, integrity, and security of their data—data governance—and custom‐
ers approach it with the same sort of apprehension that our small team in academia
did.
Yet the tools and capabilities that an enterprise has at its disposal to carry out data
governance are quite powerful and diverse. We hope to convince you that you should
not be afraid of data governance, and that properly applying data governance can
open up new worlds of possibility. While you might initially approach data gover‐
nance purely from a legal or regulatory compliance standpoint, applying governance
policies can drive growth and lower costs.

Enhancing Trust in Data
Ultimately, the purpose of data governance is to build trust in data. Data governance
is valuable to the extent that it adds to stakeholders’ trust in the data—specifically, in
how that data is collected, analyzed, published, or used.
Ensuring trust in data requires that a data governance strategy address three key
aspects: discoverability, security, and accountability (see Figure 1-2). Discoverability

3 It was on the radio, but you can read about it on NPR’s All Tech Considered blog.

What Data Governance Involves | 5



itself requires data governance to make technical metadata, lineage information, and
a business glossary readily available. In addition, business critical data needs to be
correct and complete. Finally, master data management is necessary to guarantee that
data is finely classified and thus ensure appropriate protection against inadvertent or
malicious changes or leakage. In terms of security, regulatory compliance, manage‐
ment of sensitive data (personally identifiable information, for example), and data
security and exfiltration prevention may all be important depending on the business
domain and the dataset in question. If discoverability and security are in place, then
you can start treating the data itself as a product. At that point, accountability
becomes important, and it is necessary to provide an operating model for ownership
and accountability around boundaries of data domains.

Figure 1-2. The three key aspects of data governance that must be addressed to enhance
trust in data

Classification and Access Control
While the purpose of data governance is to increase the trustworthiness of enterprise
data so as to derive business benefits, it remains the case that the primary activity
associated with data governance involves classification and access control. Therefore,
to understand the roles involved in data governance, it is helpful to consider a typical
classification and access control setup.
Let’s take the case of protecting the human resources information of employees, as
shown in Figure 1-3.

6 | Chapter 1: What Is Data Governance?



Figure 1-3. Protecting the human resources information of employees

The human resources information includes several data elements: each employee’s
name, their date of hire, past salary payments, the bank account into which those sal‐
ary payments were deposited, current salary, etc. Each of these data elements is pro‐
tected in different ways, depending on the classification level. Potential classification
levels might be public (things accessible by people not associated with the enterprise),
external (things accessible by partners and vendors with authorized access to the
enterprise internal systems), internal (things accessible by any employee of the orga‐
nization), and restricted. For example, information about each employee’s salary pay‐
ments and which bank account they were deposited into would be restricted to
managers in the payroll processing group only. On the other hand, the restrictions
could be more dynamic. An employee’s current salary might be visible only to their
manager, and each manager might be able to see salary information only for their
respective reports. The access control policy would specify what users can do when
they access the data—whether they can create a new record, or read, update, or delete
existing records.
The governance policy is typically specified by the group that is accountable for the
data (here, the human resources department)—this group is often referred to as the
governors. The policy itself might be implemented by the team that operates the data‐
base system or application (here, the information technology department), and so
changes such as adding users to permitted groups are often carried out by the IT
team—hence, members of that team are often referred to as approvers or data stew‐
ards. The people whose actions are being circumscribed or enabled by data gover‐
nance are often referred to as users. In businesses where not all employees have access

What Data Governance Involves | 7



to enterprise data, the set of employees with access might be called knowledge workers
to differentiate them from those without access..
Some enterprises default to open—for example, when it comes to business data, the
domain of authorized users may involve all knowledge workers in the enterprise.
Other enterprises default to closed—business data may be available only to those with
a need to know. Policies such as these are within the purview of the data governance
board in the organization—there is no uniquely correct answer on which approach is
best.

Data Governance Versus Data Enablement and Data Security
Data governance is often conflated with data enablement and with data security.
Those topics intersect but have different emphases:

• Data governance is mostly focused on making data accessible, reachable, and
indexed for searching across the relevant constituents, usually the entire organi‐
zation’s knowledge-worker population. This is a crucial part of data governance
and will require tools such as a metadata index, a data catalog to “shop for” data.
Data governance extends data enablement into including a workflow in which
data acquisition can take place. Users can search for data by context and descrip‐
tion, find the relevant data stores, and ask for access, including the desired use
case as justification. An approver (data steward) will need to review the ask,
determine whether the ask is justified and whether the data being requested can
actually serve the use case, and kick off a process through which the data can be
made accessible.

• Data enablement goes further than making data accessible and discoverable; it
extends into tooling that allows rapid analysis and processing of the data to
answer business-related questions: “how much is the business spending on this
topic,” “can we optimize this supply chain,” and so on. The topic is crucial and
requires knowledge of how to work with data, as well as what the data actually
means—best addressed by including, from the get-go, metadata that describes the
data and includes value proposition, origin, lineage, and a contact person who
curates and owns the data in question, to allow for further inquiry.

• Data security, which intersects with both data enablement and data governance,
is normally thought about as a set of mechanics put in place to prevent and block
unauthorized access. Data governance relies on data security mechanics to be in
place but goes beyond just prevention of unauthorized access and into policies
about the data itself, its transformation according to data class (see Chapter 7),
and the ability to prove that the policies set to access and transform the data over
time are being complied with. The correct implementation of security mechanics
promotes the trust required to share data broadly or “democratize access” to the
data.

8 | Chapter 1: What Is Data Governance?



Why Data Governance Is Becoming More Important
Data governance has been around since there was data to govern, although it was
often restricted to IT departments in regulated industries, and to security concerns
around specific datasets such as authentication credentials. Even legacy data process‐
ing systems needed a way to not only ensure data quality but also control access to
data.
Traditionally, data governance was viewed as an IT function that was performed in
silos related to data source type. For example, a company’s HR data and financial
data, typically highly controlled data sources with strictly controlled access and spe‐
cific usage guidelines, would be controlled by one IT silo, whereas sales data would be
in a different, less restrictive silo. Holistic or “centralized” data governance may have
existed within some organizations, but the majority of companies viewed data gover‐
nance as a departmental concern.
Data governance has come into prominence because of the recent introductions of
GDPR- and CCPA-type regulations that affect every industry, beyond just healthcare,
finance, and a few other regulated industries. There has also been a growing realiza‐
tion about the business value of data. Because of this, the data landscape is vastly dif‐
ferent today.
The following are just a few ways in which the topography has changed over time,
warranting very different approaches to and methods for data governance.

The Size of Data Is Growing
There is almost no limit to the kinds and amount of data that can now be collected. In
a whitepaper published in November 2018, International Data Corporation predicts
that the global datasphere will balloon to 175 ZB by 2025 (see Figure 1-4).4

This rise in data captured via technology, coupled with predictive analyses, results in
systems nearly knowing more about today’s users than the users themselves.

4 David Reinsel, John Gantz, and John Rydning, “The Digitization of the World: From Edge to Core”, Novem‐
ber 2018.

Why Data Governance Is Becoming More Important | 9



Figure 1-4. The size of the global datasphere is expected to exhibit dramatic growth

The Number of People Working and/or Viewing the Data Has Grown
Exponentially
A report by Indeed shows that the demand for data science jobs had jumped 78%
between 2015 and 2018.5 IDC also reports that there are now over five billion people
in the world interacting with data, and it projects this number to increase to six bil‐
lion (nearly 75% of the world’s population) in 2025. Companies are obsessed with
being able to make “data-driven decisions,” requiring an inordinate amount of head‐
count: from the engineers setting up data pipelines to analysts doing data curation
and analyses, and business stakeholders viewing dashboards and reports. The more
people working and viewing data, the greater the need for complex systems to man‐
age access, treatment, and usage of data because of the greater chance of misuse of the
data.

Methods of Data Collection Have Advanced
No longer must data only be batch processed and loaded for analysis. Companies are
leveraging real-time or near real-time streaming data and analytics to provide their
customers with better, more personalized engagements. Customers now expect to
access products and services wherever they are, over whatever connection they have,
and on any device. IDC predicts that this infusion of data into business workflows
and personal streams of life will result in nearly 30% of the global datasphere to be
real-time by 2025, as shown in Figure 1-5.6

5 “The Best Jobs in the US: 2019”, Indeed, March 19, 2019.
6 Reinsel et al. “The Digitization of the World.”

10 | Chapter 1: What Is Data Governance?



Figure 1-5. More than 25% of the global datasphere will be real-time data by 2025

The advent of streaming, however, while greatly increasing the speed to analytics, also
carries with it the potential risk of infiltration, bringing about the need for complex
setup and monitoring for protection.

Advanced Data Collection in Sports
It used to be that when you talked about sport statistics, you were talking about rela‐
tively coarse-grained data—things like wins and losses. In some sports, you might
have information about a player’s performance (for example, average number of runs
a cricketer gets per inning). However, the quantity and type of data collected in sports
has changed dramatically because teams are looking to better understand what levers
they can pull to be successful in these highly competitive fields.
It’s therefore no surprise that the National Football League (NFL) wanted to better
quantify the value of specific plays and actions within a play, which is why it started
using league-wide analytics in 2015. If you’re not familiar with American football, it is
a complex sport, primarily governed by the NFL. The NFL is a professional American
football league consisting of 32 teams that are divided equally between the National
Football Conference (NFC) and the American Football Conference (AFC).
Traditional metrics such as “yards per carry” or “total rushing yards” can be flawed;
recognizing a need to grow its analytics and data collection process, the NFL created 
Next Gen Stats (NGS). NGS is a league program wherein the pads of every player and
official, along with game balls, pylons, and first down chains are all tagged with a
radio frequency identification (RFID) chip, which allows for a very robust set of

Why Data Governance Is Becoming More Important | 11



statistics after every game. These statistics constitute real-time data for every player
on every play, in every situation, anywhere on the field, including location, speed,
acceleration, and velocity (Figure 1-6).

Figure 1-6. An example of analysis and visualization of the NFL data in Kaggle. Note‐
book by Rob Mulla

The types of questions the NFL wanted an answer to included, for example, “What
contributes to a successful run play?” It wanted to know whether success depends
mostly on the ball carrier who takes the hand off, or on their teammates (by way of
blocking), or on the coach (by way of the play call). And could the data even show
what part was played by the opposing defense and the actions it took? The NFL
would also want to predict how many yards a team might gain on given rushing plays

12 | Chapter 1: What Is Data Governance?



as they happen. Deeper insight into rushing plays ultimately helps teams, media, and
fans better understand the skillsets of players and the strategies of coaches.
Because of this, each year the league hosts the NFL’s Big Data Bowl, a sports analytics
contest that challenges talented members of the analytics community—from college
students to professionals—to contribute to the NFL’s continuing evolution in the use
of advanced analytics.7 Contestants are asked to analyze and rethink trends and player
performance in order to innovate on the way football is played and coached.
This NFL example showcases just how much the methods of data collection have
advanced. It’s no surprise that this is what’s really accelerating the amount of data
being generated in the global datasphere.

More Kinds of Data (Including More Sensitive Data) Are Now
Being Collected
It’s projected that by 2025 every person using technology and generating data will
have more than 4,900 digital data engagements per day; that’s about about one digital
interaction every eighteen seconds (see Figure 1-7).8

Figure 1-7. By 2025, a person will interact with data-creating technology more than
4,900 times a day

Many of those interactions will include the generation and resulting collection of a
myriad of sensitive data such as social security numbers, credit card numbers, names,
addresses, and health conditions, to name a few categories. The proliferation of the
collection of these extremely sensitive types of data carries with it great customer
(and regulator) concern about how that data is used and treated, and who gets to view
it.

7 Kaggle: NFL Big Data Bowl.
8 Reinsel et al. “The Digitization of the World.”

Why Data Governance Is Becoming More Important | 13



The Use Cases for Data Have Expanded
Companies are striving to use data to make better business decisions, coined data-
driven decision making. They not only are using data internally to drive day-to-day
business execution, but are also using data to help their customers make better deci‐
sions. Amazon is an example of a company doing this via collecting and analyzing
items in customers’ past purchases, items the customers have viewed, and items in
their virtual shopping carts, as well as the items they’ve ranked/reviewed after pur‐
chase, to drive targeted messaging and recommendations for future purchases.
While this Amazon use case makes perfect business sense, there are types of data
(sensitive) coupled with specific use cases for that data that are not appropriate (or
even legal). For sensitive types of data, it matters not only how that data is treated but
also how it’s used. For example, employee data may be used/viewed internally by a
company’s HR department, but it would not be appropriate for that data to be used/
viewed by the marketing department.

Use of Data to Make Better Decisions
Many of us enjoy the suggestions our friends and family make for books, movies,
music, and so on. Have you noticed how some of these suggestions pop up while you
are searching for something online? Here we would like to highlight a few examples
that show how use of data resulted in better decisions and better business outcomes.
An example from Safari Books Online shows the use of modern analytics tools to
achieve improved sales. Safari Books Online is known for its very large and diverse
customer base and for having more than 30,000 books and videos accessible from
various platforms. Safari Books Online wanted to unlock value from its massive
amounts of usage data, user search results, and trends, and to connect all this data to
drive better sales intelligence and higher sales.
The key was to achieve this in near real-time—after all, none of us likes to wait 10
minutes for the results of an online search. In order to provide real-time insights, the
Safari Books Online team routinely transferred the usage data corresponding to the
content delivery network (CDN) to a cloud native data warehouse to make the infor‐
mation available outside the original silo (see Figure 1-8).
The Safari Books Online team wanted to drill into the data, deliver various dash‐
boards, provide a better user experience, and deliver faster ad hoc queries. Using the
new analytics made supporting users much quicker and simpler and achieved higher
customer satisfaction. This was because the team could get to relevant information
about users (such as their IP addresses, or the title of a book they were querying) in
near real-time.

14 | Chapter 1: What Is Data Governance?



Figure 1-8. Bringing the usage data from content delivery network (CDN) and web
application logs into a smart data analytics platform to achieve better data-driven
decisions

Achieving better sales intelligence was among the most important use cases for Safari
Books Online when it began its journey into data-driven decision making. All the
data it had that was once buried, or not even available from its web logs, became sales
leads. Assessment of interest among likely readers was integrated into the CRM sys‐
tem and quickly turned into actionable information (see Figure 1-9).

Figure 1-9. A dashboard that is used to monitor titles and trends

Another good example is from California Design Den, which transformed its
decision-making process with data on pricing and inventory management. By leaning
on smart analytics platforms, they were able to achieve much faster pricing decisions,
sell their inventory, and achieve better profitability.
The ability to aggregate different types of data for decision making (while balancing
which data to retain and which to get rid of) is key. Not all data can be valuable for
better decision making. It is equally important to guard against biases when you are
trying to establish your data-driven decision-making process. You need to define your

Why Data Governance Is Becoming More Important | 15



objectives—maybe set some easy-to-measure goals at first—and create a list of high-
value questions to which you want to elicit answers. It is OK to go back and revisit
your starting point, objectives, and metrics. The answers are to be found in your data,
but looking at it from different perspectives will help you decide which data is
relevant.
The world is your oyster when it comes to gaining insight from your data. Asking the
high-value questions to obtain deeper insights is the essential part of the value chain
when it comes to data-driven decision making. Whether you want to drive sales intel‐
ligence and increase your revenue, improve support and customer experience, or
detect malicious use to prevent operational issues, data-driven decision making is key
for any business and any operation, and there are various smart tools out there to
help you start taking advantage of your invaluable data.

New Regulations and Laws Around the Treatment of Data
The increase in data and data availability has resulted in the desire and need for regu‐
lations on data, data collection, data access, and data use. Some regulations that have
been around for quite some time—for example, the Health Insurance Portability and
Accountability Act of 1996 (HIPAA), the law protecting the collection and use of per‐
sonal health data—not only are well known, but companies that have had to comply
with them have been doing so for decades—meaning their processes and methodol‐
ogy for treatment of this sensitive data are fairly sophisticated. New regulations, such
as the EU’s General Data Protection Regulation (GDPR) and the California Con‐
sumer Privacy Act (CCPA) in the US, are just two examples of usage and collection
controls that apply to myriad companies, for many of whom such governance of data
was not baked into their original data architecture strategy. Because of this, compa‐
nies that have not had to worry about regulatory compliance before have a more dif‐
ficult time modifying their technology and business processes to maintain
compliance with these new regulations.

Ethical Concerns Around the Use of Data
While use cases themselves can fit into the category of ethical use of data, new tech‐
nology around machine learning and artificial intelligence has spawned new concerns
around the ethical use of data.
One recent example from 2018 is that of Elaine Herzberg, who, while wheeling her
bike across a street in Tempe, Arizona, was struck and killed by a self-driving car.9

This incident raised questions about responsibility. Who was responsible for Elaine’s

9 Aarian Marshall and Alex Davies, “Uber’s Self-Driving Car Saw the Woman It Killed, Report Says”, Wired,
May 24, 2018.

16 | Chapter 1: What Is Data Governance?



death? The person in the driver’s seat? The company testing the car’s capabilities? The
designers of the AI system?
While not deadly, consider the following additional examples:

• In 2014, Amazon developed a recruiting tool for identifying software engineers it
might want to hire; however, it was found that the tool discriminated against
women. Amazon eventually had to abandon the tool in 2017.

• In 2016, ProPublica analyzed a commercially developed system that was created
to help judges make better sentencing decisions by predicting the likelihood that
criminals would reoffend, and it found that it was biased against Black people.10

Incidents such as these are enormous PR nightmares for companies.
Consequently, regulators have published guidelines on the ethical use of data. For
example, EU regulators published a set of seven requirements that must be met for AI
systems to be considered trustworthy:

• AI systems should be under human oversight.
• They need to have a fall-back plan in case something goes wrong. They also need

to be accurate, reliable, and reproducible.
• They must ensure full respect for privacy and data protection.
• Data, system, and AI business models should be transparent and offer

traceability.
• AI systems must avoid unfair bias.
• They must benefit all human beings.
• They must ensure responsibility and accountability.

However, the drive for data-driven decisions, fueled by more data and robust analyt‐
ics, calls for a necessary consideration of and focus on the ethics of data and data use
that goes beyond these regulatory requirements.

Examples of Data Governance in Action
This section takes a closer look at several enterprises and how they were able to derive
benefits from their governance efforts. These examples demonstrate that data gover‐
nance is being used to manage accessibility and security, that it addresses the issue of
trust by tackling data quality head-on, and that the governance structure makes these
endeavors successful.

10 Jonathan Shaw, “Artificial Intelligence and Ethics”, Harvard Magazine, January–February 2019, 44-49, 74.

Examples of Data Governance in Action | 17



Managing Discoverability, Security, and Accountability
In July 2019, Capital One, one of the largest issuers of consumer and small business
credit cards, discovered that an outsider had been able to take advantage of a miscon‐
figured web application firewall in its Apache web server. The attacker was able to
obtain temporary credentials and access files containing personal information for
Capital One customers.11 The resulting leak of information affected more than 100
million individuals who had applied for Capital One credit cards.
Two aspects of this leak limited the blast radius. First, the leak was of application data
sent to Capital One, and so, while the information included names, social security
numbers, bank account numbers, and addresses, it did not include log-in credentials
that would have allowed the attacker to steal money. Second, the attacker was swiftly
caught by the FBI, and the reason for the attacker being caught is why we include this
anecdote in this book.
Because the files in question were stored in a public cloud storage bucket where every
access to the files was logged, access logs were available to investigators after the fact.
They were able to figure out the IP routes and narrow down the source of the attack
to a few houses. While misconfigured IT systems that create security vulnerabilities
can happen anywhere, attackers who steal admin credentials from on-premises sys‐
tems will usually cover their tracks by modifying the system access logs. On the pub‐
lic cloud, though, these access logs are not modifiable because the attacker doesn’t
have access to them.
This incident highlights a handful of lessons:

• Make sure that your data collection is purposeful. In addition, store as narrow a
slice of the data as possible. It was fortunate that the data store of credit card
applications did not also include the details of the resulting credit card accounts.

• Turn on organizational-level audit logs in your data warehouse. Had this not
been done, it would not have been possible to catch the culprit.

• Conduct periodic security audits of all open ports. If this is not done, no alerts
will be raised about attempts to get past security safeguards.

• Apply an additional layer of security to sensitive data within documents. Social
security numbers, for example, should have been masked or tokenized using an
artificial intelligence service capable of identifying PII data and redacting it.

The fourth best practice is an additional safeguard—arguably, if only absolutely nec‐
essary data is collected and stored, there would be no need for masking. However,

11 “Information on the Capital One Cyber Incident”, Capital One, updated September 23, 2019; Brian Krebs,
“What We Can Learn from the Capital One Hack”, Krebs on Security (blog), August 2, 2019.

18 | Chapter 1: What Is Data Governance?



most organizations have multiple uses of the data, and in some use cases, the decryp‐
ted social security number might be needed. In order to do such multi-use effectively,
it is necessary to tag or label each attribute based on multiple categories to ensure the
appropriate controls and security are placed on it. This tends to be a collaborative
effort among many organizations within the company. It is worth noting that systems
like these that remove data from consideration come with their own challenges and
risks.12

As the data collected and retained by enterprises has grown, ensuring that best practi‐
ces like these are well understood and implemented correctly has become more and
more important. Such best practices and the policies and tools to implement them are
at the heart of data governance.

Improving Data Quality
Data governance is not just about security breaches. For data to be useful to an orga‐
nization, it is necessary that the data be trustworthy. The quality of data matters, and
much of data governance focuses on ensuring that the integrity of data can be trusted
by downstream applications. This is especially hard when data is not owned by your
organization and when that data is moving around.
A good example of data governance activities improving data quality comes from the
US Coast Guard (USCG). The USCG focuses on maritime search and rescue, ocean
spill cleanup, maritime safety, and law enforcement. Our colleague Dom Zippilli was
part of the team that proved the data governance concepts and techniques behind
what became known as the Authoritative Vessel Identification Service (AVIS). The
following sidebar about AVIS is in his words.

How the US Coast Guard Improved Data Quality
Dom Zippilli

Figure 1-10 illustrates what AVIS looked like when looking at a vessel with no data
discrepancies. The data from Automatic Identification Systems (AIS) corresponds
well with what was in AVIS, which is best described as “what we think we know”
about a ship—an amalgam of information from other USCG systems that handled
vessel registration, integration with the International Maritime Organization (IMO),
citations, and so on.

12 See, for example, the book Dark Data: Why What You Don’t Know Matters by David Hand (Princeton Univer‐
sity Press).

Examples of Data Governance in Action | 19



Figure 1-10. What AVIS looked like. Figure courtesy of NAVCEN.

Unfortunately, not all data corresponded this cleanly. Figure 1-11 illustrates a patho‐
logical case: no ship image, mismatched name, mismatched Maritime Mobile Service
Identity (MMSI), mismatched IMO number, mismatched everything.

Figure 1-11. A pathological case with a lot of inconsistencies between what’s tracked in
AIS and what’s known from an amalgam of other sources. Figure courtesy of NAVCEN.

Such mismatches make knowing which ships are where, and information about those
ships, much harder to figure out for USCG in the field. The vessels that cropped up in

20 | Chapter 1: What Is Data Governance?



the AVIS UI were the ones that couldn’t be resolved using automated tooling and thus
required some human intervention. Automating is nice (and this was almost 10 years
ago), but even surfacing the work that had to be done by a human was a huge step
forward. In almost all cases, the issues were the result of innocent mistakes, but get‐
ting things back on track required identifying the issues and reaching out to the mari‐
time community.
The business value of such corrections comes down to maritime domain awareness
(MDA), a key part of the USCG’s mission. Domain awareness is pretty hard to come
by when your data quality is poor. Here are some qualitative examples of how AVIS
helped.
For example, imagine a scenario in which a vessel needs to be investigated for some
kind of violation, or interdicted for any reason. If that vessel is among many broad‐
casting with the same MMSI number, our track for that vessel looks like Figure 1-12.
This could be even more serious in a search and rescue situation in which we need to
locate nearby vessels that could render aid faster than a USCG vessel (cooperation is a
major theme of maritime life).

Figure 1-12. Effects of duplicate vehicle id numbers. Figure courtesy of NAVCEN.

Over time, in the pilot program, as shown in Figure 1-13, we saw a drastic reduction
in the number of ambiguous vessel tracks received each day. While zero was always
the goal, this is by nature a community effort, so it requires constant upkeep.

Examples of Data Governance in Action | 21



Figure 1-13. Improvements in data quality due to pilot program to correct vessel IDs

The closest I have to a quantitative result (though it doesn’t spell out the mission
value exactly, as that was expected to be obvious to the reader) is this highlight from a
whitepaper that is unfortunately no longer available publicly:

Over the course of the project, the AVIS team was able to virtually eliminate uniden‐
tified and uncorrelated AIS vessel signals broadcasting unregistered MMSI numbers
such as 1, 2, 123456789, etc. Specifically, 863 out of 866 vessels were corrected by
September 2011, eliminating nearly 100% of incorrect broadcasts.13

863 might not seem like a lot, but keep in mind the global merchant fleet is something
on the order of 50,000 vessels. So, just for US waters, this is actually a big part of the
population, and as you know, it doesn’t take a lot of bad data to make all the data
useless.

The USCG program is a handy reminder that data quality is something to strive for
and constantly be on the watch for. The cleaner the data, the more likely it is to be
usable for more critical use cases. In the USCG case, we see this in the usability of the
data for search and rescue tasks as well.

13 David Winkler, “AIS Data Quality and the Authoritative Vessel Identification Service (AVIS)” (PowerPoint
presentation, National GMDSS Implementation Task Force, Arlington, VA, January 10, 2012).

22 | Chapter 1: What Is Data Governance?



The Business Value of Data Governance
Data governance is not solely a control practice. When implemented cohesively, data
governance addresses the strategic need to get knowledge workers the insights they
require with a clear process to “shop for data.” This makes possible the extraction of
insights from multiple sources that were previously siloed off within different busi‐
ness units.
In organizations where data governance is a strategic process, knowledge workers can
expect to easily find all the data required to fulfill their mission, safely apply for
access, and be granted access to the data under a simple process with clear timelines
and a transparent approval process. Approvers and governors of data can expect to
easily pull up a picture of what data is accessible to whom, and what data is “outside”
the governance zone of control (and what to do about any discrepancies there). CIOs
can expect to be able to review a high-level analysis of the data in the organization in
order to holistically review quantifiable metrics such as “total amount of data” or
“data out of compliance” and even understand (and mitigate) risks to the organiza‐
tion due to data leakage.

Fostering Innovation
A good data governance strategy, when set in motion, combines several factors that
allow a business to extract more value from the data. Whether the goal is to improve
operations, find additional sources of revenue, or even monetize data directly, a data
governance strategy is an enabler of various value drivers in enterprises.
A data governance strategy, if working well, is a combination of process (to make data
available under governance), people (who manage policies and usher in data access
across the organization, breaking silos where needed), and tools that facilitate the
above by applying machine learning techniques to categorize data and indexing the
data available for discovery.
Data governance ideally will allow all employees in the organization to access all data
(subject to a governance process) under a set of governance rules (defined in greater
detail below), while preserving the organization’s risk posture (i.e., no additional
exposure or risks are introduced due to making data accessible under a governance
strategy). Since the risk posture is maintained and possibly even improved with the
additional controls data governance brings, one could argue there is only an upside to
making data accessible. Giving all knowledge workers access to data, in a governed
manner, can foster innovation by allowing individuals to rapidly prototype answers
to questions based on the data that exists within the organization. This can lead to
better decision making, better opportunity discovery, and a more productive organi‐
zation overall.

The Business Value of Data Governance | 23



The quality of the data available is another way to ascertain whether governance is
well implemented in the organization. A part of data governance is a well-understood
way to codify and inherit a “quality signal” on the data. This signal should tell poten‐
tial data users and analysts whether the data was curated, whether it was normalized
or missing, whether corrupt data was removed, and potentially how trustworthy the
source for the data is. Quality signals are crucial when making decisions on potential
uses of the data; for example, within machine learning training datasets.

The Tension Between Data Governance and Democratizing
Data Analysis
Very often, complete data democratization is thought of as conflicting with data gov‐
ernance. This conflict is not necessarily an axiom. Data democratization, in its most
extreme interpretation, can mean that all analysts or knowledge workers can access
all data, whatever class it may belong to. The access described here makes a modern
organization uncomfortable when you consider specific examples, such as employee
data (e.g., salaries) and customer data (e.g., customer names and addresses). Clearly,
only specific people should be able to access data of the aforementioned types, and
they should do so only within their specific job-related responsibilities.
Data governance is actually an enabler here, solving this tension. The key concept to
keep in mind is that there are two layers to the data: the data itself (e.g., salaries) and
the metadata (data about the data—e.g., “I have a table that contains salaries, but I
won’t tell you anything further”).
With data governance, you can accomplish three things:

• Access a metadata catalog, which includes an index of all the data managed (full
democratization, in a way) and allows you to search for the existence of certain
data. A good data catalog also includes certain access control rules that limit the
bounds of the search (for example, I will be able to search “sales-related data,” but
“HR” is out of my purview completely, and therefore even HR-metadata is inac‐
cessible to me).

• Govern access to the data, which includes an acquisition process (described
above) and a way to adhere to the principle of least access: once access is reques‐
ted, provide access limited to the boundaries of the specific resource; don’t
overshare.

• Independently of the other steps, make an “audit trail” available to the data access
request, the data access approval cycle, and the approver (data steward), as well
as to all the subsequent access operations. This audit trail is data itself and there‐
fore must comply with data governance.

24 | Chapter 1: What Is Data Governance?



In a way, data governance becomes the facility where you can enable data democra‐
tization, allowing more of your data to be accessible to more of the knowledge
employee population, and therefore be an accelerator to the business in making the
use of data easier and faster.
Business outcomes, such as visibility into all parts of a supply chain, understanding of
customer behavior on every online asset, tracking the success of a multipronged cam‐
paign, and the resulting customer journeys, are becoming more and more possible.
Under governance, different business units will be able to pull data together, analyze
it to achieve deeper insight, and react quickly to both local and global changes.

Manage Risk (Theft, Misuse, Data Corruption)
The key concerns CIOs and responsible data stewards have had for a long time (and
this has not changed with the advent of big data analytics) have always been: What
are my risk factors, what is my mitigation plan, and what is the potential damage?
CIOs have been using these concerns to assign resources based on the answer to
those questions. Data governance comes to provide a set of tools, processes, and posi‐
tions for personnel to manage the risk to data, among other topics presented therein
(for example, data efficiency, or getting value from data). Those risks include:
Theft

Data theft is a concern in those organizations where data is either the product or
a key factor in generating value. Theft of data about parts, suppliers, or price in
an electronics manufacturer supply chain can cause a crippling blow to the busi‐
ness if competition uses that information to negotiate with those very same sup‐
pliers, or to derive a product roadmap from the supply-chain information. Theft
of a customer list can be very damaging to any organization. Setting data gover‐
nance around information that the organization considers to be sensitive can
encourage confidence in the sharing of surrounding data, aggregates, and so on,
contributing to business efficiency and breaking down barriers to sharing and
reusing data.

Misuse
Misuse is often the unknowing use of data in a way that’s different from the pur‐
pose it was collected for—sometimes to support the wrong conclusions. This is
often a result of a lack of information about the data source, its quality, or even
what it means. There is sometimes malicious misuse of data as well, meaning that
information gathered with consent for benign purposes is used for other unin‐
tended and sometimes nefarious purposes. An example is AT&T’s payout to the
FCC in 2015, after its call center employees were found to have disclosed con‐
sumers’ personal information to third parties for financial gain. Data governance
can protect against misuse with several layers. First, establish trust before sharing
data. Another way to protect against misuse is declarative—declare the source of

The Business Value of Data Governance | 25



the data within the container, the way it was collected, and what it was intended
for. Finally, limiting the length of time for which data is accessible can prevent
possible misuse. This does not mean placing a lid on the data and making it inac‐
cessible. Remember, the fact that the data exists should be shared alongside its
purpose and description—which should make data democratization a reality.

Data corruption
Data corruption is an insidious risk because it is hard to detect and hard to pro‐
tect against. The risk materializes when deriving operational business conclu‐
sions from corrupt (and therefore incorrect) data. Data corruption often occurs
outside of data governance control and can be due to errors on data ingest, join‐
ing “clean” data with corrupt data (creating a new, corrupt product). Partial data,
autocorrected to include some default values, can be misinterpeted, for example,
as curated data. Data governance can step into the fray here and allow recording,
even at the structured data column level, of the processes and lineage of the data,
and the level of confidence, or quality, of the top-level source of the data.

Regulatory Compliance
Data governance is often leveraged when a set of regulations are applicable to the
business, and specifically to the data the business processes. Regulations are, in
essence, policies that must be adhered to in order to play within the business environ‐
ment the organization operates in. GDPR is often referred to as an example regula‐
tion around data. This is because, among other things, GDPR mandates a separation
of (European citizens') personal data from other data, and treatment of that data in a
different way, especially around data that can be used to identify a person. This
manuscript does not intend to go into the specifics of GDPR.
Regulation will usually refer to one or more of the following specifics:

• Fine-grained access control
• Data retention and data deletion
• Audit logging
• Sensitive data classes

Let’s discuss these one by one.

Regulation around fine-grained access control
Access control is already an established topic that relates to security most of all. Fine-
grained access control adds the following considerations to access control:

26 | Chapter 1: What Is Data Governance?



When providing access, are you providing access to the right size of container?
This means making sure you provide the minimal size of the container of the
data (table, dataset, etc.) that includes the requested information. In structured
storage this will most commonly be a single table, rather than the whole dataset
or project-wide permission.

When providing access, are you providing the right level of access?
Different levels of access to data are possible. A common access pattern is being
able to either read the data or write the data, but there are additional levels: you
can choose to allow a contributor to append (but possibly not change) the data,
or an editor may have access to modify or even delete data. In addition, consider
protected systems in which some data is transformed on access. You could redact
certain columns (e.g., US social security numbers, which serve as a national ID)
to expose just the last four digits, or coarsen GPS coordinates to city and country.
A useful way to share data without exposing too much is to tokenize (encrypt)
the data with symmetric (reversible) encryption such that key data values (for
example, a person’s ID) preserve uniqueness (and thus you can count how many
distinct persons you have in your dataset) without being exposed to the specific
details of a person’s ID.
All the levels of access mentioned here should be considered (read/write/delete/
update and redact/mask/tokenize).

When providing access, for how long should access remain open?
Remember that access is usually requested for a reason (a specific project must be
completed), and permissions granted should not “dangle” without appropriate
justification. The regulator will be asking “who has access to what,” and thus lim‐
iting the number of personnel who have access to a certain class of data will make
sense and can prove efficient.

Data retention and data deletion
A significant body of regulation deals with the deletion and the preservation of data.
A requirement to preserve data for a set period, and no less than that period, is com‐
mon. For example, in the case of financial transaction regulations, it is not uncom‐
mon to find a requirement that all business transaction information be kept for a
duration of as much as seven years to allow financial fraud investigators to backtrack.
Conversely, an organization may want to limit the time it retains certain information,
allowing it to draw quick conclusions while limiting liability. For example, having
constantly up-to-date information about the location of all delivery trucks is useful
for making rapid decisions about “just-in-time” pickups and deliveries, but it
becomes a liability if you maintain that information over a period of time and can, in
theory, plot a picture of the location of a specific delivery driver over the course of
several weeks.

The Business Value of Data Governance | 27



Audit logging
Being able to bring up audit logs for a regulator is useful as evidence that policies are
complied with. You cannot present data that has been deleted, but you can show an
audit trail of the means by which the data was created, manipulated, shared (and with
whom), accessed (and by whom), and later expired or deleted. The auditor will be
able to verify that policies are being adhered to. Audit logs can serve as a useful foren‐
sic tool as well.
To be useful for data governance purposes, audit logs need to be immutable, write-
only (unchangeable by internal or external parties), and preserved, by themselves, for
a lengthy period—as long as the most demanding data preservation policy (and
beyond that, in order to show the data being deleted).
Audit logs need to include information not only about data and data operations by
themselves but also about operations that happen around the data management
facility. Policy changes need to be logged, and data schema changes need to be logged.
Permission management and permission changes need to be logged, and the logging
information should contain not only the subject of the change (be it a data container
or a person to be granted permission) but also the originator of the action (the
administrator or the service process that initiated the activity).

Sensitive data classes
Very often, a regulator will determine that a class of data should be treated differently
than other data. This is the heart of the regulation that is most commonly concerned
with a group of protected people, or a kind of activity. The regulator will be using
legal language (e.g., personally identifiable data about European Union residents, or
“financial transaction history”). It will be up to the organization to correctly identify
what portion of that data it actually processes, and how this data compares to the data
stored in structured or unstructured storage. For structured data it is sometimes eas‐
ier to bind a data class into a set of columns (PII is stored in these columns) and tag
the columns so that certain policies apply to these columns specifically, including
access and retention. This supports the principles of fine-grained access control as
well as adhering to the regulation about the data (not the data store or the personnel
manipulating that data).

Considerations for Organizations as They Think About Data
Governance
When an organization sits down and begins to define a data governance program and
the goals of such a program, it should take into consideration the environment in
which it operates. Specifically, it should consider what regulations are relevant and
how often these change, whether or not a cloud deployment makes sense for the

28 | Chapter 1: What Is Data Governance?



organization, and what expertise is required from IT and data analysts/owners. We
discuss these factors next.

Changing regulations and compliance needs
In past years, data governance regulations have garnered more attention. With GDPR
and CCPA joining the ranks of HIPAA- and PCI-related regulations, the affected
organizations are reacting.
The changing regulation environment has meant that organizations need to remain
vigilant when it comes to governance. No organization wants to be in the news for
getting sued for failing to handle customer information as per a set of regulations. In
a world where customer information is very precious, firms need to be careful how
they handle customer data. Not only should firms know about existing regulations,
but they also need to keep up with any changing mandates or stipulations, as well as
any new regulations that might affect how they do business. In addition, changes to
technology have also created additional challenges. Machine learning and AI have
allowed organizations to predict future outcomes and probabilities. These technolo‐
gies also create a ton of new datasets as a part of this process. With these new
predicted values, how do companies think about governance? Should these new data‐
sets assume the same policies and governance that the original datasets had, or
should they have their own set of policies for governance? Who should have access to
this data? How long should it be retained for? These are all questions that need to be
considered and answered.

Data accumulation and organization growth
With infrastructure cost rapidly decreasing, and organizations growing both organi‐
cally and through acquisition of additional business units (with their own data
stores), the topic of data accumulation, and how to properly react to quickly amassing
large amounts of data, becomes important. With data accumulation, an organization
is collecting more data from more sources and for more purposes.
Big data is a term you will keep hearing, and it alludes to the vast amounts of data
(structured and unstructured) now collected from connected devices, sensors, social
networks, clickstreams, and so on. The volume, variety, and velocity of data has
changed and accelerated over the past decade. The effort to manage and even consoli‐
date this data has created data swamps (disorganized and inconsistent collections of
data without clear curation) and even more silos—i.e., customers decided to consoli‐
date on System Applications and Products (SAP), and then they decided to consoli‐
date on Hive Metastore, and some consolidated on the cloud, and so on. Given these
challenges, knowing what you have and applying governance to this data is compli‐
cated, but it’s a task that organizations need to undertake. Organizations thought that
building a data lake would solve all their issues, but now these data lakes are becom‐
ing data swamps with so much data that is impossible to understand and govern. In

The Business Value of Data Governance | 29



an environment in which IDC predicts that more than a quarter of the data generated
by 2025 will be real-time in nature, how do organizations make sure that they are
ready for this changing paradigm?

Moving data to the cloud
Traditionally, all data resided in infrastructure provided and maintained by the orga‐
nization. This meant the organization had full control over access, and there was no
dynamic sharing of resources. With the emergence of cloud computing—which in
this context implies cheap but shared infrastructure—organizations need to think
about their response and investment in on-premises versus cloud infrastructure.
Many large enterprises still mention that they have no plans to move their core data,
or governed data, to the cloud anytime soon. Even though the largest cloud compa‐
nies have invested money and resources to protect customer data in the cloud, most
customers still feel the need to keep this data on-prem. This is understandable,
because data breaches in the cloud feel more consequential. The potential for damage,
monetary as well as to reputation, explains why enterprises want more transparency
in how governance works to protect their data on the cloud. With this pressure,
you’re seeing cloud companies put more guardrails in place. They need to “show” and
“open the hood” to how governance is being implemented, as well as provide controls
that not only engender trust among customers, but also put some power into custom‐
ers’ hands. We discuss these topics in Chapter 7.

Data infrastructure expertise
Another consideration for organizations is the sheer complexity of the infrastructure
landscape. How do you think about governance in a hybrid and multi-cloud world?
Hybrid computing allows organizations to have both on-premise and cloud infra‐
structure, while multicloud allows organizations to utilize more than one cloud pro‐
vider. How do you implement governance across the organization when the data
resides on-premises and on other clouds? This makes governance complicated and
therefore goes beyond the tools used to implement it. When organizations start
thinking about the people, the processes, and the tools and define a framework that
encompasses these facets, then it becomes a little easier to extend governance across
on-prem and in the cloud.

Why Data Governance Is Easier in the Public Cloud
Data governance involves managing risk. The practitioner is always trading off the
security inherent in never allowing access to the data against the agility that is possi‐
ble if data is readily available within the organization to support different types of
decisions and products. Regulatory compliance often dictates the minimal require‐
ments for access control, lineage, and retention policies. As we discussed in the

30 | Chapter 1: What Is Data Governance?



previous sections, the implementation of these can be challenging as a result of
changing regulations and organic growth.
The public cloud has several features that make data governance easier to implement,
monitor, and update. In many cases, these features are unavailable or cost-prohibitive
in on-premises systems.

Location
Data locality is mostly relevant for global organizations that store and use data across
the globe, but a deeper look into regulation reveals that the situation is not so simple.
For example, if, for business reasons, you want to leverage a data center in a central
location (say, in the US, next to your potential customers) but your company is a Ger‐
man company, regulation requires that data about employees remains on German
soil; thus your data strategy just became more involved.
The need to store user data within sovereign boundaries is an increasingly common
regulatory requirement. In 2016, the EU Parliament approved data sovereignty meas‐
ures within GDPR, wherein the storage and processing of records about EU citizens
and residents must be carried out in a manner that follows EU law. Specific classes of
data (e.g., health records in Australia, telecommunications metadata in Germany, or
payment data in India) may also be subject to data locality regulations; these go
beyond mere sovereignty measures by requiring that all data processing and storage
occur within the national boundaries. The major public cloud providers offer the
ability to store your data in accordance with these regulations. It can be convenient to
simply mark a dataset as being within the EU multi-region and know that you have
both redundancy (because it’s a multi-region) and compliance (because data never
leaves the EU). Implementing such a solution in your on-premises data center can be
quite difficult, since it can be cost-prohibitive to build data centers in every sovereign
location that you wish to do business in and that has locality regulations.
Another reason that location matters is that secure transaction-aware global access
matters. As your customers travel or locate their own operations, they will require
you to provide access to data and applications wherever they are. This can be difficult
if your regulatory compliance begins and ends with colocating applications and data
in regional silos. You need the ability to seamlessly apply compliance roles based on
users, not just on applications. Running your applications in a public cloud that runs
its own private fiber and offers end-to-end physical network security and global time
synchronization (not all clouds do this) simplifies the architecture of your
applications.

Why Data Governance Is Easier in the Public Cloud | 31



Reduced Surface Area
In heavily regulated industries, there are huge advantages if there is a single “golden”
source of truth for datasets, especially for data that requires auditability. Having your
enterprise data warehouse (EDW) in a public cloud, particularly in a setting in which
you can separate compute from storage and access the data from ephemeral clusters,
provides you with the ability to create different data marts for different use cases.
These data marts are provided data through views of the EDW that are created on the
fly. There is no need to maintain copies, and examination of the views is enough to
ensure auditability in terms of data correctness.
In turn, the lack of permanent storage in these data marts greatly simplifies their gov‐
ernance. Since there is no storage, complying with rules around data deletion is trivial
at the data mart level. All such rules have to be enforced only at the EDW. Other rules
around proper use and control of the data still have to be enforced, of course. That’s
why we think of this as a reduced surface area, not zero governance.

Ephemeral Compute
In order to have a single source of data and still be able to support enterprise applica‐
tions, current and future, we need to make sure that the data is not stored within a
compute cluster, or scaled in proportion to it. If our business is spiky, or if we require
the ability to support interactive or occasional workloads, we will require infinitely
scalable and readily burstable compute capability that is separate from storage archi‐
tecture. This is possible only if our data processing and analytics architecture is serv‐
erless and/or clearly separates computes and storage.
Why do we need both data processing and analytics to be serverless? Because the util‐
ity of data is often realized only after a series of preparation, cleanup, and intelligence
tools are applied to it. All these tools need to support separation of compute and stor‐
age and autoscaling in order to realize the benefits of a serverless analytics platform.
It is not sufficient just to have a serverless data warehouse or application architecture
that is built around serverless functions. You need your tooling frameworks them‐
selves to be serverless. This is available only in the cloud.

Serverless and Powerful
In many enterprises, lack of data is not the problem—it’s the availability of tools to
process data at scale. Google’s mission of organizing the world’s information has
meant that Google needed to invent data processing methods, including methods to
secure and govern the data being processed. Many of these research tools have been
hardened through production use at Google and are available on Google Cloud as
serverless tools (see Figure 1-14). Equivalents exist on other public clouds as well. For
example, the Aurora database on Amazon Web Services (AWS) and Microsoft’s Azure

32 | Chapter 1: What Is Data Governance?



Cosmos DB are serverless; S3 on AWS and Azure Cloud Storage are the equivalent of
Google Cloud Storage. Similarly, Lambda on AWS and Azure Functions provide the
ability to carry out stateless serverless data processing. Elastic Map Reduce (EMR) on
AWS and HDInsight on Azure are the equivalent of Google Cloud Dataproc. At the
time of writing, serverless stateful processing (Dataflow on Google Cloud) is not yet
available on other public clouds, but this will no doubt be remedied over time. These
sorts of capabilities are cost-prohibitive to implement on-premises because of the
necessity to implement serverless tools in an efficient manner while evening out the
load and traffic spikes across thousands of workloads.

Figure 1-14. Many of the data-processing techniques invented at Google (top panel; see
also http://research.google.com/pubs/papers.html) exist as managed services on Google
Cloud (bottom panel).

Labeled Resources
Public cloud providers provide granular resource labeling and tagging in order to
support a variety of billing considerations. For example, the organization that owns
the data in a data mart may not be the one carrying out (and therefore paying for) the
compute. This gives you the ability to implement regulatory compliance on top of the
sophisticated labeling and tagging features of these platforms.

Why Data Governance Is Easier in the Public Cloud | 33



These capabilities might include the ability to discover, label, and catalog items (ask
your cloud provider whether this is the case). It is important to be able to label
resources, not just in terms of identity and access management but also in terms of
attributes, such as whether a specific column is considered PII in certain jurisdic‐
tions. Then it is possible to apply consistent policies to all such fields everywhere in
your enterprise.

Security in a Hybrid World
The last point about having consistent policies that are easily applicable is key. Con‐
sistency and a single security pane are key benefits to hosting your enterprise soft‐
ware infrastructure on the cloud. However, such an all-or-nothing approach is
unrealistic for most enterprises. If your business operates equipment (handheld devi‐
ces, video cameras, point-of-sale registers, etc.) “on the edge,” it is often necessary to
have some of your software infrastructure there as well. Sometimes, as with voting
machines, regulatory compliance might require physical control of the equipment
being used. Your legacy systems may not be ready to take advantage of the separation
of compute and storage that the cloud offers. In these cases, you’d like to continue to
operate on-premises. Systems that involve components that live in a public cloud and
one other place—in two public clouds, or in a public cloud and on the edge, or a in
public cloud and on-premises—are termed hybrid cloud systems.
It is possible to greatly expand the purview of your cloud security posture and poli‐
cies by employing solutions that allow you to control both on-premises and cloud
infrastructure using the same tooling. For example, if you have audited an on-
premises application and its use of data, it is easier to approve that identical applica‐
tion running in the cloud than it is to reaudit a rewritten application. The cost of
entry to this capability is to containerize your applications, and this might be a cost
well worth paying for, for the governance benefits alone.

Summary
When discussing a successful data-governance strategy, you must consider more than
just the data architecture/data pipeline structure or the tools that perform “gover‐
nance” tasks. Consideration of the actual humans behind the governance tools as well
as the “people processes” put into place is also highly important and should not be
discounted. A truly successful governance strategy must address not only the tools
involved but the people and processes as well. In Chapters 2 and 3, we will discuss
these ingredients of data governance.
In Chapter 4, we take an example corpus of data and consider how data governance is
carried out over the entire life cycle of that data; from ingest to preparation and stor‐
age, to incorporation into reports, dashboards, and machine learning models, and on
to updates and eventual deletion. A key concern here is that data quality is an

34 | Chapter 1: What Is Data Governance?



ongoing concern; new data-processing methods are invented, and business rules
change. How to handle the ongoing improvement of data quality is addressed in
Chapter 5.
By 2025, more than 25% of enterprise data is expected to be streaming data. In Chap‐
ter 6, we address the challenges of governing data that is on the move. Data in flight
involves governing data at the source and at the destination, and any aggregations
and manipulations that are carried in flight. Data governance also has to address the
challenges of late-arriving data and what it means for the correctness of calculations if
storage systems are only eventually correct.
In Chapter 7, we delve into data protection and the solutions available for authentica‐
tion, security, backup, and so on. The best data governance is of no use if monitoring
is not carried out and leaks, misuse, and accidents are not discovered early enough to
be mitigated. Monitoring is covered in Chapter 8.
Finally, in Chapter 9, we bring together the topics in this book and cover best practi‐
ces in building a data culture—a culture in which both the user and the opportunity
is respected.
One question we often get asked is how Google does data governance internally. In
Appendix A, we use Google as an example (one that we know well) of a data gover‐
nance system, and point out the benefits and challenges of the approaches that Goo‐
gle takes and the ingredients that make it all possible.

Summary | 35






CHAPTER 2
Ingredients of Data Governance: Tools

A lot of the tasks related to data governance can benefit from automation. Machine
learning tools and automatic policy applications or suggestions can accelerate data
governance tasks. In this chapter we will review some of the tools commonly referred
to when discussing data governance.
When evaluating a data governance process/system, pay attention to the capabilities
mentioned in this chapter. The following discussion concerns tasks and tools that can
augment complete end-to-end support for the processes involved in, and the person‐
nel responsible for, data governance organization. We’ll dive into the various pro‐
cesses and solutions in more detail in later chapters.

The Enterprise Dictionary
To begin, it is important to understand how an organization works with data and
enables data governance. Usually, there is an enterprise dictionary or an enterprise
policy book of some kind.
The first of these documents, the enterprise dictionary, is one that can take many
shapes, from a paper document to a tool that encodes or automates certain policies. It
is an agreed-upon repository of the information types (infotypes) used by the organi‐
zation—that is, data elements that the organization processes and derives insights
from. An infotype will be a piece of information with a singular meaning—“email
address” or “street address,” for example, or even “salary amount.”
In order to refer to individual fields of information and drive a governance policy
accordingly, you need to name those pieces of information.

37



An organization’s enterprise dictionary is normally owned by either the legal depart‐
ment (whose focus would be compliance) or the data office (whose focus would be
standardization of the data elements used).
In Figure 2-1, you can see examples of infotypes, and a potential organization of
those infotypes into organization-specific data classes to be used in policy making.

Figure 2-1. Data classes and infotypes

Once the enterprise dictionary is defined, the various individual infotypes within it
can be grouped into data classes, and a policy can be defined for each data class. The
enterprise dictionary generally contains data classes, policies related to the data
classes, and additional metadata. The following sections expand on this.

Data Classes
A good enterprise dictionary will contain a listing of the classes of data the organiza‐
tion processes. Those will be infotypes collected into groups that are treated in a com‐
mon way from the policy management aspect. For example, an organization will not
want to treat “street addresses,” “phone numbers,” “city, state,” and “zip code” differ‐
ently in a granular manner but must rather be able to set a policy that “all location
information for consumers must be accessible only to a privileged group of personnel
and be kept only for a maximum of 30 days.” This means that the enterprise dictio‐
nary we’ve just described, will actually contain a hierarchy of infotypes—at the leaf
nodes there will be the individual infotypes (e.g., “address,” “email”), and at the root
nodes you will find a data class, or a sensitivity classification (or sometimes both).
Figure 2-2 shows an example of such a hierarchy from a fictional organization.

38 | Chapter 2: Ingredients of Data Governance: Tools



Figure 2-2. A data class hierarchy

In the data class hierarchy detailed in Figure 2-2, you can see how infotypes such as
IMEI (cellular device hardware ID), phone number, and IP address are grouped
together under personally identifiable information (PII). For this organization, these
are easily identifiable automatically, and policies are defined on “all PII data ele‐
ments.” PII is paired with PHI (protected health information) in the “restricted data”
category. It is likely that there are further policies defined on all data grouped under
the “restricted” heading.
Data classes are usually maintained by a central body within the organization,
because policies on “types of data classes” usually affect compliance to regulation.
Some example data classes seen across many organizations are:

The Enterprise Dictionary | 39



PII
This is data such as name, address, and personal phone number that can be used
to uniquely identify a person. For a retailer, this can be a customer list. Other
examples include lists of employee data, a list of third-party vendors, and similar
information.

Financial information
This is data such as transactions, salaries, benefits, or any kind of data that can
include information of financial value.

Business intellectual property
This is information related to the success and differentiation of the business.

The variety and kind of data classes will change with the business vertical and inter‐
est. The preceding example data classes (PII, financial, business intellectual property)
will not work for every organization, and the meaning of data collected and the clas‐
sification will differ between businesses and between countries. Do note that data
classes are a combination of information elements belonging to one topic. For exam‐
ple, a phone number is usually not a data class, but PII (of which phone number is a
member) is normally a data class.
The characteristics of a data class are twofold:

• A data class references a set of policies: the same retention rules and access rules
are required on this data.

• A data class references a set of individual infotypes, as in the example in
Figure 2-1.

Data Classes and Policies
Once the data the organization handles is defined in an enterprise dictionary, policies
that govern the data classes can be assigned to data across multiple containers. The
desired relationship is between a policy (e.g., access control, retention) and a data class
rather than a policy and an individual container. Associating policies with the mean‐
ing of data allows for better human understanding (“Analysts cannot access PII” ver‐
sus “Analysts cannot access column #3 on Table B”) and supports scaling to larger
amounts of data.
Above, we have discussed the relationship between data classes and policies. Fre‐
quently, along with the data class specification, the central data office, or legal, will
define an enterprise policy book. An organization needs to be able to answer the
question, “What kinds of data do we process?” An enterprise policy book records
that. It specifies the data classes the organization uses, the kinds of data that are

40 | Chapter 2: Ingredients of Data Governance: Tools



processed, and how they are processed, and it elaborates on “what are we allowed and
not allowed to do” with the data. This is a crucial element in the following respects:

• For compliance, the organization needs to be able to prove to a regulator that it
has the right policies in place around the handling of data.

• A regulator will require the organization to submit the policy book, as well as
proof (usually from audit logs) of compliance with the policies.

• The regulator will require evidence of procedures to ensure that the policy book
is enforced and may even comment on the policies themselves.

We’d like to stress the importance of having a well-thought-out and
well-documented enterprise policy book. Not only is it incredibly
helpful for understanding, organizing, and enforcing your policies,
but it also enables you to quickly and effectively react to changing
requirements and regulations. Additionally, it should be noted that
having the ability to quickly and easily provide documentation and
proof of compliance—not only for external audits but also for
internal ones—should not be discounted. Knowing at a glance how
your company is doing with its policies and its management of
those policies is critical for ensuring a successful and comprehen‐
sive governance program. During the course of our research and
interviews, many companies expressed their struggles with being
able to conduct quick internal audits to know how their gover‐
nance strategy was going. This often resulted in much time and
effort being spent backtracking and documenting policies, how
they were enforced, on what data they were attached, and so on.
Creating an enterprise policy book and setting yourself up to more
easily audit internally (and thus externally) will help you avoid this
pain.

To limit liability, risk management, and exposure to legal action, an organization will
usually define a maximum (and a minimum) retention rate for data. This is impor‐
tant because during an investigation, certain law enforcement agencies will require
certain kinds of data, which the organization must therefore be able to supply. In the
case of financial institutions, for example, it is common to find requirements for
holding certain kinds of data (e.g., transactions) for a minimum of seven years. Other
kinds of data pose a liability: you cannot leak or lose control of data that you don’t
have.
Another kind of policy will be access control. For data, access control goes beyond
“yes/no” and into no access, partial access, or full access. Partial access is accessing the
data when some bits have been “starred out,” or accessing the data after a determinis‐
tic encryption transformation, which will still allow acting on distinct values, or

The Enterprise Dictionary | 41



grouping by these values, without being exposed to the underlying cleartext. You can
think of partial access as a spectrum of access, ranging from zero access to
ever-increasing details about the data in question (format only, number of digits only,
tokenized rendition, to full access). See Figure 2-3.

Figure 2-3. Examples of varying levels of access for sensitive data

Normally, a policy book will specify:

• Who (inside or outside the organization) can access a data class
• The retention policy for the data class (how long data is preserved)
• Data residency/locality rules, if applicable
• How the data can be processed (OK/Not OK for analytics, machine learning,

etc.)
• Other considerations by the organization

The enterprise policy book—and, with it, the enterprise dictionary—describe the data
managed by the organization. Now let’s discuss specific tools and functionality that
can accelerate data governance work and optimize personnel time.

Per-Use-Case Data Policies
Data can have different meanings and require different policies when the data use
case is taken into consideration. An illustrative example might be a furniture manu‐
facturer that collects personal data (names, addresses, contact numbers) in order to
ensure delivery. The very same data can potentially be used for marketing purposes,
but consent, in most cases, probably has not been granted for the purpose of market‐
ing. However, at the same time, I would very much like that sofa to be delivered to my
home, so I want the manufacturer to store the data! The use case, or purpose, of the
data access ideally should be an overlay on top of your organizational membership
and organizational roles. One way to think about this would be as a “window”
through which the analyst can select data, specifying the purpose ahead of time, and
potentially moving the data into a different container for that purpose (the marketing

42 | Chapter 2: Ingredients of Data Governance: Tools



database, for example), all with an audit artifact and lineage tracking that will be used
for tracking purposes.

Importance of Use Case and Policy Management
Increasingly, as requirements and regulations change to accommodate the new and
different types of data that are collected, the use case of data becomes an important
aspect of policy management. One of the things that we’ve heard companies say over
and over is that the use case of data needs to be a part of policies—the simple “data
class to role-based access type” isn’t sufficient. We’ve given the example of a furniture
manufacturer and access to, and usage of, customer address for delivery versus mar‐
keting purposes. Another example we’ve heard about throughout our research is that
of a company that has employees who may perform multiple roles. In this case, a sim‐
ple role-based access policy is inadequate, because these employees may be allowed
access to a particular set of data for performing tasks relating to one role but may not
have access for tasks relating to a different role. From this you can see how it’s more
efficient (and effective) to consider access related to what the data will be used for—its
use case—rather than only considering infotype/data class and employee role.

Data Classification and Organization
To control the governance of data, it is beneficial to automate, at least in part, the
classification of data into, at the very least, infotypes—although an even greater auto‐
mation is sometimes adopted. A data classifier will look at unstructured data, or even
a set of columns in structured data, and infer “what” the data is—e.g., it will identify
various representations of phone numbers, bank accounts, addresses, location indica‐
tors, and more.
An example classifier would be Google’s Cloud Data Loss Prevention (DLP). Another
classifier is Amazon’s Macie service.
Automation of data classification can be accomplished in two main ways:

• Identify data classes on ingest, triggering a classification job on the addition of
data sources

• Trigger a data-classification job periodically, reviewing samples of your data

When it is possible, identifying new data sources and classifying them as they are
added to the data warehouse is most efficient, but sometimes this is not possible with
legacy or federated data.
Upon classifying data, you can do the following, depending on the desired level of
automation:

The Enterprise Dictionary | 43



• Tag the data as “belonging to a class” (see “The Enterprise Dictionary” on page
37)

• Automatically (or manually) apply policies that control access to and retention of
the data according to the definition of the data class, “purpose,” or context for
which the data is accessed or manipulated

Data Cataloging and Metadata Management
When talking about data, data classification, and data classes, we need to discuss the
metadata, or the “information about information”—specifically, where it’s stored and
what governance controls there are on it. It would be naive to think that metadata
obeys the same policies and controls as the underlying data itself. There are many
cases, in fact, where this can be a hindrance. Consider, for example, searching in a
metadata catalog for a specific table containing customer names. While you may not
have access to the table itself, knowing such a table exists is valuable (you can then
request access, you can attempt to review the schema and figure out if this table is
relevant, and you can avoid creating another iteration of this information if it already
exists). Another example is data residency–sensitive information, which must not
leave a certain national border, but at the same time that restriction does not neces‐
sarily apply to information about the existence of the data itself, which may be rele‐
vant in a global search. A final example is information about a listing of phone calls
(who called whom, from where, and when), which can be potentially more sensitive
than the actual calls themselves, because a call list places certain people at certain
locations at certain times.
Crucial to metadata management is a data catalog, a tool to manage this metadata.
Whereas enterprise data warehouses, such as Google BigQuery, are efficient at pro‐
cessing data, you probably want a tool that spans multiple storage systems to hold the
information about the data. This includes where the data is and what technical infor‐
mation is associated with it (table schema, table name, column name, column
description)—but you also should allow for the attachment of additional “business”
metadata, such as who in the organization owns the data, whether the data is locally
generated or externally purchased, whether it relates to production use cases or test‐
ing, and so on.
As your data governance strategy grows, you will want to attach the particulars of
data governance information to the data in a data catalog: data class, data quality,
sensitivity, and so on. It is useful to have these dimensions of information schema‐
tized, so that you can run a faceted search like “Show me all data of type:table and
class:X in the “production” environment”.

44 | Chapter 2: Ingredients of Data Governance: Tools



The data catalog clearly needs to efficiently index all this information and must be
able to present it to the users whose permissions allow it, using high-performing
search and discovery tooling.

Data Assessment and Profiling
A key step in most insight generation workflows, as you sift through data, is to review
that data for outliers. There are many possible reasons for outliers, and best practice
is to review before making sweeping/automated decisions. Outliers can be the result
of data-entry errors or may just be inconsistent with the rest of the data, but they can
also be weak signals or less-represented new segments or patterns. In many cases, you
will need to normalize the data for the general case before driving insights. This nor‐
malization (keeping or removing outliers) should be done in the context of the busi‐
ness purpose the data is being used for—for example, if you are looking for unusual
patterns or not.
The reason for normalizing data is to ensure data quality and consistency (sometimes
data entry errors lead to data inconsistencies). Again, this must be done in the con‐
text of the business purpose of the data use case. Data quality cannot be performed
without a directing business case, because “reviewing transactions” is not the same
for a marketing team (which is looking for big/influential customers) and a fraud
analysis team (which is looking for provable indications of fraud).
Note that machine learning models, for example, are susceptible to arriving at the
wrong conclusions by extracting generalizations from erroneous data. This is also
true for many other types of analytics.
Data engineers are usually responsible for producing a report that contains data outli‐
ers and other suspected quality issues. Unless the data errors are obviously caused by
data entry or data processing, the data errors are fixed/cleansed by data analysts who
are part of the organization that owns/produces the data, as previously mentioned.
This is done in light of the expected use of the data. However, the data engineers can
leverage a programmatic approach in fixing the data errors, as per the data owners’
requests and requirements. The data engineers will look for empty fields, out-of-
bounds values (e.g., people of ages over 200 or under 0), or just plain errors (a string
where a number is expected). There are tools to easily review a sample of the data and
make the cleanup process easier, for example, Dataprep by Trifacta and Stitch.
These cleanup processes work to ensure that using the data in applications, such as
generating a machine learning model, does not result in it being skewed by data outli‐
ers. Ideally, data should be profiled in order to detect anomalies per column and
make a determination on whether anomalies are making sense in the relevant context
(e.g., customers shopping in a physical store outside of store hours are probably an
error, while late-night online ordering is very much a reality). The bounds for what

The Enterprise Dictionary | 45



kinds of data are acceptable for each field are set, and automated rules prepare and
clean up any batch of data, or any event stream, for ingestion. Care must be taken to
avoid introducing bias into the data, such as by eliminating outliers where they
should not be eliminated.

Data Quality
Data quality is an important parameter in determining the relevant use cases for a
data source, as is being able to rely on data for further calculations/inclusions with
other datasets. You can identify data quality by looking at the data source—i.e.,
understanding where it physically came from. (Error-prone human entry? Fuzzy IoT
devices optimizing for quantity, not quality? Highly exact mobile app event stream?)
Knowing the quality of data sources should guide the decision of whether to join
datasets of varying quality, because low-quality data will reduce confidence in higher-
quality sources. Data quality management processes include creating controls for val‐
idation, enabling quality monitoring and reporting, supporting the triage process for
assessing the level of incident severity, enabling root cause analysis and recommenda‐
tion of remedies to data issues, and data incident tracking.
There should be different confidence levels assigned to different quality datasets.
There should also be considerations around allowing (or at least curating) resultant
datasets with mixed-quality ancestors. The right processes for data quality manage‐
ment will provide measurably trustworthy data for analysis.
One possible process that can be implemented to improve data quality is a sense of
ownership: making sure the business unit responsible for generating the data also
owns the quality of that data and does not leave it behind for users downstream. The
organization can create a data acceptance process wherein data is not allowed to be
used until the owners of the data prove the data is of a quality that passes the organi‐
zation’s quality standards.

Lineage Tracking
Data does not live in a vacuum; it is generated by certain sources, undergoes various
transformations, aggregates additionals, and is eventually supporting certain insights.
A lot of valuable context is generated from the source of the data and how it was
manipulated along the way, which is crucial to track. This is data lineage.
Why is lineage tracking important? One reason is understanding the quality of a
resulting dashboard/aggregate. If that end product was generated from high-quality
data, but later the information is merged into lower-quality data, that leads to a differ‐
ent interpretation of the dashboard. Another example will be viewing, in a holistic

46 | Chapter 2: Ingredients of Data Governance: Tools



manner, the movement of a sensitive data class across the organization data scape, to
make sure sensitive data is not inadvertently exposed into unauthorized containers.
Lineage tracking should be able, first and foremost, to present a calculation on the
resultant metrics, such as “quality,” or on whether or not the data was “tainted” with
sensitive information. And later it must be able to present a graphical “graph” of the
data traversal itself. This graph is very useful for debugging purposes but is less so for
other purposes.

Lineage Tracking and Time/Cost
When describing how lineage tracking—especially doing so visually—is so important,
companies have often referred to the level of effort they have to put into debugging
and troubleshooting. They have stated how much time is spent not on fixing prob‐
lems or errors but just on trying to find out if there are any errors or problems at all.
We have heard time and time again that better tracking (e.g., notifications about
what’s going on and when things are “on fire” and should be looked at) not only
would help solve issues better but also would save valuable time—and thus expense—
in the long run. Often when lineage is talked about, the focus is on knowing where
data came from and where it’s going to, but there is also value in visually seeing/
knowing when and where something breaks and being able to take immediate action
on it.

Lineage tracking is also important when thinking about explaining decisions later on.
By identifying input information into a decision-making algorithm (think about a
neural net, or a machine learning model), you can rationalize later why some busi‐
ness decisions (e.g., loan approval) were made in a certain way in the past and will be
made in a certain way in the future. By making business decisions explainable (past
transactions explaining a current decision) and keeping this information transparent
to the data users, you practice good data governance.
This also brings up the importance of the temporal dimension of lineage. The more
sophisticated solutions track lineage across time—tracking not only what the current
inputs to a dashboard are but also what those inputs were in the past, and how the
landscape has evolved.

Key Management and Encryption
One consideration when storing data in any kind of system is whether to store it in a
plain-text format or whether to encrypt it. Data encryption provides another layer of
protection (beyond protecting all data traffic itself), as only the systems or users that

The Enterprise Dictionary | 47



have the keys can derive meaning from the data. There are several implementations
of data encryptions:

• Data encryption where the underlying storage can access the key. This allows the
underlying storage system to effect efficient storage via data compression
(encrypted data usually does not compress well). When the data is accessed out‐
side the bounds of the storage system—for example, if a physical disk is taken out
of a data center—the data should be unreadable and therefore secure.

• Data encryption where the data is encrypted by a key inaccessible to the storage
system, usually managed separately by the customer. This provides protection
from a bad actor within the storage provider itself in some cases but results in
inefficient storage and performance impact.

• Just-in-time decryption, where, in some cases and for some users, it is useful to
decrypt certain data as it is being accessed as a form of access control. In this
case, encryption works to protect some data classes (e.g., “customer name”) while
still allowing insights such as “total aggregate revenues from all customers” or
“top 10 customers by revenue,” or even identifying subjects who meet some con‐
dition, with the option to ask for de-masking of these subjects later via a trouble
ticket.

All data in Google Cloud is encrypted by default, both in transit and at rest, ensuring
that customer data is always protected from intrusions and attacks. Customers can
also choose customer-managed encryption keys (CMEK) using Cloud KMS, or they
can opt for customer-supplied encryption keys (CSEK) when they need more control
over their data.
To provide the strongest protections, your encryption options should be native to the
cloud platform or data warehouse you choose. The big cloud platforms all have a
native key management that usually allows you to perform operations on keys
without revealing the actual keys. In this case, there are actually two keys in play:
A data encryption key (DEK)

This key is used to directly encrypt the data by the storage system.
A key encryption key (KEK)

This key is used to protect the data encryption key and resides within a protected
service, a key management service.

48 | Chapter 2: Ingredients of Data Governance: Tools



A Sample Key Management Scenario
In the scenario depicted in Figure 2-4, the table on the right is encrypted in chunks
using the plain data encryption key.1 The data encryption key is not stored with the
table but instead is stored in a protected form (wrapped) by a striped key encryption
key. The key encryption key resides (only) in the key management service.

Figure 2-4. Key management scenario

To access the data, a user (or process) follows these steps:

1. The user/process requests the data, instructing the data warehouse (BigQuery) to
use the “striped key” to unwrap the data encryption key, basically passing the key
ID.

2. BigQuery retrieves the protected DEK from the table metadata and accesses the
key management service, supplying the wrapped key.

3. The key management service unwraps the data protection key, while the KEK
never leaves the vault of the key management service.

4. BigQuery uses the DEK to access the data and then discards it, never storing it in
a persistent manner.

This scenario ensures that the key encryption key never leaves a secure separate store
(the KMS) and that the data encryption key never resides on disk—only in memory,
and only when needed.

1 Protection of data at rest is a broad topic; a good starter book would be Applied Cryptography by Bruce Schne‐
ier (John Wiley and Sons).

The Enterprise Dictionary | 49



Data Retention and Data Deletion
Another important item in the data governance tool chest is the ability to control
how long data is kept—that is, setting maximal and minimal values. There are clear
advantages to identifying data that should survive occasional storage space optimiza‐
tion as being more valuable to retain, but setting a maximum amount of time on data
retention for a less-valuable data class and automatically deleting it seems less obvi‐
ous. Consider that retaining PII presents the challenges of proposer disclosure,
informed consent, and transparency. Getting rid of PII after a short duration (e.g.,
retaining location only while on the commute) simplifies the above.
When talking about data retention and data deletion, we’re often thinking about them
in the context of how to treat sensitive data—that is, whether to retain it, encrypt it,
delete it, or handle it some other way. But there are also scenarios around which your
governance policies not only can save you from being out of compliance but also can
protect you from lost work.
Although it’s a bit old, an example that comes to mind around the subject of protec‐
tion against data deletion is the accidental deletion of the movie Toy Story 2 in 1998.2

During the film’s production process, Oren Jacob, Pixar’s CTO, and Technical Direc‐
tor Galyn Susman were looking at a directory that was holding the assets for several
characters when they encountered an error—something along the lines of “directory
no longer valid,” meaning the location of those characters in the directory had been
deleted. During their effort to walk back through the directory to find where the
problem occurred, they witnessed, in real time, assets for several of the characters
vanish before their eyes.
When all was said and done, an erroneous 10-character command had mistakenly
deleted 90% of the movie. They were able to recover most of the movie, but unfortu‐
nately about a week’s worth of work was lost forever.
You can see from this example that even though sensitive data wasn’t leaked or lost, a
multitude of data was still accidentally deleted—some of which could never be recov‐
ered. We challenge you to consider in your governance program not only how you
will deal with and treat sensitive data in terms of where or how long you retain it, and
whether or not you delete it, but also how that same program can be implemented on
other classes and categories of data that are important for you to back up. While a loss
of data may not result in a breach in compliance, it could certainly result in other
catastrophic business consequences that, if planned for, will hopefully not happen to
you.

2 Dan Moshkovich, “Accidental Data Deletion: The Cautionary Tale of Toy Story 2 and MySpace”, HubStor
(blog), August 17, 2020.

50 | Chapter 2: Ingredients of Data Governance: Tools



Case Study: The Importance of Clear Data Retention Rules for
Data Governance

A lesson about data retention was learned by a Danish taxi company that actually had
a data governance policy in place.3

To understand the story, we need a little bit of context about GDPR Article 5, the reg‐
ulation covering treatment of European citizens’ data by tech companies. GDPR
details the standards that organizations have to follow when processing personal data.
These standards state that data must be handled in a way that is transparent to its sub‐
ject (the European citizen), collected for a specific purpose, and used only for that
purpose. GDPR Article 5(1)(c) addresses data minimization by requiring that per‐
sonal data be limited to what is necessary relative to the purpose for which it is pro‐
cessed. And Article 5(1)(e)—the one that’s most relevant to the taxi company example
—specifies that data cannot be stored any longer than is necessary for the purposes
for which it was gathered.
The Danish taxi company in this example (Figure 2-5) processed taxi ridership data
for legitimate purposes, making sure there was a record of every ride, and of the asso‐
ciated fare, for example, for chargeback and accounting purposes.

Figure 2-5. Danish taxis (photo courtesy of Håkan Dahlström, Creative Commons
License 2.0

The Danish taxi company, as mentioned, had a data retention policy in place: after
two years, the data from a taxi ride was made anonymous by deleting the name of the
passenger. However, that action did not make the data completely anonymous, as mul‐
tiple additional details were (albeit transparently) collected. These included the geolo‐
cation of the taxi ride’s start and end and the phone number of the passenger. With
these details, even without the name of the passenger, it was easy to identify the

3 For more about this case study, see coverage by the law firm Gorrissen Federspiel.

The Enterprise Dictionary | 51



passenger, and therefore the company was not in compliance with its own statement
of anonymization; thus the data retention policy was actually not effective.
The lesson learned is that considering the business goal of a data retention policy, and
whether or not that goal is actually achieved by the policy set in place, is essential. In
our case, the taxi company was fined by the European Union and was criticized for
the fact that telephone numbers are actually used as unique identifiers within the taxi
ride management system.

Workflow Management for Data Acquisition
One of the key workflows tying together all the tools mentioned so far is data acquisi‐
tion. This workflow usually begins with an analyst seeking data to perform a task.
The analyst, through the power of a well-implemented data governance plan, is able
to access the data catalog for the organization and, through a multifaceted search
query, is able to review relevant data sources. Data acquisition continues with identi‐
fying the relevant data source and seeking an access grant to it. The governance con‐
trols send the access request to the right authorizing personnel, and access is granted
to the relevant data warehouse, enforced through the native controls of that ware‐
house. This workflow—identifying a task, shopping for relevant data, identifying rele‐
vant data, and acquiring access to it—constitutes a data access workflow that is safe.
The level of data access requested—that is, access to metadata for search, access to the
data itself, querying the data in aggregate—these are all data governance decisions.

IAM—Identity and Access Management
When talking about data acquisition, it’s important to detail how access control
works. The topic of access control relies on user authentication and, per the user, the
authorization of the user to access certain data and the conditions of access.
The objective of authenticating a user is to determine that “you are who you say you
are.” Any user (and, for that matter, any service or application) operates under a set of
permissions and roles tied to the identity of a service. The importance of securely
authenticating a user is clear: if I can impersonate a different user, there is a risk of
assuming that user’s roles and privileges and breaking data governance.
Authentication used to be traditionally accomplished by supplying a password tied to
the user requesting access. This method has the obvious drawback that anyone who
has somehow gained access to the password can gain access to everything that user
has access to. Nowadays, proper authentication requires:

52 | Chapter 2: Ingredients of Data Governance: Tools



• Something you know—this will be your password or passphrase; it should be
hard to guess and changed regularly.

• Something you have—this serves as a second factor of authentication. After pro‐
viding the right passphrase, a user will be prompted to prove that they have a cer‐
tain device (a cell phone able to accept single-use codes, a hardware token),
adding another layer of security. The underlying assumption is that if you mis‐
place that “object” you would report it promptly, ensuring the token cannot be
used by others.

• Something you are—sometimes, for another layer of security, the user will add
biometric information to the authentication request: a fingerprint, a facial scan,
or something similar.

• Additional context—another oft-used layer of security is ensuring that an
authenticated user can access only certain information from within a specific
sanctioned application or device, or other conditions. Such additional context
often includes:
— Being able to access corporate information only from corporate hardware

(sanctioned and cleared by central IT). This, for example, eliminates the risk
of “using the spouse’s device to check for email” without enjoying the default
corporate anti-malware software installed by default on corporate hardware.

— Being able to access certain information only during working hours—thus
eliminating the risk of personnel using their off-hours time to manipulate
sensitive data, maybe when those employees are not in appropriate surround‐
ings or are not alert to risk.

— Having limited access to sensitive information when not logged in to the cor‐
porate network—when using an internet café, for example, and risking net‐
work eavesdropping.

The topic of authentication is the cornerstone of access control, and each organiza‐
tion will define its own balance between risk aversion and user-authentication fric‐
tion. It is a known maxim that the more “hoops” employees have to jump through in
order to access data, the more these employees will seek to avoid complexity, leading
to shadow IT and information siloing—directions opposed to data governance (data
governance seeks to promote data access to all, under proper restrictions). There are
detailed volumes written on this topic.4

4 An example book about identity and access management is Identity and Access Management by Ertem Osma‐
noglu (Elsevier Science, Syngress).

The Enterprise Dictionary | 53



User Authorization and Access Management
Once the user is properly authenticated, access is determined by a process of checking
whether the user is authorized to access or otherwise perform an operation on the
data object in question, be it a table, a dataset, a pipeline, or streaming data.
Data is a rich medium, and sample access policies can be:

• For reading the data directly (performing “select” SQL statement on a table, read‐
ing a file).

• For reading or editing the metadata associated with the data. For a table, this
would be the schema (the names and types of columns, the table name). For a
file, this would be the filename. In addition, metadata also refers to the creation
date, update date, and “last read” dates.

• For updating the content, without adding new content.
• For copying the data or exporting it.
• There are also access controls associated with workflows, such as performing an

extract-transform-load (ETL) operation to move and reshape the data (replacing
rows/columns with others).

We have expanded here on the policies previously mentioned for data classes, which
also detail partial-read access—which can be its own authorized function.
It’s important to define identities, groups, and roles and assign access rights to estab‐
lish a level of managed access.
Identity and access management (IAM) should provide role management for every
user, with the capability to flexibly add custom roles that group together meaningful
permissions relevant to your organization, ensuring that only authorized and authen‐
ticated individuals and systems are able to access data assets according to defined
rules. Enterprise-scale IAM should also provide context (IP, device, the time the
access request is being generated from, and, if possible, use case for access). As good
governance results in context-specific role and permission determination before any
data access, the IAM system should scale to millions of users, issuing multiple data
access requests per second.

54 | Chapter 2: Ingredients of Data Governance: Tools



Summary
In this chapter, we have gone through the basic ingredients of data governance: the
importance of having a policy book containing the data classes managed, and how to
clean up the data, secure it, and control access. Now it is time to go beyond the
tooling and discuss the importance of the additional ingredients of people and pro‐
cesses to a successful data governance program.

Summary | 55






CHAPTER 3
Ingredients of Data Governance:

People and Processes

As mentioned in the preceding chapters, companies want to be able to derive more
insights from their data. They want to make “data-driven decisions.” Gone are the
days of business decisions being based exclusively on intuition or observation. Big
data and big data analytics now allow for decisions to be made based on collecting
data and extracting patterns and facts from that data.
We have spent much time explaining how this movement into using big data has
brought with it a host of considerations around the governance of that data, and we
have outlined tools that aid in this process. Tools, however, are not the only factor to
evaluate when designing a data governance strategy—the people involved and the pro‐
cess by which data governance is implemented are key to a successful implementation
of a data governance strategy.
The people and process are aspects of a data governance strategy that often get over‐
looked or oversimplified. There is an exceedingly heavier reliance on governance
tools, and though tools are getting better and more robust, they are not enough by
themselves; how the tools are implemented, an understanding of the people using
them, and the process set up for their proper use are all critical to governance success
as well.

The People: Roles, Responsibilities, and Hats
Many data governance frameworks revolve around a complex interplay of many roles
and responsibilities. These frameworks rely heavily on each role playing its part in
keeping the well-oiled data governance machine running smoothly.

57



The problem with this is that most companies are rarely able to exactly or even semi-
fully match these frameworks, due to lack of employee skill set or, more commonly, a
simple lack of headcount. For this reason, employees working in the information and
data space of their company often wear different user hats. We use the term hat to
delineate the difference between an actual role or job title and tasks that are done. The
same person can perform tasks that align with many different roles or wear many dif‐
ferent hats as part of their day-to-day job.

User Hats Defined
In Chapter 1, we outlined three broad categories of governors, approvers, and users.
Here we will look in depth at the different hats (versus roles) within each category
(and expand on an additional category of ancillary hats), the tasks associated with
each hat, and finally, the implications and considerations when looking at these hats
from a task-oriented perspective versus a role-based approach. In Table 3-1 we’ve lis‐
ted out each hat with its respective category and key tasks for quick reference, with
more detailed descriptions following.

Table 3-1. Different hats with their respective categories and the tasks associated with them
Hat Category Key tasks
Legal Ancillary Knows of and communicates legal requirements for compliance
Privacy tsar Governor Ensures compliance and oversees company’s governance strategy/process
Data owner Approver (can also be Physically implements company’s governance strategy 

governor) (e.g., data architecture, tooling, data pipelining, etc.)
Data steward Governor Performs categorization and classification of data
Data analyst/data User Runs complex data analytics/queries
scientist
Business analyst User Runs simple data analyses
Customer support Ancillary (can also be Views customer data (but does not use this data for any kind of analytical
specialist a user) purpose)
C-suite Ancillary Funds company’s governance strategy
External auditor Ancillary Audits a company’s compliance with legal regulations

Legal (ancillary)
Contrary to the title of legal, this hat may or may not be an actual attorney. This hat
includes the tasks of ensuring the company is up to date in terms of compliance with
the legal requirements for its data handling and communicating this information
internally. Depending on the company, the person with this hat may actually be an
attorney (this is especially true for highly regulated companies, which will be covered
in depth later) who must have a deep knowledge of the type of data collected and how
it’s treated to ensure that the company is in compliance in the event of an external
audit. Other companies, especially those dealing with sensitive but not highly

58 | Chapter 3: Ingredients of Data Governance: People and Processes



regulated data, are more likely to simply have someone whose task is to be up to date
on regulations and have a deep understanding of which ones apply to the data the
company collects.

Privacy tsar (governor)
We chose to title this hat privacy tsar because this is a term we use internally at Goo‐
gle, but this hat has also been called governance manager, director of privacy, and
director of data governance in other literature. The key tasks of this hat are those
which ensure that the regulations the legal department has deemed appropriate are
followed. Additionally, the privacy tsar also generally oversees the entire governance
process at the company, which includes defining which governance processes should
be followed and how. We will discuss other process approaches later in this chapter.
It’s important to note that the privacy tsar may or may not have an extremely techni‐
cal background. On the surface it might seem that this hat would come from a techni‐
cal background, but depending on the company and the resources it has dedicated to
its data governance efforts, these tasks are often performed by people who sit more
on the business side of the company rather than on the technical side.
Understanding the movement of people is of utmost importance when it comes to
battling COVID-19. Google, a company that processes significant amounts of highly
personal data that includes location information, was torn between helping health‐
care providers and authorities to battle the deadly pandemic more effectively and pre‐
serving the trust of the billions of people worldwide who use Google’s services.

Privacy tsar, work example 1: Community mobility reports.    The challenge of preserving
privacy while at the same time providing useful, actionable data to health authorities
required the full attention of Google’s privacy tsars, the people entrusted with creating
the internal regulations that make sure that technology does not intrude into users’
personal data and privacy.
The solution they found was to provide information in an aggregated form, based on
anonymized sets of data from only those users who have turned on “location history”
in Google’s services.1 This setting is off by default, and users need to “opt in” to enable
it. Location information history can always be deleted by the user at any time. In
addition, differential privacy (a technique covered in Chapter 7) was further used to
identify small groups of users with outlier results and eliminate those groups com‐
pletely from the provided solution. Another differential privacy technique was
employed to add statistical noise to the results; the noise, statistically irrelevant for
aggregates, helps ensure that no individual can be tracked back through the data.

1 See Google’s “Community Mobility Reports” for more on this.

The People: Roles, Responsibilities, and Hats | 59



The result is a useful set of reports tracking communities and changes in behavior
over time. Health officials can then assess whether “stay at home” orders are being
complied with, and trace back sources of infection due to people congregating.
In Figure 3-1, we see the results of that work. A sample from the report for San Fran‐
cisco County shows increased people presence in residential areas (bottom right) but
a reduced presence across the board in retail sites, grocery stores, parks, transit sta‐
tions, and workplaces. Note that no individual location is named, yet the data is use‐
ful for health officials in estimating where people congregate. For example, an order
about the reopening of retail stores can be considered.

Figure 3-1. Example from the Google mobility reports

Privacy tsar, work example 2: Exposure notifications.    An even more daunting task related
to COVID-19 is how to safely (from a privacy perspective) inform people about pro‐
longed exposure to a person diagnosed with COVID-19.2 Because the virus is highly
contagious and can be transmitted through the air, identifying exposures and making
sure people who were inadvertently exposed to a positively diagnosed person get tes‐
ted (and isolate themselves if testing positive), is crucial to breaking infection chains
and limiting outbreaks. This process is a recognized technique in battling infections

2 For more about exposure notifications, see “Exposure Notifications: Using Technology to Help Public Health
Authorities Fight COVID‑19” and “Privacy: Preserving Contact Tracing”.

60 | Chapter 3: Ingredients of Data Governance: People and Processes



and is otherwise known as contact tracing. Technology can augment this technique by
immediately alerting the individual as an alternative to a prolonged phone investiga‐
tion in which public health authorities question a positively diagnosed individual as
to their whereabouts over the incubation period. (Many people cannot accurately
identify where they have been over the course of the past few days, nor can everyone
easily produce a list of all the people they have interacted with over the course of the
past week.)
However, a positive COVID-19 diagnosis is highly personal, and having this informa‐
tion delivered to everyone that a diagnosed person came in contact with is an emo‐
tionally loaded process. Furthermore, having your technology do that for you is
highly intrusive and will cause resistance to the point of not enabling this technology.
So how does a privacy tsar thread the needle between preserving personal informa‐
tion and privacy and combating a deadly disease?
The solution that was found maintains the principles required to ensure privacy:

• It must be an “opt-in” solution—the people must enable it, and the product pro‐
vides information to ensure consent is acquired after being informed.

• Since the topic is whether or not the subject was next to a diagnosed person,
location information, though it might be useful to health authorities to under‐
stand where the incident occurred, is not collected. This is a decision made by
the privacy tsar in favor of preserving privacy.

• The information is shared only with public health authorities and not with Goo‐
gle or Apple.

So how does the solution work? Every phone beams a unique yet random and fre‐
quently changing identifier to all nearby phones; the phones collect the list of bea‐
cons, and this list is matched with anyone who has reported their diagnosis. If your
phone was in proximity to the phone of someone who has uploaded a positive diag‐
nosis, the match will be reported to you (see Figure 3-2). Note that the identity of the
infected individual is not reported, nor is the specific time and place. Thus the crucial
information (you have been near a positively diagnosed individual so get tested!) is
shared without sacrificing the privacy of the infected individual.

The People: Roles, Responsibilities, and Hats | 61



Figure 3-2. Excerpt from the Google/Apple guide to the exposure notification technology

Data owner (approver/governor)
In order for the privacy tsar’s governance strategy/process to be realized, the data
owner is needed.3 The tasks of the data owner include physically implementing the

3 While “classical literature” on data governance often separates data owners and data custodians (the former
residing more on the business side of things, and the latter more on the technical side), during the course of
our research and interviews with many companies we found that, in practice, these two “sides of the coin” are
often conflated, and the actual tasks of data ownership tend to fall on those with technical expertise.

62 | Chapter 3: Ingredients of Data Governance: People and Processes



processes and/or strategies laid out by the privacy tsar. This most often includes the
ideation and creation of the data architecture of the company, along with choosing
and implementing tooling and data pipeline and storage creation, monitoring, and
maintenance—in other words, “owning the data.” It’s clear from the task descriptions
that these must be performed by someone with quite a bit of technical background
and expertise; hence people who wear the data owner hat largely are engineers or
folks with an engineering background.

Data steward (governor)
When researching data governance, you will find that there is likely to be much
weight given to the role of data steward, and that’s with good reason—the tasks of a
data steward include categorization and classification of data. For any sort of gover‐
nance to be implemented, data must be defined and labeled to clearly identify what
the data is: sensitive, restricted, health related, and so on. A large part of the reason
we advocate the usage of the term hats versus roles is exemplified by the fact that it’s
very rare to find a singular person doing the “role” of data steward in the wild. The
act of data “stewardship” is highly, highly manual and extremely time consuming. In
fact, one company we spoke to said that for a short while it actually had a “full time”
data steward who quit after several months, citing the job as being “completely soul
sucking.” Because of the manual, time-consuming nature of the role—coupled with
the fact that in most cases there is no dedicated person to perform stewardship
duties—these duties often fall to many different people across the company or to
someone who has another role/other duties they perform as well. As such, full data
categorization/classification is often not done well, not done fully, or, in the worst
cases, just not done at all. This is an important item to note, because without steward‐
ship, governance is incomplete at best. We will cover this in more depth later, but
here is where we begin to see a recurring theme when looking at the people and the
process. Many of the governance processes that most companies employ right now
are undertaken to get around the fact that stewardship falls short. As we outlined in
Chapter 1, the quick growth and expansion of data collected by companies has resul‐
ted in an overwhelm of data, and companies simply don’t have the time or headcount
to dedicate to being able to categorize, classify, and label ALL of their data, so they
create and utilize other methods and strategies to “do their best given the limitations.”

Data analyst/data scientist (user)
Data analysts and data scientists are, in general, some of the primary or key users of
data within a company and are largely who data governance efforts are for. Compa‐
nies struggle with governance and security of their data versus the democratization of
their data. In order to be data driven, companies must collect and analyze large
amounts of data. The more data that can be made available to analysts and scientists,
the better—unless of course that data is sensitive and should have limited access.

The People: Roles, Responsibilities, and Hats | 63



Therefore, the better the governance execution, the better (and the more safely) ana‐
lysts or scientists are able to do their job and provide valuable business insights.

Business analyst (user)
While data analysts and data scientists are the main users or consumers of data, there
are a few people in the periphery of a company who also use and view data. In mov‐
ing toward being more data driven, companies have folks who sit on the business side
of things who are very interested in the data analyses produced by analysts and scien‐
tists. In some companies, data engineers aid in the creation and maintenance of much
simpler analytics platforms to aid in “self-service” for business users. More and more
business people in companies have questions that they hope crunching some data will
help them to answer. Analysts/scientists thus end up fielding many, many inquiries,
some of which they simply don’t have time to answer. By enabling business users to
directly answer some of their own questions, analysts/scientists are able to free up
their time to answer more complex analysis questions.

Customer support specialists (user/ancillary)
While a customer support specialist is technically only a “viewer” of data, it’s worth
noting that there are folks with this role who will need access to some sensitive data,
even though they don’t have any tasks around manipulating that data. In terms of
hats, customer support specialists do tend to have this as their sole role and are not
also doing other things; however, they are consumers of data, and their needs, and
how to grant them appropriate access, must be considered and managed by the other
hats executing a company’s governance strategy.

C-suite (ancillary)
In many companies, members of the C-suite have limited tasks in relation to the
actual execution of a data governance strategy. They are nonetheless a critical hat in
the grand scheme of governance because they “hold the purse strings.” As we men‐
tioned earlier, tools and headcount are critical factors when considering a successful
data governance strategy. It thus makes sense that the person who actually funds that
strategy must understand it and be on board with the funding that makes it a reality.

External auditor (ancillary)
We have included the hat of external auditor in this section despite the fact that exter‐
nal auditors are not within a particular company. Many of the companies we’ve spo‐
ken to have mentioned the importance of the external auditor in their governance
strategy. No longer is it good enough to simply be “compliant” with regulations—
companies now often need to prove their compliance, which has direct implications
for the way a governance strategy and process is employed. Oftentimes, companies
need to prove who has access to what data as well as all the different locations and

64 | Chapter 3: Ingredients of Data Governance: People and Processes



permutations of that data (its lineage). While internal tool reporting can generally
help with providing proof of compliance, the way that a governance strategy is set up
and tended to can help, or hinder, the production of this documentation.

Data Enrichment and Its Importance
As we mentioned at the beginning of this section, one person may wear many hats—
that is, perform many of the tasks involved in carrying out a data governance strategy
at a company. As can be seen in the list of hats and the (fairly long) list of tasks within
each hat shown in Table 3-1, it’s easy to see how many of these tasks may not be com‐
pleted well, if at all.
While there are many tasks that are important to successful implementation of a data
governance strategy, it could be argued that the most critical are data categorization,
classification, and labeling. As we pointed out, these tasks are manual and highly time
consuming, which means that they rarely get executed fully. There is a saying: “In
order to govern data, you must first know what it is.” Without proper data enrich‐
ment (the process of attaching metadata to data), the central task of the data steward
hat, proper data governance falls short. This central task of the data steward is so key,
however, that what we commonly see is that the person who wears the data steward
hat also often wears the privacy tsar hat as well as the data owner hat, and/or they
may even wear a hat in a completely different part of the company (a common one we
see is business analyst). Wearing many hats results in a limited amount of tasks that
can be done (one person can only do so much!), and most often the majority of the
time-consuming task of data enrichment falls off the list.
In the next section we will cover some common governance processes we’ve observed
over the years. One thing to keep in mind while reading through the processes is how
the hats play a role in the execution of these processes. We will discuss their interplay
later in this chapter.

The Process: Diverse Companies, Diverse Needs and
Approaches to Data Governance
It’s important to note that in the discussion of the people and processes around data
governance, there is no “one size fits all” approach. As mentioned, in this section we
will begin to examine some broad company categories, outline their specific con‐
cerns, needs, and considerations, and explore how those interplay with a company’s
approach to data governance.

The Process: Diverse Companies, Diverse Needs and Approaches to Data Governance | 65



We’d like to reemphasize the point that there is no “one size fits all”
approach to governance; and you need to fit your program to your
needs—whether those be mitigated by headcount, the kind of data
you collect, your industry, etc. We have seen many governance
approaches and frameworks that are somewhat inflexible and thus
might be difficult to implement if you don’t fit their particular
parameters. We hope that through exploration of elements to con‐
sider you’re able to create a framework for yourself that not only
matches your governance needs and goals but also matches where
your company is right now, and where you’d like to be in the future.

Legacy
Legacy companies are defined as companies that have been around for quite some
time and most certainly have, or have had, legacy on-premises (on-prem) systems—
most often many different systems, which bring with them a host of issues. The time
and level of effort this work requires often leads to it not being done fully, or simply
not being done at all. Further, for consistency (and arguably for the most effective use
of data and data analytics), every company should have a central data dictionary,
defining all data names, classes, and categories that is standardized and used through‐
out the company. Many legacy companies lack this central data dictionary because
their data is spread out through these various on-prem systems. More often than not,
these on-prem systems and the data within them are associated with a particular
branch or line of business. And that branch, in and of itself, is agnostic of the data
that resides in the other lines of the business’s systems. As such, there ends up being a
dictionary for each system and line of business that may not align to any other line of
business or system, which makes cross-system governance and analytics nearly
impossible.
A prime example of this would be a large retailer that has a system that houses its
online sales and another system that handles its brick-and-mortar sales. In one sys‐
tem the income the company receives from a sale is called “revenue,” while in the
other system it’s simply called “sales.” Between the two systems, the same enterprise
dictionary is not being used—one can see where this becomes problematic if execu‐
tives are trying to figure out what the total income for the company is. Analytics are
nearly impossible to run when the same data does not have the same metadata ver‐
nacular attached. This becomes an even larger issue when considering sensitive data
and its treatment. If there is no agreed-upon, company-wide terminology (as outlined
in the enterprise dictionary), and if that terminology is not implemented for all sour‐
ces of data, governance is rendered incomplete and ineffective.
The power of the cloud and big data analytics drives many companies to want to
migrate their data, or a portion of their data, to the cloud—but the past pain of incon‐
sistent enterprise dictionaries and haphazard governance gives them pause. They

66 | Chapter 3: Ingredients of Data Governance: People and Processes



don’t want to “repeat their past mistakes”; they don’t want to simply replicate all of
their current issues. While tools can help to right past wrongs, they simply aren’t
enough. Companies need (and desire) a framework for how to move their data and
have it properly governed from the beginning, with the right people working the right
process.

Cloud Native/Digital Only
Cloud-native companies (sometimes referred to as digital only) are defined as compa‐
nies who have, and have always had, all of their data stored in the cloud. Not always,
but in general, these tend to be much younger companies who have never had on-
premises systems and thus have never had to “migrate” their data to a cloud environ‐
ment. Based on that alone, it’s easy to see why cloud-native companies don’t face the
same issues that legacy companies do.
Despite the fact that cloud-native companies do not have to deal with on-prem sys‐
tems and the “siloing” of data that often comes along with that, they still may deal
with different clouds as well as different storage solutions within and between those
clouds that have their own flavor of “siloing.” For one, a centralized data dictionary, as
we’ve explored, is already a challenge, and having one that spans multiple clouds is
even more daunting. Even if a centralized data dictionary is established, the process
and tools (because some clouds require that only certain tools be used) by which data
is enriched and governed in each cloud will probably be slightly different. And that is
something that hinders consistency (in both process and personnel) and thus hinders
effectiveness. Further, even within a cloud, there can be different storage solutions
that also may carry with them different structures of data (e.g., files versus tables ver‐
sus jpegs). These structures can be difficult to attach metadata to, which makes gover‐
nance additionally difficult.
In terms of cloud—and data governance within clouds specifically—cloud-native
companies tend to have better governance and data management processes set up
from the beginning. Because of their relatively young “age” in dealing with data, there
is often less data overall, and they have more familiarity with some common data-
handling best practices. Additionally, cloud-native companies by definition have all
of their data in the cloud, including their most sensitive data—which means that
they’ve most likely been dealing with the need for governance since the beginning,
and they most likely have some processes in place for governing, if not all of their
data, at least their most sensitive data.

Retail
Retail companies are an interesting category, as not only do they often ingest quite a
bit of data from their own stores (or online stores), but they also tend to ingest and
utilize quite a bit of third-party data. This presents yet another instance in which data

The Process: Diverse Companies, Diverse Needs and Approaches to Data Governance | 67



governance is only as good as the process set up for it and the people who are there to
execute it. The oft-mentioned importance of creating and implementing a data dictio‐
nary applies, as well as having a process around how this third-party data is ingested,
where it lands, and how governance can be applied.
One twist that we have not discussed but that very much applies in the case of retail is
that of governance beyond simple classification of data. So far we have discussed the
importance of classifying data (especially sensitive data) so that it can be known and
thus governed appropriately. That governance most often relates to the treatment of
and/or access to said data. But data access and data treatment are not the only aspects
to consider in some instances; the use case for that data is also important. In the case
of retail, there may be a certain class of data—email for example—that was collected
for the purpose of sending a customer their receipt from an in-store purchase. In the
context (use case) of accessing this data for the purposes of sending a customer a
receipt, this is completely acceptable. If, however, one wanted to access this same data
for the purpose of sending a customer some marketing material around the item they
just purchased, this use case would not be acceptable unless the customer has given
explicit consent for their email to be used for marketing. Now, depending on the
employee structure at a company, this problem may not be solvable with simple role-
based access controls (fulfillment gets access, marketing does not) if the same
employee may cover many different roles. This warrants the need for a more complex
process that includes establishing use cases for data classes.

Combo of Legacy and Retail
A particularly interesting use case we’ve come across in our customer interviews is
that of a very large legacy retail company. In business for over 75 years, this company
is looking to leverage powerful data analytics tools to help it move toward being a
more data-driven company.
The struggle, interestingly, is not only in its old legacy on-prem data-storage systems
but also in its internal processes around data and data management.
It currently has its data separated into several data marts, a configuration aimed at
distributing responsibility for segments of data: a mart for marketing for in-store
sales, for third-party sales, and so on. This, however, has become problematic for the
company, as not only is there no “central source of truth” in terms of an enterprise
dictionary, but it also cannot run any kind of analytics across its data marts, resulting
in duplication of data from one mart to another since analytics can only be run within
a mart.
Historically, the company was very focused on keeping all of its data on-prem; how‐
ever, this view has changed with the increased security that is now available in the
cloud, and the company is now looking to migrate all of its data off-premises.
Through this migration effort, not only is the company looking to change the basic

68 | Chapter 3: Ingredients of Data Governance: People and Processes



infrastructure of its data story from a decentralized one (multiple marts) to a central‐
ized one (centralized data warehouse), but it is also using this as an opportunity to
restructure its internal processes around data management and governance (see
Figure 3-3).

Figure 3-3. Breaking down data silos by restructuring internal processes is often a key
stage after enterprises consolidate their on-premises data into the cloud

Data centralization enables the company to have one enterprise dictionary that allows
for all new data—namely sensitive data—to be quickly marked and treated accord‐
ingly. Quick and easy handling of sensitive data also enables quicker and easier access
controls (as they need to be applied only once, as opposed to each mart). This not
only saves overhead effort but also allows for the implementation of more easy-to-use
self-service analytics tools for employees who may not be analysts by trade but, with
the right tools and access to data, are able to run simple analytics. This new process,
however, is really only good for new incoming data.
This company, like many legacy companies with a lot of historical data, is struggling
with how to handle the data it already has stored. It currently has 15 years of data
(~25 terabytes) stored on-premises, and while it knows there is a wealth of informa‐
tion there, the time and effort required to migrate, enrich, and curate all this data
seems daunting at best, especially when the payoff is not known.

Highly Regulated
Highly regulated companies represent the sector of companies that deal with
extremely sensitive data—data that often carries additional compliance requirements
beyond the usual handling of sensitive information. Some examples of highly
regulated companies would be those dealing with financial, pharmaceutical, or
healthcare services.

The Process: Diverse Companies, Diverse Needs and Approaches to Data Governance | 69



Highly regulated companies deal with multiple kinds of sensitive data. They have to
juggle not only basic data governance best practices but also the additional regula‐
tions related to the data they collect and deal in, and they face regular audits to make
sure that they are aboveboard and compliant. As a result of this, many highly regula‐
ted companies are more sophisticated in their data-governance process. As they’ve
had to deal with compliance related to their data from the get-go, they often have bet‐
ter systems in place to identify and classify their sensitive data and treat it
appropriately.
Also, for many of these kinds of companies, their business is based solely around sen‐
sitive data, so not only do they tend to have better processes in place from the begin‐
ning, but also those processes most often include a more well-funded and
sophisticated organization of the people who handle that sensitive data. These com‐
panies tend to actually have people dedicated to each of the hats discussed earlier, and
that additional headcount, as we pointed out, can be the deciding factor in whether a
data-governance strategy is successful or unsuccessful.
One final note about highly regulated companies is that, as a result of the legal
requirements they face on the handling of certain data, they can function similarly to
legacy companies in that they have difficulty moving off-prem and/or trying out new
tools. Any tool that will touch sensitive data under a regulation must meet the stand‐
ards (fully) of that regulation. As an example, a tool or product in beta may be used
by a healthcare company only if it is HIPAA compliant. Many product betas, because
they’re used for early access and as a way to work out bugs, are not designed to meet
the most stringent compliance standards. While the final product may be compliant,
the beta generally is not, meaning that highly regulated companies often don’t get to
experiment with these new tools/products and thus have trouble migrating to new,
potentially better tools than the ones they’re currently using.

Unique Highly Regulated Organization: Hospital/University
When discussing highly regulated industries, finance and healthcare are the ones
most commonly referenced. In our discussions with customers we came across
another highly regulated industry that had some unique challenges: hospital/universi‐
ties. These “companies” are unique in that they collect a lot of clinical data from their
hospitals, but they also collect (and produce) a lot of research data through
university-sponsored research studies.
Each of these types of data comes with its own specific regulations and standards for
research—e.g., HIPAA covers clinical data, and the Institutional Review Board (IRB)
protects the rights and welfare of human research subjects recruited to participate in
research activities.

70 | Chapter 3: Ingredients of Data Governance: People and Processes



One particular hospital/university we spoke to was looking at the use case of being
able to run secondary analytics across its clinical and research data. Currently it has
two main pipeline solutions for its data: one for clinical and one for research.
For its clinical pipelines, data is stored on-prem in a Clarity data warehouse for each
hospital, and only analysts with access to that database are able to run analytics.
For its research pipelines, each “lab” within the university has its own on-prem stor‐
age, and again, only analysts with access to that server can run analytics. It can be
seen that not only does this structure not allow for secondary analyses to be run
across labs, but they also can’t be run across clinical and research.
Knowing that there is a wealth of value in these secondary analytics, this hospital/
university decided to migrate a large portion of its clinical and research data to the
cloud, to get it all in one central location. In order to do this, however, much needs to
be done to make the data comply with healthcare and research regulations. As such,
the hospital/university created a specialized team dedicated to this migration effort;
that team’s role included tasks such as: creating an enterprise dictionary, enriching
data, reviewing the presence of sensitive data, reviewing policies attached to data,
applying new policies to data, and applying a standardized file structure.
Fortunately for this organization, it was able to secure funding for such an elaborate
undertaking, and yet it is still looking at ways to automate its process. Migration and
the setup of data in the cloud is but one hurdle—maintaining and managing a data
store going forward will require effort as well, and as such, tools that enable automa‐
tion are top of mind for this organization.

Small Companies
For our purposes, we define a small company as one that has fewer than one thou‐
sand employees. One of the benefits of small companies is their smaller employee
footprint.
Smaller companies often have small data-analytics teams, which means that there are
fewer people who actually need to touch data. This means that there is less risk over‐
all. One of the primary reasons to govern data is to make sure that sensitive data does
not fall into the wrong hands. Fewer employees also makes for a much less arduous
and less complicated process of setting up and maintaining access controls. As we
discussed earlier, access controls and policy management can get increasingly compli‐
cated, especially when factors such as use case for data come into play.
Another benefit of a small amount of people touching data is that there is often less
proliferation of datasets. Data analysts and scientists, as part of their jobs, create many
different views of datasets and joins (combining tables from different dataset sour‐
ces). This proliferation of data makes it hard to track where the data came from and
where it’s going to (not to mention who had and now has access). Fewer analysts/

The Process: Diverse Companies, Diverse Needs and Approaches to Data Governance | 71



scientists mean fewer datasets/tables/joins, resulting in data that’s much easier to
track and thus easier to govern.

Large Companies
While there are many more company types, we will end our exploration of broad cat‐
egories with large companies, defined as companies with more than one thousand
employees.
If small companies have the governance benefit of dealing with less data, large com‐
panies deal with the reverse, in that they often deal with a lot of data. Large compa‐
nies not only generate a great deal of data themselves but also often deal with a lot of
third-party data. This results in immense difficulty in wrapping their arms around it
all; they are overwhelmed by the data and often struggle to govern even a portion of
it. As such, only some data gets enriched and curated, which means that only some of
their data is able to be used to drive insights.
Large companies often put processes in place to limit this overwhelm by choosing
only select data to enrich, curate, and thus govern. A common strategy for limiting
the data that a data steward must enrich is to select a number of categories and gov‐
ern only the data that falls within those categories. Another is to only govern known
pipelines of data (these are the pipelines consisting of the primary data a company
deals with, such as daily sales numbers from a retail store) and to handle “ad hoc”
pipeline data (engineers at times are asked to create new pipelines to address one-
time or infrequent data analysis use cases) only as time allows or if absolutely
necessary.
It’s easy to see that these strategies (which we will discuss in more depth later) result
in a sort of iceberg of data where enriched (known), curated data sits at the top, and
below that is a mound of data that is, in the main, un-nriched and thus unknown.
Companies don’t know what this data is, which means they can’t govern it, and
ungoverned data is quite scary. It could be sensitive, it could be noncompliant, and
there could be dire consequences if it happens to get leaked. This causes much fear
and forces companies with this problem to use strategies to help mitigate their risk.
Not only do large companies deal with copious amounts of data, but they also tend to
have a much larger workforce of data analysts, data scientists, and data engineers—all
of whom need access to data to do their jobs. More data, plus more people who need
access to data, results in much more complicated (and often poorly managed) pro‐
cesses around access control. Access control is generally based on user role, which
should determine the data (and only that data) that they need access to. This strategy
may seem simple, but it is a difficult process to implement for two main reasons.
First, in order to know what data is critical to a particular user’s role, data must be
known. And we know from previous discussion in this book that the majority of data
a company has is unknown. This results in not only the inability to govern (it’s

72 | Chapter 3: Ingredients of Data Governance: People and Processes



impossible to govern what you don’t know you have) but also the inability to know
what data is appropriate for whom. Data specialists still need to do their job, however,
so (too much) access is often granted at the expense of risk. Companies try to offset
this risk by creating a “culture of security,” which puts the onus on the employee to do
the right thing and not expose or misuse potentially sensitive information. Another
issue with role-based access is that roles and their ensuing uses for data are not always
black and white; the use case for data must also be considered. Depending on the
company, a user in the same role may be able to use data for some use cases and not
for others (an example being the use case we outlined in the retail section). As
touched on Chapter 2, and as will be covered in depth later, the addition of use case
as a parameter during policy creation helps to mitigate this issue.
Like legacy companies, large companies often also deal with the issue of having many
different storage systems, the majority of which are legacy. Large companies tend to
be built up over time, and with time comes more data and more storage systems.
We’ve already discussed how the siloed nature of different storage systems makes
governance difficult. Different storage systems naturally create data silos, but large
companies have another factor that results in even more complex “silos”: acquisitions.
Large companies are sometimes built completely from within, but others become
large (or larger) due to acquisitions. When companies acquire other, smaller compa‐
nies, they also acquire all their data (and its underlying storage), which brings along a
whole host of potential problems—primarily, how the acquired company handled its
data, as well as its overall governance process and approach. This includes how the
company managed its data: its method of data classification, its enterprise dictionary
(or lack thereof), its process and methods around access control, and its overall cul‐
ture of privacy and security. For these reasons, many large companies find it nearly
impossible to marry their central governance process with that of their acquisitions,
which often results in acquired data sitting in storage and not being used for
analytics.

People and Process Together: Considerations, Issues, and
Some Successful Strategies
We have now outlined the different people involved in various kinds of companies, as
well as some specific processes and approaches utilized by the different company
types.
There is obviously a synergy between people and process, of which there are some
considerations and issues. We will review several of the issues we’ve observed, along
with outlining a few strategies we have seen in which the right people and process
together have resulted in moving toward a successfully implemented data-governance
strategy.

People and Process Together: Considerations, Issues, and Some Successful Strategies | 73



Considerations and Issues
This certainly is not an exhaustive list of all the potential issues with implementing a
successful data-governance strategy; we are simply highlighting some of the top
issues we’ve observed. We only briefly note the mitigation efforts to combat these
issues; the latter half of this text will go into these in much more detail.

“Hats” versus “roles” and company structure
We previously discussed our intentional use of the terms hats versus roles and the
impact that wearing many hats has on the data governance process. To expand on
that idea further, an additional issue that arises with hats versus roles is that responsi‐
bility and accountability become unclear. When looking at the different kinds of
approaches that different companies take to achieve governance, an underlying need
is for actual people to take responsibility for the parts of that process. This is easy
when it is clearly someone’s job to conduct a piece of the process, but when the lines
between what is and what is not within a person’s purview are blurred, these fuzzy
lines often result in inadequate work, miscommunication, and overall mismanage‐
ment. It’s clear that a successful governance strategy will rely not simply on roles but
on tasks, and on who is responsible or accountable for these tasks.

Tribal knowledge and subject matter experts (SMEs)
When talking with customers about their pain points with data governance, one of
the things we hear over and over again is that they need tools to help their analysts
find which datasets are “good,” so that when an analyst is searching for data, they
know that this dataset is of the best quality and is the most useful one for their use
case. The companies state that this would help their analysts save time searching for
the “right/best” dataset, as well as assisting them in producing better analytics. Cur‐
rently, through most companies, the way analysts know which datasets they should
work with is by word of mouth, or “tribal knowledge.” This is an obvious problem for
companies because roles change, people move on, etc. Companies request “crowd‐
sourcing” functionality, such as allowing analysts to comment on or rank datasets to
help give them a “usefulness” score for others to see when searching. This suggestion
is not without merit but uncovers the larger problems of findability and quality.
Companies are relying on people to know the usefulness of a dataset and to transfer
that knowledge on to others, yet this strategy is fallible and difficult (if not impossi‐
ble) to scale. This is where tools that lessen (or negate) the effort placed on a particu‐
lar user or users aid in the process. A tool that can detectthe most-used datasets and
surface these first in a search, for example, can help minimize the reliance on tribal
knowledge and SMEs.

74 | Chapter 3: Ingredients of Data Governance: People and Processes



Definition of data
Regardless of their type, all companies want to be able to collect data that can be used
to drive informed business decisions. They are certain that more data, and the analyt‐
ics that could be run on that data, could result in key insights—insights that have the
potential to skyrocket the success of their business. The problem, however, is that in
order for data to be used, it must be known. It must be known what the letters or
numbers or characters in a column of a table mean. And now it must also be known
whether those numbers, letters, or characters represent information that is sensitive
in nature and thus needs to be treated in a specific way. Data enrichment is key to
“knowing” data, yet it is largely a manual process. It generally requires actual people
to look at each and every piece of data to determine what it is. As we discussed, this
process is cumbersome on its own, and it becomes almost impossible when the extra
complexity of disparate data storage systems and different data definitions and cata‐
logs are considered. The “impossible” nature of this work in general means that it just
never gets done; this leaves companies scrambling to use a few tools and to imple‐
ment some half strategies to make up for it—and also hoping that educating people
on how data should be treated and handled will somehow be enough.

Old access methods
Gone are the days of simple access controls—i.e., these users/roles get access and
these users/roles do not. Historically, there were not many users or roles who even
had the need to view or interact with data, which meant that only a small portion of
employees in a company needed to be granted access in the first place. In today’s
data-driven businesses there is the potential for many, many users who may need to
touch data in a variety of ways, as seen in the beginning of this chapter. Each hat has
different types of tasks that it may need to do in relation to data that requires varying
levels of access and security privilege.
We already discussed the problematic nature of unknown data; another layer to that
is the implementation of access controls. There is a synergy between knowing which
data even needs access restrictions (remember, data must be known in order for it to
be governed) and knowing what those restrictions should be for what users. As dis‐
cussed in Chapter 2, there are varying levels of access control, all the way from access
to plain text to access to hashed or aggregated data.
A further complication around access is that of the intent of the user accessing data.
There may be use cases for which access can and should be granted, and other use
cases for which access should strictly be denied. A prime example (and one we’ve
heard from more than one company) is a customer’s shipping address. Imagine the
customer just bought a new couch, and their address was recorded in order to fulfill
shipment of that couch. A user working to fulfill shipping orders should undoubtedly
get access to this information. Now let’s say that the slipcovers for the couch this cus‐
tomer just bought go on sale, and the company would like to send a marketing flyer

People and Process Together: Considerations, Issues, and Some Successful Strategies | 75



to the customer to let them know about this sale. It may be the case that the user in
the company who handles shipping data also happens to handle marketing data
(remember hats?). If the customer has opted OUT of promotional mail, the user in
this example would not be able to send the marketing material even though they have
access to that data. This means that access controls and policies need to be sensitive
enough to account not only for a black-and-white “gets access/doesn’t get access” rule
for a particular user, but also for what purpose the user is using the data.

Regulation compliance
Another struggle that companies have is around compliance with regulations. Some
regulations, such as those in financial and healthcare industries, have been around for
quite a while, and as we pointed out before, companies who deal with these kinds of
data tend to have better governance strategies, for two main reasons: one, they’ve
been dealing with these regulations for some time and have built in processes to
address them, and two, these regulations are fairly established and don’t change
much.
The advent of a proliferation of data collection has brought about new regulations
such as GDPR and CCPA that aim to protect all of a person’s data, not just their most
sensitive data (i.e., health or financial data). Companies in all types of industries, not
just highly regulated ones, now must comply with these new regulations or face seri‐
ous financial consequences. This is a difficult endeavor for companies that previously
did not have regulations to comply with and thus perhaps did not address this during
the setting up of their data infrastructure. As an example, one of the main compo‐
nents of GDPR is the “right to be forgotten,” or the ability for a person to request that
all their data collected by a company be deleted. If the company does not have its data
set up to find all the permutations of a person’s individual data, it will struggle to be
compliant. Thus, it can be seen how findability is important not only from the per‐
spective of finding the right data to analyze but also from the perspective of finding
the right data to delete.

Case Study: Gaming of Metrics and Its Effect on Data Governance
Be aware of the human element when you assign regulations and compliance. A rele‐
vant case study about how, when introducing metrics, you should factor in the human
response to these metrics is in Washington, DC’s school system. In 2009, Washington,
DC introduced a new ranking system called IMPACT through which teachers were
assessed and scored. The system, born out of the intention to promote “good” teach‐
ers and generally improve education—and, even further, to allow “low scoring” teach‐
ers to be dismissed—actually did not achieve the desired result.
The ranking system was based on an algorithm that took into account standardized
test scores of students, with the underlying assumption that test scores for students

76 | Chapter 3: Ingredients of Data Governance: People and Processes



with “good” teachers should show improvement year after year. However, the ranking
system did not include social and familial circumstances, nor did it include any kind
of feedback or training from a controlled set.
The result of implementing the system was that teachers were let go based only on the
results of the standardized tests, and without regard to feedback from administrators.
The immediate reaction was that teachers pivoted to focusing on how to pass the
standardized tests rather than on the underlying principles of the subjects. This resul‐
ted in high scores and advancement for teachers whose students passed the tests, but
it did not, sadly, promote excellence among the students.
In fact, in 2018 a city-commissioned investigation revealed that one in three students
graduating in 2017 received a diploma despite missing classes. After being challenged,
and after a detailed account of the usage of the algorithm and its usefulness had been
carried out, the system was eventually “reconsidered” in 2019.4

The process lessons here are that, although the motivation (improving teaching, pro‐
moting excellent teachers) should have been easily agreed upon by both teachers’
unions and the school district administrators, the implementation was lacking. A
proper way to introduce a process is to first have a discussion with the affected peo‐
ple. Take feedback to heart and act on it. Run the process for the first time as a trial,
with transparent results, open discussion, and a willingness to pivot if it isn’t working
as intended.

Processes and Strategies with Varying Success
The issues with people and process are many, and we have observed some strategies
that have been implemented with varying degrees of success. While we will explore
some of these strategies here, the latter part of this book dives deeper into how these
strategies (and others) can be implemented to achieve greater data governance
success.

Data segregation within storage systems
We’ve reviewed some of the issues that arise from having multiple data-storage sys‐
tems; however, some companies use multiple storage systems and even different stor‐
age “areas” within the same storage system in an advantageous and systematic way.
Their process is to separate curated/known data from uncurated/unknown data. They
may do this in a variety of ways, but we have heard two common, prevailing
strategies.

4 Perry Stein, “Chancellor Pledges to Review D.C.’s Controversial Teacher Evaluation System”, Washington Post,
October 20, 2019.

People and Process Together: Considerations, Issues, and Some Successful Strategies | 77



The first strategy is to keep all uncurated data in an on-prem storage system and to
push curated data that can be used for analytics to the cloud. The benefit that compa‐
nies see in this strategy is that the “blast radius,” or the potential for data to be leaked
either by mistake or by a bad actor, is greatly diminished if only known, clean, and
curated data is moved into a public cloud. To be clear, companies often indicate that it
is not their total distrust of cloud security that is the problem (although that can play
a role as well); it is their concern that their own employees may unwittingly leak data if
it’s in a public cloud versus an on-prem environment.
Figure 3-4 shows an example of data stored on-premises and in the cloud. As you can
see, even though both storage systems have structured datasets, only the cloud dataset
has been enriched with metadata and thus has been treated with the appropriate gov‐
ernance controls (hashing in this case). The on-premises dataset, while it has the
same information, hasn’t been curated or enriched; we don’t know just by looking at it
that the numbers in the first column are credit card numbers or that the words in the
second column are customer names. Once these columns have been curated and the
appropriate metadata (credit card number, customer name, etc.) has been attached,
we can attach governance controls, so that even if the data gets leaked, it will be the
protected version and not plain text.

Figure 3-4. Enriched versus not enriched datasets on-premises and in the public cloud

As we’ve stated, an obvious benefit of this strategy is that if sensitive data resides only
on-premises, then if it’s leaked or incorrectly accessed, that could only have been
done by someone within the company. On the other hand, if that same sensitive data
is in a public cloud, it could potentially have been accessed by anyone if leaked or
hacked into.

78 | Chapter 3: Ingredients of Data Governance: People and Processes



There are a few drawbacks to this strategy, however. One is that when data is segrega‐
ted this way—some residing on premise and some residing in the cloud—cross-
storage analytics are difficult if not impossible to complete. Since one of the main
drivers of collecting so much data is being able to run powerful analytics, hindering
this seems counterproductive. The other drawback is that segregation also requires
upkeep of and attendance to these multiple storage systems and data pipelines, not to
mention the creation, maintenance, and enforcement of additional access controls, all
of which are difficult to properly manage and stay on top of over time.
The second strategy is similar to the first in that there is a separation between curated
and uncurated data, but this is done within the same cloud environment. Companies
will create different layers or zones (as can be seen in Table 3-2) within their cloud
environment and base access on these; the bottom, uncurated zone may be accessed
by only a few users, while the uppermost zone, curated and cleaned (of sensitive
data), may be accessed by any user.

Table 3-2. A “data lake” within cloud storage with multiple tiers showing what kind of data
resides in each and who has access

Types of data Access
Insights Known, enriched, curated, and cleaned data. Data also has likely had Highest level of access. Most if not all
zone governance controls such as encryption, hashing, redaction, etc. data analysts/scientists and others in a

Example: Well-labeled, structured datasets. user role.
Staging More known and structured data. Data from multiple sources is likely More access. Mostly data engineers—
zone to be joined here. This is also where data engineers prep data, cleanse those in charge of putting together

it, and get it ready to drop into the insights zone. datasets for analytics.
Raw zone Any kind of data. Unstructured and uncurated. Could also include Very restricted access. Likely a handful

things such as videos, text files, etc. Example: Video files, unstructured of people or just an admin.
datasets

Clearly the benefits and drawbacks of this strategy are nearly a reverse of those of the
previous strategy. In this strategy, management and upkeep of the storage system,
data pipelines, and policies are limited to one system, which makes it far simpler and
more streamlined to stay on top of. Analytics are also largely easier to run because
they can all be run within one central storage system, as opposed to trying to run
them across multiple systems, or dealing with the constant moving of data from one
storage system to another for the purposes of analysis.
As we’ve noted, all data residing within a public cloud does carry the potential draw‐
back of data leaking (whether intentionally or unintentionally) out onto the public
internet—a cause for trepidation to be sure.

People and Process Together: Considerations, Issues, and Some Successful Strategies | 79



Data segregation and ownership by line of business
As we’ve mentioned more than a few times, data enrichment is a key challenge in suc‐
cessful data governance for many reasons, the primary ones being level of effort and
lack of accountability/ownership.
One way we’ve observed companies handle this issue is by segregating their data by
line of business. In this strategy, each line of business has dedicated people to do the
work of governance on just that data. While this is often not actually delineated by
role, each line of business has a deep knowledge of the kind of business it has and
handles the ingress and egress of data (pipelines), data enrichment, enforcement/
management of access controls and governance policies, and data analysis. Depend‐
ing on the company size and/or complexities of data within each line of business,
these tasks may be handled by just one person, a handful of people, or a large team.
There are several reasons this process tends to be quite successful. The first is that the
amount of data any given “team” must wrap its arms around is smaller. Instead of
having to look at and manage a company’s entire data story, the team needs to under‐
stand and work on only a portion of it. Not only does this result in less work, but it
also allows for deeper knowledge of that data. Deeper knowledge of data has a multi‐
tude of advantages, a few being quicker data enrichment and the ability to run
quicker, more robust analytics.
The second reason this process is successful is that there is clear, identifiable owner‐
ship and accountability for data: there is a specific person (or persons) to go to when
something goes wrong or when something needs to change (such as the addition of a
new data source or the implementation of a new compliance policy). When there is
no clear accountability and responsibility for data, it’s easy for that data to be lost or
forgotten or worse—mismanaged.
In Figure 3-5, we’ve tried to show an example flow of different lines of business into a
central repository. As you can see, retail store sales, marketing, online sales, and HR
not only all feed into the central enterprise data repository in this example, but they
are also fed by this repository.
To give you an idea of the different key hats and their tasks in each line of business,
let’s take marketing as an example. In this line of business in our example, we have
the hats of data owner, data steward, and business analyst.
The data owner sets up and manages the pipelines in this line of business, along with
managing requests for new pipelines and ingestion sources. They also perform the
tasks of monitoring, troubleshooting, and fixing any data quality issues that arise, as
well as implementing any of the technical aspects of the company’s governance poli‐
cies and strategies.

80 | Chapter 3: Ingredients of Data Governance: People and Processes



The data steward is the subject matter expert (SME) in this line of business; knowing
what data resides here, what it means, how it should be categorized/classed, and what
data is sensitive and what isn’t. They also serve as the point of contact between their
line of business and the central governing body at their company for staying up to
date on compliance and regulations, and they’re responsible for ensuring that the data
in their line of business is in compliance.
Finally, the business analyst is the expert on the business implications for the data in
this line of business. They’re also responsible for knowing how their data fits into the
broader enterprise, and for communicating which data from their line of business
should be used in enterprise analytics. In addition, they need to know what addi‐
tional/new data will need to be collected for this particular line of business to help
answer whatever the current or future business questions are.
From this example you can see how each hat has their role and tasks for just this one
line of business, and how a breakdown of any of those tasks can result in a less than
efficient flow/implementation of a governance program.

Figure 3-5. Flowchart of an enterprise with four lines of business and their data ingress/
egress, as well as a close-up of one line of business, its key hats

This process, while successful, does come with some pitfalls, however. The primary
one is that segregating data by line of business encourages the siloing of data and can,
depending on how it’s set up, inhibit cross-company analytics.
We recently worked with a large retail company that was dealing with this exact issue.

People and Process Together: Considerations, Issues, and Some Successful Strategies | 81



This large US retailer (with over 350,000 employees), in an effort to better deal with
the tremendous amount of data it collects, divided up its data by several lines of busi‐
ness. These included air shipping, ground shipping, retail store sales, and marketing.
Having the data separated in this way greatly enabled the company to devote specific
data analytics teams to each line of business, whereby it could more quickly enrich its
data, allowing it to apply governance policies to aid with CCPA compliance. This
strategy, however, created issues when the company began to want to run analytics
across its different lines of business. To separate its data, the company created infra‐
structure and storage solutions with data pipelines that fed directly (and only) into
one or another line of business. We won’t go into too much depth on data pipelines
here, but in short, it is a common practice to have data “land” in only one storage
solution, because duplicating data and transferring it to additional storage areas is
costly and difficult to maintain. Because specific data resided only in one storage
area/silo, the company could not run analytics across its lines of business to see what
patterns might emerge between, for example, air shipping and retail store sales
(Figure 3-6).

Figure 3-6. The preceding example company’s two data silos: air shipping and ground
shipping. Each silo has its own data pipeline and stores this data within its own bucket,
meaning analytics can be run only within silos unless data from that pipeline is duplica‐
ted and piped into another silo.

This process of segregation by line of business to aid with accountability and respon‐
sibility is not a bad strategy, and while it’s obvious there are pitfalls, there are also
ways to make it more successful, which we will cover later in the book.

Creation of “views” of datasets
A classic strategy employed by many companies is to create different “views” of
datasets. As seen in Table 3-3, these views are really just different versions of the same
dataset and/or table that have sensitive information sanitized or removed.

82 | Chapter 3: Ingredients of Data Governance: People and Processes



Table 3-3. Three potentially different types of “views” of data: one plain text, one with
sensitive data hashed, and another with sensitive data redacted

Plain text customer name Hashed customer name Redacted customer name
Anderson, Dan Anderson, ##### ********
Buchanan, Cynthia Buchanan, ##### ********
Drexel, Frieda Drexel, ##### ********
Harris, Javiar Harris, ##### ********

This strategy is classic (and works) because it allows analytics to easily be run, worry
free, by virtually anyone on the “clean” view (the one with sensitive data either hashed
or removed). It takes away the risk of access to and usage of sensitive data.
While this strategy works, it is problematic in the long run for several reasons. The
first is that it takes quite a bit of effort and manpower to create these views. The clean
view has to be manually created by someone who does have access to all of the data.
They must go in and identify any and all columns, rows, or cells that contain sensitive
data and decide how they should be treated: hashed, aggregated, completely removed,
etc. They then must create an entirely new dataset/table with these treatments in place
and make it available to be used for analytics.
The second issue is that new views constantly need to be created as fresher data
comes in. This results not only in much time and effort being put into creating “fresh”
views but also in a proliferation of datasets/tables that are difficult to manage. Once
all these views are created (and re-created), it’s hard to know which is the most fresh
and should be used. Past datasets/tables often can’t be immediately deprecated, as
there may be a need down the line for that specific data.
While views have some success and merit in aiding in access controls, we will pro‐
pose and discuss some strategies we have seen that not only are easier to manage but
scale better as a company collects more and more data.

A culture of privacy and security
The final process we’d like to discuss is that of creating a culture of privacy and secu‐
rity. While certainly every company and employee should respect data privacy and
security, the ways in which this strategy is thought through and implemented are
truly a special ingredient in creating not just a good data governance strategy but a
successful one.
We have dedicated an entire chapter to the strategy of building a successful data cul‐
ture (see Chapter 9), so more on that is to come; however, as a short introduction to
the concept, we will note that we have seen companies driven to really look at their
data culture and work to implement a new (successful) one because of one (or all) of
the following occurring within their organization:

People and Process Together: Considerations, Issues, and Some Successful Strategies | 83



• Their governance/data management tools are not working sufficiently (meaning
the tools do not in and of themselves provide all of the governance “functional‐
ity” desired by a company).

• People are accessing data they should not be accessing and/or using data in ways
that are not appropriate (whether intentionally or not).

• People are not following processes and procedures put into place (again, whether
intentionally or not).

• The company knows that governance standards/data compliance are not being
met and don’t know what else to do, but they hope that educating people on
doing the “right thing” will help.

To be sure, throughout this text we have already covered in great detail governance
tools, the people involved, and the processes that are/can be followed. This is where
the importance of putting all the pieces together can be seen—i.e., tools, people, and
process. One or even two pieces are insufficient to achieve a successful governance
strategy.
As evidenced by the four reasons we’ve just listed even though a few tools, some peo‐
ple, and several processes are in place, some gaps may still remain.
This is where it must all come together—in a collective data culture that encompasses
and embodies how the company thinks about and will execute its their governance
strategy. That encompasses what tools it will use, what people it will need, and the
exact processes that will bring it all together.

Summary
In this chapter, we have reviewed multiple unique considerations regarding the peo‐
ple and process of data governance for different kinds of companies. We’ve also cov‐
ered some issues commonly faced by companies, as well as some strategies we’ve seen
implemented with varying success.
From our discussion it should be clear that data governance is not simply an imple‐
mentation of tools but that the overall process and consideration of the people
involved—while it may vary slightly from company to company or industry to indus‐
try—is important and necessary for a successful data governance program.
The process for how to think about data, how it should be handled and classified
from the beginning, how it continually needs to be (re)classified and (re)categorized,
and who will do this work and be responsible for it—coupled with the tools that
enable these tasks to be done efficiently and effectively—is key, if not mandatory, for
successful data governance.

84 | Chapter 3: Ingredients of Data Governance: People and Processes



CHAPTER 4
Data Governance over a Data Life Cycle

In previous chapters, we introduced governance, what it means, and the tools and
processes that make governance a reality, as well as the people and process aspects of
governance. This chapter will bring together those concepts and provide a data life
cycle approach to operationalize governance within your organization.
You will learn about a data life cycle, the different phases of a data life cycle, data life
cycle management, applying data governance over a data life cycle, crafting a data
governance policy, best practices along each life cycle phase, applicable examples, and
considerations for implementing governance. For some, this chapter will validate
what you already know; for others, it will help you ponder, plant seeds, and consider
how these learnings can be applied within your organization. This chapter will intro‐
duce and address a lot of concepts that will help you get started on the journey to
making governance a reality. Before getting into the detailed aspects of governance,
it’s important to center our understanding on data life cycle management and what it
means for governance.

What Is a Data Life Cycle?
Defining what a data life cycle is should be easy—but in reality, it’s quite complex. If
you look up the definition of a data life cycle and its phases, you quickly realize that it
varies from one author to another, and from one organization to another. There’s
honestly not one right way to think about the different stages a piece of data goes
through; however, we can all agree that each phase that is defined has certain charac‐
teristics that are important in distinguishing it from the other phases. And because of
these different characteristics within each phase, the way to think about governance
will also vary as each piece of data moves through the data life cycle. In this chapter,
we will define a data life cycle as the order of stages a piece of data goes through from

85



its initial generation or capture to its eventual archival or deletion at the end of its
useful life.
It’s important to quickly point out that this definition tries to capture the essence of
what happens to a piece of data; however, not all data goes through each phase, and
these phases are simply logical dependencies and not actual data flows.
Organizations work with transactional data as well as with analytical data. In this
chapter, we will primarily focus on the analytics data life cycle, from the point when
data is ingested into a platform all the way to when it is analyzed, visualized, purged,
and archived.
Transactional systems are databases that are optimized to run day-to-day transactional
operations. These are fully optimized systems that allow for a high number of concur‐
rent users and transaction types. Even though these systems generate data, most are
not optimized to run analytics processes. On the other hand, analytical systems are
optimized to run analytical processes. These databases store historical data from vari‐
ous sources, including CRM, IOT sensors, logs, transactional data (sales, inventory),
and many more. These systems allow data analysts, business analysts, and even exec‐
utives to run queries and reports against the data stored in the analytic database.
As you can quickly see, transactional data and analytical data can have completely
different data life cycles depending on what an organization chooses to do. That said,
for many organizations, transactional data is usually moved to an analytics system for
analysis and will therefore undergo the phases of a data life cycle that we will outline
in the following section.
Proper oversight of data throughout its life cycle is essential for optimizing its useful‐
ness and minimizing the potential for errors. Data governance is at the core of mak‐
ing data work for businesses. Defining this process end-to-end across the data life
cycle is needed to operationalize data governance and make it a reality. And because
each phase has distinct governance needs, this ultimately helps the mission of data
governance.

Phases of a Data Life Cycle
As mentioned earlier, you will see a data life cycle represented in many different ways,
and there’s no right or wrong answer. Whichever framework you choose to use for
your organization has to be the one guiding the processes and procedures you put in
place. Each phase of the data life cycle as shown in Figure 4-1 has distinct characteris‐
tics. In this section, we will go through each phase of the life cycle as we define it,
delve into what each phase means, and walk through the implications for each phase
as you think about governance.

86 | Chapter 4: Data Governance over a Data Life Cycle



Figure 4-1. Phases of a data life cycle

Data Creation
The first phase of the data life cycle is the creation or capture of data. Data is gener‐
ated from multiple sources, in different formats such as structured or unstructured
data, and in different frequencies (batch or stream). Customers can choose to use
existing data connectors, build ETL pipelines, and/or leverage third-party ingestion
tools to load data into a data platform or storage system. Metadata—data about
data—can also be created and captured in this phase. You will notice data creation
and data capture used interchangeably, mostly because of the source of data. When
new data is created, that is referred to as data creation, and when existing data is fun‐
neled into a system, it is referred to as data capture.
In Chapter 1, we mentioned that the rate at which data is generated is growing expo‐
nentially, with IDC predicting that worldwide data will grow to 175 zettabytes by
2025.1 This is enormous! Data is typically created in one of these three ways:
Data acquisition

When an organization acquires data that has been produced by a third-party
organization

Data entry
When new data is manually entered by humans or devices within the
organization

Data capture
When data generated by various devices in an organization, such as IOT sensors,
is captured

It’s important to mention that each of these ways of generating data offers significant
data governance challenges. For example, what are the different checks and balances
for data acquired from outside your organization? There are probably contracts and
agreements that outline how the enterprise is allowed to use this data and for what
purposes. There might also be limitations as to who can access that specific data. All
these offer considerations and implications for governance. Later in the chapter, we
will look at how to think about governance during this phase, and we will call out the
different tools you should think about when designing your governance strategy.

1 Andy Patrizio, “IDC: Expect 175 Zettabytes of Data Worldwide by 2025”, Network World, December 3, 2018.

Phases of a Data Life Cycle | 87



Data Processing
Once data has been captured, it is then processed, without yet deriving any value
from it for the enterprise. This is done prior to its use. Data processing is also referred
to as data maintenance, and this is when data goes through processes such as integra‐
tion, cleaning, scrubbing, or extract-transform-load (ETL) to get it ready for storage
and eventual analysis.
In this phase, some of the governance implications that you will come across are data
lineage, data quality, and data classification. All these have been discussed in much
more detail in Chapter 2. To make governance a reality, how do you make sure that as
data is being processed, its lineage is tracked and maintained? In addition, checking
data quality is very important to make sure you’re not missing any important values
before storing this data. You should also think about data classification. How are you
dealing with sensitive information? What is it? How are you ensuring management of
and access to this data so it doesn’t get into the wrong hands? Finally, as this data is
moving, it needs to be encrypted in transit and then later at rest. There are a lot of
governance considerations during this phase. We will delve into these concepts later
in the chapter.

Data Storage
The third phase in the data life cycle is data storage, where both data and metadata are
stored on storage systems and devices with the appropriate levels of protection.
Because we’re focusing on the analytics data life cycle, a storage system could be a
data warehouse, a data mart, or a data lake. Data should be encrypted at rest to pro‐
tect it from intrusions and attacks. In addition, data needs to be backed up to ensure
redundancy in the event of a data loss, accidental deletion, or disaster.

Data Usage
The data usage phase is important to understanding how data is consumed within an
organization to support the organization’s objectives and operations. In this phase,
data becomes truly useful and empowers the organization to make informed business
decisions when it can be viewed, analyzed, and/or visualized for insights. In this
phase, users get to ask all types of questions of the data, via a user interface or busi‐
ness intelligence tools, with the hope of getting “good” answers. This is where the
rubber meets the road, especially when confirming whether the governance processes
already instituted in previous phases truly work. If data quality is not implemented
correctly, the types of answers you receive will be incorrect or might not make too
much sense, and this could potentially jeopardize your business operations.

88 | Chapter 4: Data Governance over a Data Life Cycle



In this phase, data itself may be the product or service that the organization offers. If
data is indeed the product, then different governance policies need to be enacted to
ensure proper handling of this data.
Because data is consumed by multiple internal and external stakeholders and pro‐
cesses during this phase, proper access management and audits are key. In addition,
there might be regulatory or contractual constraints on how data may actually be
used, and part of the role of data governance is to ensure that these constraints are
observed accordingly.

Data Archiving
In the data archiving phase, data is removed from all active production environments
and copied to another environment. It is no longer processed, used, or published, but
is stored in case it is needed again in an active production environment. Because the
volume of data generated is growing, it’s inevitable that the volume of archived data
grows. In this phase, no maintenance or general usage occurs. A data governance
plan should guide the retention of this data and define the length of time it will be
stored, including the different controls that will be applied to this data.

Data Destruction
In this final phase, data is destroyed. Data destruction, or purging, refers to the
removal of every copy of data from an organization, typically done from an archive
storage location. Even if you wanted to save all your data forever, it’s just not feasible.
It’s very expensive to store data that is not in use, and compliance issues create the
need to get rid of data you no longer need. The primary challenge of this phase is
ensuring that all the data is properly destroyed and at the right time.
Before destroying any data, it is critical to confirm whether there are any policies in
place that would require you to retain the data for a certain period of time. Coming
up with the right timeline for this cycle means understanding state and federal regu‐
lations, industry standards, and governance policies to ensure that the right steps are
taken. You will also need to prove that the purge has been done properly, which
ensures that data doesn’t consume more resources than necessary at the end of its
useful life.
You should now have a solid understanding about the different phases of a data life
cycle and what some of the governance implications are. As stated previously, these
phases are logical dependencies and not necessarily actual data flows. Some pieces of
data might go back and forth between different processing systems before being
stored. And some that are stored in a data lake might skip processing altogether and
get stored first, and then get processed later. Data does not need to pass through all
the phases.

Phases of a Data Life Cycle | 89



We’re sure you’ve heard the phrase “Rome was not built in a day,” but that’s really
what this data life cycle is trying to do. Applying data governance in an organization
is a daunting task and can be very overwhelming. However, if you think about your
data within these logical data life cycle phases, implementing governance can be a
task that can be broken down into each phase and then thought through and imple‐
mented accordingly.

Data Life Cycle Management
Now that you understand the data life cycle, another common term you will run into
is data life cycle management (DLM). What’s interesting is that many authors will
refer to data life cycle and data life cycle management interchangeably. Even though
there might be a need or desire to bundle them together, it’s important to realize that
a data life cycle can exist without data life cycle management. DLM, therefore, refers
to a comprehensive policy-based approach to manage the flow of data throughout its
life cycle, from creation to the time when it becomes obsolete and is purged. When an
organization is able to define and organize the life cycle processes and practices into
repeatable steps for the company, this refers to DLM. As you start learning about
DLM, you will quickly run into a data management plan. So let’s quickly look at what
that means and what it entails.

Data Management Plan
A data management plan (DMP) defines how data will be managed, described, and
stored. In addition, it defines standards you will use and how data will be handled
and protected throughout its life cycle. You will primarily see data management plans
required to drive research projects within institutions, but the concepts of the process
are fundamental to implementing governance. Because of this, it’s worth us doing a
deep dive into them and seeing how these could be applied to implement governance
within an organization.
With governance, you will quickly realize that there is no a lack of templates and
frameworks—see, for example, the DMPTool from Massachusetts Institute of Tech‐
nology. You simply need to pick a plan or framework that works for your project and
organization and march ahead; there’s not one right or wrong way to do it. If you
choose to use a data management plan, here is some quick guidance to get you
started. The concepts here are much more fundamental than the template or frame‐
work, so if you were able to capture these in a document, then you’d be ahead of the
curve.

90 | Chapter 4: Data Governance over a Data Life Cycle



Guidance 1: Identify the data to be captured or collected
Data volume is important to helping you determine infrastructure costs and people
time. You need to know how much data you’re expecting and the types of data you
will be collecting:
Types

Outline the various types of data you will be collecting. Are they structured or
unstructured? This will help determine the right infrastructure to use.

Sources
Where is the data coming from? Are there restrictions on how this data can be
used or manipulated? What are those rules? All of these need to be documented.

Volume
This can be a little difficult to predict, especially with the exponential growth in
data; however, planning for that increase early on and projecting what it could be
would set you apart and help you be prepared for the future.

Guidance 2: Define how the data will be organized
Now that you know the type, sources, and volume of data you’re collecting, you need
to determine how that data will be managed. What tools do you need across the data
life cycle? Do you need a data warehouse? Which type, and from which vendor? Or
do you need a data lake? Or do you need both? Understanding these implications and
what each means will allow you to better define what your governance policies should
be. There are many regulations that govern how data can and cannot be used, and
understanding them is vital.

Guidance 3: Document a data storage and preservation strategy
Disasters happen, and ensuring that you’ve adequately prepared for one is very
important. How long will a piece of data be accessible, and by whom? How will the
data be stored and protected over its life? As we mentioned previously, data purging
needs to happen according to the rules set forth. In addition, understanding what
your systems’ backup and retention policies are, is important.

Guidance 4: Define data policies
It’s important to document how data will be managed and shared. Identify the licens‐
ing and sharing agreements that pertain to the data you’re collecting. Are there
restrictions that the organization should adhere to? What are the legal and ethical
restrictions on access to and use of sensitive data, for example? Regulations like
GDPR and CCPA can easily get confusing and can even become contradictory. In this

Data Life Cycle Management | 91



step, ensure that all the applicable data policies are captured accordingly. This also
helps in case you’re audited.

Guidance 5: Define roles and responsibilities
Chapter 3 defined roles and responsibilities. With those roles in mind, determine
which are the right ones for your organization and what each one means for you.
Which teams will be responsible for metadata management and data discovery? Who
will ensure governance policies are followed all the way? And there are many more
roles that you can define.
A DMP should provide your organization with an easy-to-follow roadmap that will
guide others and explain how data will be treated throughout its life cycle. Think of
this as a living document that evolves with your organization as new datasets are cap‐
tured, and as new laws and regulations are enacted.
If this was a data management plan for a research project, it would have included a
lot more steps and items for consideration. Those plans tend to be more robust
because they guide the entire research project and data end-to-end. We will cover a
lot more concepts later in the chapter, so we chose to select items that were easily
transferable to creating a governance policy and plan for your organization.

Applying Governance over the Data Life Cycle
We’ve gone through fundamental concepts thus far; now let’s bring everything
together and look at how you can apply governance over the data life cycle. Gover‐
nance needs to bring together people, processes, and technology to govern data
throughout its life cycle. In Chapter 2, we outlined a robust set of tools to make gov‐
ernance a reality, and Chapter 3 focused on the people and process side of things. It’s
important to point out that implementing governance is complicated; there’s no easy
way to simply apply everything and consider he job done. Most technologies need to
be stitched together, and as you can imagine, they’re all coming from different ven‐
dors with different implementations. You would need to integrate the best-in-class
suite of products and services to make things work. Another option is to purchase a
fully integrated data platform or governance platform. This is not a trivial task.

Data Governance Framework
Frameworks help you visualize the plan, and there are several frameworks that can
help you think about governance across the data life cycle. Figure 4-2 is one such
framework in which we highlight all the concepts from Chapter 2, overlaid with the
concepts we’ve discussed in this chapter.

92 | Chapter 4: Data Governance over a Data Life Cycle



Figure 4-2. Governance over a data life cycle

This framework oversimplifies things to make them easier to understand; it assumes
things are linear, from left to right, which is usually not the case. When data is inges‐
ted from various sources on the left, this is simply at the point of data creation or cap‐
ture. That data is then processed and stored, and then it is consumed by the different
stakeholders, including data analysts, data engineers, data stewards, and so on.
Data archiving and data destruction are not reflected in this framework because those
take place beyond the point when data is used. As we previously outlined, during
archiving, data is removed from all active production environments. It is no longer
processed, used, or published but is stored in case it is needed again in the future.
Destruction is when data comes to the end of its life and is removed according to
guidelines and procedures that have been set forth.
One discrepancy that you will quickly notice is that metadata management should be
considered from the point of data creation—where enterprises need to discover and
curate the data as it’s ingested (especially for sensitive data)—to when data is stored
and discovered in the applicable storage system. Archiving, even though mentioned
within data management, tends to happen when the data’s usefulness is done and it is
removed from production environments. Though archiving is an important part of
governance, this diagram implies that it is taking place in the middle of the data life
cycle. That said, it’s also possible to have an archiving strategy when data is simply
stored in the applicable storage systems, so we cannot completely rule this out.

Applying Governance over the Data Life Cycle | 93



We want to reiterate that Figure 4-2 provides a logical representation of the phases a
piece of data goes through, from left to right, and not necessarily the actual step-by-
step flow of the data. There’s a lot of back and forth that happens between each phase,
and not all pieces of data go through each one of these phases.
Frameworks are good at providing a holistic view of things. However, they are not the
be-all, end-all. Make sure whichever framework you select works for your organiza‐
tion and your data.

We’ve mentioned it already, but would like to emphasize again the
idea of selecting a framework that work for your organization. This
can include considerations around the kind of data you collect or
work with, as well as what kind of personnel you have dedicated to
your data governance efforts. One thing we challenge you to con‐
sider is how to take what you have and fit enough framework
around it. Take these ideas as laid out (noting that not each step is
required or even necessary) and layer on what you have to work
with currently as a place to start. More pieces (and people, for that
matter) can be added later, but if you focus on at least laying the
groundwork—the foundation—you will be in a much better posi‐
tion if or when you do have more pieces to add to your framework.

Data Governance in Practice
OpenStreetMap (OSM) was created by Steve Coast in the UK in 2004 and was
inspired by the success of Wikipedia. It is open source, which means it is created by
people like you and is free to use under an open license. It was a response to the pro‐
liferation of siloed, proprietary international geographical data sources and dozens of
mapping software products that didn’t talk to each other. OSM has grown signifi‐
cantly to over two million contributors, and what’s amazing is that it works. In fact, it
works well enough to be the trusted source of data for a number of Fortune 500 com‐
panies, including other small and medium-sized businesses. With so many contribu‐
tors, OSM is successful because it was able to establish data standards early in the
process and ensured contributors adhered to them. As you can imagine, a crowd‐
sourced mapping system without a way to standardize contributor data could go
wrong very quickly. Defining governance standards can bring value to your organiza‐
tion and provide trusted data for your users.
And now that you have an understanding of the data life cycle with an overlay of the
different governance tools, let’s delve further into how the different data governance
tools we outlined in Chapters 1 and 2 can be applied and used across this life cycle.
This section also includes best practices, which can help you start to define your
organization’s data standards.

94 | Chapter 4: Data Governance over a Data Life Cycle



Data creation
As previously mentioned, this is the initial phase of the data life cycle, where data is
created or captured. During this phase, an organization can choose to capture both
the metadata, and the lineage of the data. Metadata describes the data, while the line‐
age describes the where of the data and how it will flow and be transformed and used
downstream. Trying to capture these during this initial phase sets you up well for the
later phases.
In addition, processes such as classification and profiling can be employed, especially
if you’re dealing with sensitive data assets. Data should also be encrypted in transit to
offer protection from intrusions and attacks. Cloud service providers such as Google
Cloud offer encryption in transit and at rest by default.

Define the Type of Data
Establish a set of guidelines for categorizing data that takes into
account the sensitivity of the information as well its criticality and
value to the organization. Profiling and classifying data helps
inform which governance policies and procedures apply to the
data.

Data processing
During this phase, data goes through processes such as integration, cleaning, scrub‐
bing, or extract-transform-load (ETL) prior to its use, to get it ready for storage and
eventual analysis. It’s important that the integrity of the data is preserved during this
phase; that is why data quality plays a critical role.
Lineage needs to be captured and tracked here also, to ensure that the end users
understand which processes led to which transformation and where the data origina‐
ted from. We heard this from one user: “It would be nice to have a better understand‐
ing of the lineage of data. When finding where a certain column in a table comes
from, I need to manually dig through the source code of that table and follow that
trail (if I have access). Automate this process.” This is a common pain point felt by
many, and one in which DLM and governance are critical.

Document Data Quality Expectations
Different data consumers may have different data quality require‐
ments, so it’s important to provide a means to document data qual‐
ity expectations while the data is being captured and processed, as
well as techniques and tools for supporting the data’s validation and
monitoring as it goes through the data life cycle. The right pro‐
cesses for data quality management will provide measurable and
trustworthy data for analysis.

Applying Governance over the Data Life Cycle | 95



Data storage
In this phase, both data and metadata are stored and made ready for analysis. Data
should be encrypted at rest to protect it from intrusions and attacks. In addition, data
needs to be backed up to ensure redundancy.

Automated Data Protection and Recovery
Because data is stored in storage devices in this phase, find solu‐
tions and products that provide automated data protection to
ensure that exposed data cannot be read, including encryption at
rest, encryption in transit, data masking, and permanent deletion.
In addition, implement a robust recovery plan to protect your busi‐
ness when a disaster strikes.

Data usage
In this phase, data is analyzed and consumed for insights and consumed by multiple
internal and external stakeholders and processes in the organization. In addition,
analyzed data is visualized and used to support the organization’s objectives and oper‐
ations; business intelligence tools play a critical role in this phase.
A data catalog is vital to helping users discover data assets using captured metadata.
Privacy, access management, and auditing are paramount at this stage, which ensures
that the right people and systems are accessing and sharing the data for analysis. Fur‐
thermore, there might be regulatory or contractual constraints on how data may
actually be used, and part of the role of data governance is to ensure that these con‐
straints are observed.

Data Access Management
It is important to provide data services that allow data consumers
to access their data with ease. Documenting what and how the data
will be used, and for what purposes, can help you define identities,
groups, and roles and assign access rights to establish a level of
managed access. This ensures that only authorized and authentica‐
ted individuals and systems are able to access data assets according
to defined rules.

Data archiving
In this phase, data is removed from all active production environments. It is no
longer processed, used, or published but is stored in case it is needed again in the
future. Data classification should guide the retention and disposal method of data.

96 | Chapter 4: Data Governance over a Data Life Cycle



Automated Data Protection Plan
Beyond being a way to prevent unauthorized individuals from
accessing data, perimeter security is not and never has been suffi‐
cient for protecting data. The same protections applied in data stor‐
age would apply here as well to ensure that exposed data cannot be
read, including encryption at rest, data masking, and permanent
deletion. In addition, in case of a disaster, or in the event that
archive data is needed again in a production environment, it’s
important to have a well-defined process to revive this data and
make it useful.

Data destruction
Finally, data is destroyed, or rather, it is removed from the enterprise at the end of its
useful life. Before purging any data, it is critical to confirm whether there are any pol‐
icies in place that would require you to retain the data for a certain period of time.
Data classification should guide the retention and disposal method of data.

Create a Compliance Policy
Coming up with the right timeline for this cycle means under‐
standing state and federal regulations, industry standards, and gov‐
ernance policies and staying up to date on any changes. Doing so
helps to ensure that the right steps are taken and that the purge has
been done properly. It also ensures that data doesn’t consume more
resources than necessary at the end of its useful life.
IT stakeholders are urged to revisit the guidelines for destroying
data every 12–18 months to ensure compliance, since rules change
often.

Example of How Data Moves Through a Data Platform
Here’s an example scenario of how data could move through a data platform with the
framework in Figure 4-2.

Scenario
Let’s say that a business wants to ingest data onto a cloud-data platform, like Google
Cloud, AWS, or Azure, and share it with data analysts. This data may include sensi‐
tive elements such as US social security numbers, phone numbers, and email
addresses. Here are the different pieces it might go through:

Applying Governance over the Data Life Cycle | 97



1. Business configures an ingestion data pipeline using a batch or streaming service:
a. Goal: As they move raw data into the platform, it will need to be scanned,

classified, and tagged before it can be processed, manipulated, and stored.
b. Staged ingestion buckets:

i. Ingest: heavily restricted
ii. Released: processed data

iii. Admin quarantine: needs review
2. Data is then scanned and classified for sensitive information such as PII.
3. Some data may be redacted, obfuscated, or anonymized/de-identified. This pro‐

cess may generate new metadata, such as what keys were used for tokenization.
This metadata would be captured at this stage.

4. Data is tagged with PII tags/labels.
5. Aspects of data quality can be accessed—that is, are there any missing values, are

primary keys in the right format, etc.
6. Start to capture data provenance information for lineage.
7. As data moves between the different services along the life cycle, it is encrypted

in transit.
8. Once ingestion and processing are complete, the data will need to be stored in a

data warehouse and/or a data lake, where it is encrypted at rest. Backup and
recovery processes need to be employed as well, in case of a disaster.

9. While in storage, additional business and technical metadata can be added to the
data and cataloged, and users need to be able to discover and find the data.

10. Audit trails need to be captured throughout this data life cycle and made visible
as needed. Audits allow you to check the effectiveness of controls in order to
quickly mitigate threats and evaluate overall security health.

11. Throughout this process, it is important to ensure that the right people and serv‐
ices have access and permissions to the right data across the data platform using a
robust identity and access management (IAM) solution.

12. You need to be able to run analytics and visualize the results for use. In addition
to access management, additional privacy, de-identification, and anonymization
tools may be employed.

13. Once this data is no longer needed in a production environment, it is archived
for a determined period of time to maintain compliance.

14. At the end of its useful life, it is completely removed from the data platform and
destroyed.

98 | Chapter 4: Data Governance over a Data Life Cycle



Case Study: Nike and the 4% Improving Running Shoes
In 2018, Nike launched a new running shoe, and the related advertising campaign
claimed that the “Nike Zoom Vaporfly 4%” will make you run 4% faster. While a 4%
speed increase does not sound like a lot, over a marathon run that averages to 4–5
hours, this can potentially lead to 10–12 minutes’ improvement in the time it takes to
finish the race.
The claims were significant and were met with skepticism. A Nike-sponsored study
did not help because it was based, by necessity, on a small dataset. Many athletes
would pay for an expensive shoe to improve their result by that margin, if the claim
were true, but the source of this information (the vendor) sowed doubt. Running an
independent scientific experiment to conclude whether or not the claim was true
would have been challenging, as this would have required significant investment, and
getting the runner(s) to use different kinds of shoes over the same courses in the same
conditions to truly eliminate all possible variables and challenges.
Happily, many athletes use a popular record-keeping app called Strava. Strava makes
the data publicly available, and many athletes also record the shoes they use when
running (Figure 4-3). This created a natural experiment in which you could look over
existing data and, with enough data, could potentially tease out patterns.

Figure 4-3. Strava data. Does the Nike Vaporfly make you run 4% faster?

The New York Times investigated, collecting half a million real-life performance
records from Strava.Keven Quealy and Josh Katz, “Nike Says Its $250 Running Shoes
Will Make You Run Much Faster. What If That’s Actually True?” New York Times, July
18, 2018. The next step was to determine whether or not the data was useful. While
the ideal way would have been to measure runs of identical runners on the same
course with different shoes, this was not possible at this scale. However, the large

Applying Governance over the Data Life Cycle | 99



amount of data did enable the finding of natural experiments.2 This was not a small-
scale lab experiment, but an actual account of (admittedly amateur, for the most part)
weekend runners reporting and sharing their race results.
The New York Times was able to compile 280,000 marathon and 215,000 half mara‐
thon results, and then compared running conditions (same races) and concurrent
results from the same athletes (different shoes, different races or dates). These com‐
parisons ensured that conditions similar to the ideal experiment were met, and by
including knowledge about the races themselves (weather, course layout and diffi‐
culty), the data was curated to keep the quality records in while eliminating outliers
(less-frequented races, extreme conditions).
The New York Times was able to conclude that the Nike shoes were very often part of
a more successful run for many runners. The paper pointed out that these results
were not gathered through a lab experiment in controlled conditions, but their results
were consistent with the Nike-funded study.
This effort would not have been possible without the availability of an open dataset,
contributed to freely by runners globally, and preserved and made accessible to
researchers. This example of a dataset in which data is made available under a con‐
trolled environment (Strava does protect runners’ personal data and allows runners
full control over how their data is shared, including the ability to opt out and delete
their own data) is an excellent example of proper information cycle and data
governance.

Operationalizing Data Governance
It’s one thing to have a plan, but it’s something else to ensure that plan works for your
organization. NASA learned things the hard way. In September 1999, after almost 10
months of travel to Mars, the $125-million Mars Climate Orbiter lost communication
and then burned and broke into pieces a mere 37 miles away from the planet’s sur‐
face. The analysis found out that, while NASA had used the metric system, one of its
partners had used the International System of Units (SI). This inconsistency was not
discovered until it was time to land the orbiter, leading to a complete loss of the satel‐
lite. This of course was crushing to the team. After this incident, proper checks and
balances were implemented to ensure that something similar did not happen again.
Bringing things together so that issues such as the one NASA experienced are caught
early and rectified before a disaster happens starts with the creatiion of a data gover‐

2 A natural experiment is a situation in which you can identify experimental and control groups determined by
factors outside the control of the investigators. In our example here, the runners naturally fell into groups
defined by the shoes they wore, rather than being assigned shoes externally. The groups of runners were large
enough to qualify for good “experimental” and “control” groups with controlled number of external factors.

100 | Chapter 4: Data Governance over a Data Life Cycle



nance policy. A data governance policy is a living, breathing document that provides
a set of rules, policies, and guidance for safeguarding an organization’s data assets.

What Is a Data Governance Policy?
A data governance policy is a documented set of guidelines for ensuring that an
organization’s data and information assets are managed consistently and used prop‐
erly. A data governance policy is essential in order to implement governance. The guide‐
lines will include individual policies for data quality, access, security, privacy, and
usage, which are paramount for managing data across its life cycle. In addition, data
governance policies center on establishing roles and responsibilities for data that
include access, disposal, storage, backup, and protection, which should all be familiar
concepts. This document helps to bring everything together toward a common goal.
The data governance policy is usually created by a data governance committee or data
governance council, which is made up of business executives and other data owners.
This policy document defines a clear data governance structure for the executive
team, managers, and line workers to follow in their daily operations.
To get started operationalizing governance, a data governance charter template could
be useful. Figure 4-4 shows an example template that could help you socialize your
ideas across the organization and get the conversation started. Information in this
template will funnel directly into your data governance policy.
Use the data governance charter template to kick off the conversation and get your
team assembled. Once it has bought into your vision, mission, and goals, that is the
team that will help you create and define your governance policy.

Operationalizing Data Governance | 101



Figure 4-4. Data governance charter template

Importance of a Data Governance Policy
When you have a business idea and are going to friends to socialize the idea and pos‐
sibly get them to buy into it, you will quickly run into someone who asks for a busi‐
ness plan. “Do you have a business plan you can share so I can read more about this
idea and what your plans are?” A data governance policy allows you to have all the
important elements of operationalizing governance documented according to your
organization’s needs and objectives. It also allows consistency within the organization
over a long period of time. It is the document that everyone will refer to when ques‐
tions and issues arise. It should be reviewed regularly and updated when things in the
organization change. You can consider it your business plan—or to another extreme,
it can also be your governance bible.
When a data governance policy is well drafted, it will ensure:

• Consistent, efficient, and effective management of the data assets throughout the
organization and data life cycle and over time.

• The appropriate level of protection of the organization’s data assets based on their
value and risk as determined by the data governance committee.

• The appropriate protection and security levels for different categories of data as
established by the governance committee.

102 | Chapter 4: Data Governance over a Data Life Cycle



Developing a Data Governance Policy
A data governance policy is usually authored by the data governance committee or
appointed data governance council. This committee will establish comprehensive
policies for the data program that outline how data will be collected, stored, used, and
protected. The committee will identify risks and regulatory requirements and look
into how they will impact or disrupt the business.
Once all the risks and assessments have been identified, the data governance commit‐
tee will then draft policy guidelines and procedures that will ensure the organization
has the data program that was envisioned. When a policy is well written, it helps cap‐
ture the strategic vision of the data program. The vision for the governance program
could be to drive digital transformation for the organization, or possibly to get
insights to drive new revenue or even to use data to provide new products or services.
Whichever is the case for your organization, the policies drafted should all coalesce
toward the articulated vision and mission as outlined in the data governance charter
template.
Part of the process of developing a data governance policy is establishing the expecta‐
tions, wants, and needs of key stakeholders through interviews, meetings, and infor‐
mal conversations. This will help you get valuable input, but it’s also an opportunity
to secure additional buy-in for the program.

Data Governance Policy Structure
A well-crafted policy should be unique to your organization’s vision, mission, and
goals. Don’t get hung up on every single piece of information on this template, how‐
ever; use it more like a guide to help you think things through. With that in mind,
your governance policy should address:
Vision and mission for the program

If you used a data governance charter template as outlined in Figure 4-4 to get
buy-in from other stakeholders, that means you already have this information
readily available. As mentioned before, the vision for the governance program
could be to drive digital transformation for the organization, or to get insights to
drive new revenue, or even to use data to provide new products or services.

Policy purpose
Capture goals for your organization’s data governance program, as well as metrics
for determining success. The mission and vision of the program should drive the
goals and success metrics.

Policy scope
Document the data assets covered by this governance policy. In addition, inven‐
tory the data sources and determine data classifications based on whether data is

Operationalizing Data Governance | 103



sensitive, confidential, or publicly available, along with the levels of security and
protection required at the different levels.

Definitions and terms
The data governance policy is usually viewed by stakeholders across the organi‐
zation who might not be familiar with certain terms. Use this section to docu‐
ment terms and definitions to ensure everyone is on the same page.

Policy principles
Define rules and standards for the governance program you’re looking to set up
along with the procedures and programs to enforce them. The rules could cover
data access (who has access to what data), data usage (how the data will be used
and details around what’s acceptable), data integration (what transformations the
data will undergo), and data integrity (expectations around data quality).
Develop best practices to protect data and to ensure regulations and compliance
are effectively documented.

Program structure
Define roles and responsibilities (R&Rs), which are positions within the organi‐
zation that will oversee elements of the governance program. A RACI chart could
help you map out who is responsible, who is accountable, who needs to be con‐
sulted, and who should be kept informed about changes. Information on gover‐
nance R&Rs can be found in Chapter 3 of the book.

Policy review
Determine when the policy will be reviewed and updated and how adherence to
the policy will be monitored, measured, and remedied.

Further assistance
Document the right people to address questions from the team and other stake‐
holders.

It’s not enough to document a data governance policy as outlined in Figure 4-5, com‐
municating it to all stakeholders is equally important. This could happen through a
combination of group meetings and training, one-on-one conversations, recorded
training videos, and written communication.

104 | Chapter 4: Data Governance over a Data Life Cycle



Figure 4-5. Example data governance policy template

In addition, review performance regularly with your data governance team to ensure
that you’re still on the right track. This also means regularly reviewing your data gov‐
ernance policy to make sure it still reflects the current needs of the organization and
program.

Roles and Responsibilities
When operationalizing governance over a data life cycle, you will interact with many
stakeholders within the organization, and you will need to bring them together to
work on this common goal. While it might be tempting to definitively say which roles
do what at which part of the data life cycle, as outlined in Chapter 3, many data gov‐
ernance frameworks revolve around a complex interplay of roles and responsibilities.
The reality is that most companies rarely are able to exactly or fully staff governance
roles due to lack of employee skill set or, more commonly, a simple lack of headcount.
For this reason, employees working in the information and data space of their com‐
pany often wear different user “hats.”
We will not go into detail about roles and responsibilities in this chapter, because
they’re well outlined in Chapter 3. You still need to define what these look like within
your organization and how they will interplay with each other to make governance a
reality for you. This will typically be outlined in a RACI matrix describing who is
“responsible, accountable, to be consulted, and to be informed” within a certain
enforcement, process, policy, or standard.

Operationalizing Data Governance | 105



Step-by-Step Guidance
By this section of the book, you should know that data governance goes beyond the
selection and implementation of products and tools. The success of a data governance
program depends on the combination of people, processes, and tools all working
together to make governance a reality. This section will feel very familiar, because it
gathers all the elements discussed in the previous section on data governance policy
and puts them in a step-by-step process to show you how to get started. It further
double clicks into the concepts as well.
Build the business case

As previously mentioned, data governance takes time and is expensive. If done
correctly, it can be automated as part of the application design done at the source
with a focus on business value. That said, data governance initiatives will often
vary in scope and objectives. Depending on where the initiative is originating
from, you need to be able to build a business case that will identify critical busi‐
ness drivers and justify the effort and investment of data governance. It should
identify the pain points, outline perceived data risks, and indicate how gover‐
nance helps the organization mitigate those risks and enable better business out‐
comes. It’s OK to start small, strive for quick wins, and build up ambitions over
time. Set clear, measurable, and specific goals. You cannot control what you can‐
not measure; therefore you need to outline success metrics. The data governance
charter template in Figure 4-4 is perfect for helping you get started.

Document guiding principles
Develop and document core principles associated with governance and, of
course, associated with the project you’re looking to get off the ground. A core
principle of your governance strategy could be to make consistent and confident
business decisions based on trustworthy data aligned with all the various pur‐
poses for the use of the data assets. Another core principle could be to meet regu‐
latory requirements and avoid fines or even to optimize staff effectiveness by
providing data assets that meet the desired data quality thresholds. Define princi‐
ples that are core to your business and project. If you’re still new to this area,
there are a lot of resources available. If you are looking online, there are several
vendor-agnostic, not-for-profit associations, such as the Data Governance Insti‐
tute (DGI), the Data Management Association (DAMA), the Data Governance
Professionals Organization (DGPO), and the Enterprise Data Management
Council, all of which provide great resources for business, IT, and data professio‐
nals dedicated to advancing the discipline of data governance. In addition, iden‐
tify whether there are any local data governance meetup groups or conferences
that you can possibly attend, such as the Data Governance and Information
Quality Conference, DAMA International Events, or a Financial Information
Summit.

106 | Chapter 4: Data Governance over a Data Life Cycle



Get management buy-in
It should be no surprise that without management buy-in, your governance ini‐
tiative can easily be dead from the get-go. Management controls the big decisions
and funding that you need. Outlining important KPIs, and how your plan helps
to move them, will get management to be all ears. Engage data governance cham‐
pions and get buy-in from the key senior stakeholders. Present your business
case and guiding principles to C-level management for approval. You need allies
on your side to help make the case. And once the project has gotten off the
ground, communicate frequently.

Develop an operating model
Once you have management approval, it’s time to get to work. How do you inte‐
grate this governance plan into the way of doing business in your enterprise? We
introduced you to the data governance policy, which can come in very handy
during this process. During this stage, define the data governance roles and
responsibilities, and then describe the processes and procedures for the data gov‐
ernance council and data stewardship teams who will define processes for defin‐
ing and implementing policies as well as for reviewing and remediating identified
data issues. Leverage the content from the data management policy plan to help
you define your operating model. Data governance is a team sport, with delivera‐
bles from all parts of the business.

Develop a framework for accountability
As with any project you’re looking to bring to market, establishing a framework
for assigning custodianship and responsibility for critical data domains is para‐
mount. Define ownership. Make sure there is visibility to the “data owners”
across the data landscape. Provide a methodology to ensure that everyone is
accountable for contributing to data usability. Refer back to your data manage‐
ment policy, as it probably started to capture some of these dependencies.

Develop taxonomies and ontologies
This is where a lot of the education you’ve collected thus far comes in handy.
Working closely with governance associations, leaning in on your peers, and sim‐
ply learning about things online will help you with this step. There may be a
number of governance directives associated with data classification, organization,
and, in the case of sensitive information, data protection. To enable your data
consumers to comply with those directives, there must be a clear definition of the
categories (for organizational structure) and classifications (for assessing data
sensitivity). These should be captured in your data governance policy.

Assemble the right technology stack
Once you’ve assigned data governance roles to your staff and defined and
approved your processes and procedures, you should then assemble a suite of
tools that facilitates implementation and ongoing validation of compliance with

Operationalizing Data Governance | 107



data policies and accurate compliance reporting. Map infrastructure, architec‐
ture, and tools. Your data governance framework must be a sensible part of your
enterprise architecture, the IT landscape, and the tools needed. We talked about
technology in previous sections, so we won’t go into detail about it here. Finding
tools and technology that work for you and satisfy the organizational objectives
you laid out is what’s important.

Establish education and training
As highlighted earlier, for data governance to work, it needs buy-in across the
organization. You need to ensure that your organization is keeping up and is still
buying into the project you presented. It’s therefore important to raise awareness
of the value of data governance by developing educational materials highlighting
data governance practices, procedures, and the use of supporting technology.
Plan for regular training sessions to reinforce good data governance practices.
Wherever possible, use business terms, and translate the academic parts of the
data governance discipline into meaningful content in the business context.

Considerations for Governance Across a Data Life Cycle
Data governance has been around since there was data to govern, but it was mostly
viewed as an IT function. Implementing data governance across the data life cycle is
no walk in the park. Here are some considerations you will need to think about as
you implement governance in your organization. These should not be surprising to
you, because you will quickly notice that they touch on a lot of aspects we introduced
in Chapters 1 and 2, as well as in this chapter.

Deployment time
Crafting and setting up governance processes across the data life cycle takes a lot of
time, effort, and resources. In this chapter, we have introduced a lot of concepts,
ideas, and ways to think about operationalizing governance across the data life cycle,
and you can see it gets overwhelming very quickly. There’s not a one-size-fits-all solu‐
tion; you need to identify what is unique about your business and then forge a plan
that works for you. Automation can reduce the deployment time compared with
hand-coded governance processes. In addition, artificial intelligence is seen as a way
to get arms around data governance in the future, especially for things like autodis‐
covery of sensitive data and metadata management. That means that as you look for
solutions in the market, you will need to find out how much automation and integra‐
tion is built into it, how well it works for your environment and situation, and
whether that is the most difficult part of the workflow that could use automation. In a
hybrid or even a multi-cloud world, this becomes even more complex and further
increases the deployment time.

108 | Chapter 4: Data Governance over a Data Life Cycle



Complexity and cost
Complexity comes in many forms. In Chapter 1, we talked about how much the data
landscape is and about just how quickly data was being produced in the world.
Another complexity is a lack of defined industry standards for things like metadata.
We touched on this in Chapter 2. In most cases, metadata does not obey the same
policies and controls as the underlying data itself, and a lack of standardized meta‐
data specifications means that different products and processes will have different
ways of presenting this information. Still another complexity is the sheer amount of
tools, processes, and infrastructure needed to make governance a reality. In order to
deliver comprehensive governance, organizations must either integrate best-of-breed
solutions, which are often complex and very expensive (with high license and mainte‐
nance costs), or buy turnkey, integrated solutions, which are expensive and fewer in
the market. With this in mind, cloud service providers (CSPs) are building data plat‐
forms with all these governance capabilities built in, thus creating a one-stop shop
and simplifying the process for customers. As an organization, research and compare
the different data platforms provided by CSPs and see which one works for you. Some
businesses choose to leave some of their data on-premises; however, for the data that
can move to the cloud, these CSPs are now building robust tools and processes to
help customers govern their data end-to-end on the platform. In addition, companies
such as Informatica, Alation, and Collibra offer governance-specific platforms and
products that can be implemented in your organization.

Changing regulation environment
In previous chapters, we’ve clearly outlined the implications of a constantly changing
regulatory environment with the introduction of GDPR and CCPA. We will not go
into the same detail here; however, regulations define a lot of what must be done and
implemented to ensure governance. They will outline how certain types of data need
to be handled and which types of controls need to be in place, and they sometimes
will even go as far as outlining what the repercussions are when these things are not
complied with. Complying with regulations is absolutely something your organiza‐
tion needs to think about as you implement data governance over the data life cycle.

Operationalizing Data Governance | 109



In our discussions with many different companies, we’ve heard of
two very different philosophies when it comes to considering
changes to the regulatory environment. One strategy is to assume
that, in the future, the most restrictive regulations that are present
now will cascade and be required everywhere (like CCPA being
required across the entire US and not just in California), and that
ensuring compliance now, even though not required, is a top prior‐
ity. Conversely, we’ve also heard the strategy of complying only
with what’s required right now and dealing with regulations only if
they become required. We strongly suggest you take the former
approach, because a proper and well-thought-out governance pro‐
gram not only ensures compliance with ever-changing regulations;
it also enables many of the other benefits we’ve outlined thus far,
such as better findability, better security, and more accurate analyt‐
ics from higher-quality data.

Location of data
In order to fully implement governance over a data life cycle, understanding which
data is on-premises versus in the cloud is very important. Furthermore, understand‐
ing how data will interact with other data along the life cycle does create complexity.
In the current paradigm, most organizational data lives both on-premises and in the
cloud, and having systems and tools that allow for hybrid and even multicloud sce‐
narios is paramount. In Chapter 1, we talked about why governance is easier in the
public cloud—it’sprimarily because the public cloud has several features that make
data governance easier to implement, monitor, and update. In many cases, these fea‐
tures are unavailable or cost-prohibitive in on-premises systems. Data should be pro‐
tected no matter where it is located, so a viable data life cycle management plan will
incorporate governance for all data at all times.

Organizational culture
As you know, culture is one of those intangible things in an organization that plays an
important role in how the organization functions. In Chapter 3, we touched on how
an organization can create a culture of privacy and security, which allows employees
to understand how data should be managed and treated so that they are good stew‐
ards of proper data handling and usage. In this section, we’re referring to organiza‐
tional culture, which often dictates what people do and how they behave. Your
organization might be free, allowing folks to easily raise questions and concerns, and
in such an environment, when something goes wrong, people are more likely to speak
up. In organizations in which people are reprimanded for every little thing, they will
be more afraid to speak up and report when things are not working or even when
things go wrong. In these environments, governance is a little difficult to implement,
because without transparency and proper reporting, mistakes are usually not discov‐
ered until much later. In the NASA example we provided earlier in this chapter, there

110 | Chapter 4: Data Governance over a Data Life Cycle



were a couple of people within the organization who noticed the discrepancy in the
data and even reported it. Their reports were ignored by management, and we all
know what happened. Things did not end well for NASA. Remember, instituting gov‐
ernance in an organization is often met with resistance, especially if the organization
is accustomed to decentralized operations. Creating an environment in which func‐
tions are centralized across the data life cycle simply means that these areas have to
adhere to processes that they might not have been used to in the past but that are for
the larger good of the organization.

Summary
Data life cycle management is paramount to implementing governance and ensures
that useful data is clean, accurate, and readily available to users. In addition, it
ensures that your organization remains compliant at all times.
In this chapter, we introduced you to data life cycle management, and how to apply
governance over the data life cycle. We then looked into operationalizing governance
and how the role of a data governance policy is to ensure that an organization’s data
and information assets are managed consistently and used properly. Finally, we pro‐
vided step-by-step guidance for implementing governance and finished with the con‐
siderations for governance across the data life cycle, including deployment time,
complexity and cost, and organizational culture.

Summary | 111






CHAPTER 5
Improving Data Quality

When most people hear the words data quality, they think about data that is correct
and factual. In data analytics and data governance, data quality has a more nuanced
set of qualifiers. Being correct is not enough, if all of the details are not available (e.g.,
fields in a transaction). Data quality is also measured in the context of a use case, as
we will explain. Let’s begin by exploring the characteristics of data quality.

What Is Data Quality?
Put simply, data quality is the ranking of certain data according to accuracy, com‐
pleteness (all columns have values), and timeliness. When you are working with large
amounts of data, the data is usually acquired and processed in an automated way.
When thinking about data quality, it is good to discuss:
Accuracy

Whether the data captured was actually correct. For example, an error in data
entry causing multiple zeros to be entered ahead of a decimal point, is an accu‐
racy issue. Duplicate data is also an example of inaccurate data.

Completeness
Whether all records captured were complete—i.e., there are no columns with
missing information. If you are managing customer records, for example, make
sure you capture or otherwise reconcile a complete customer details record (e.g.,
name/address/phone number). Missing fields will cause issues if you are looking
for customer records in a specific zip code, for example.

Timeliness
Transactional data is affected by timeliness. The order of events in buying and
selling shares, for example, can have an impact on the buyer’s available credit.
Timeliness also should take into account the fact that some data can get stale.

113



In addition, the data quality can be affected by outlier values. If you are looking at
retail transactions, for example, very large purchase sums are likely indications of
data-entry issues (e.g., forgetting a decimal point) and not indicators that revenues
went up by two orders of magnitude. This will be an accuracy issue.
Make sure to take all possible values into account. In the above retail example, nega‐
tive values are likely indications of returns and not “purchasing a product for nega‐
tive $” and should be accounted for differently (e.g., a possible impact will be the
average transaction size—with the purchase and the return each accounting for a sin‐
gle purchase).
Finally, there is the trustworthiness of the source of the data. Not all data sources are
equal—there is a difference, for example, between a series of temperature values col‐
lected over time from a connected thermometer and a series of human reads of a
mercury thermometer collected over time in handwriting. The machine will likely
control variables such as the time the sample was taken and will sync to a global
atomic clock. The human recording in a notebook will possibly add variance to the
time the sample was taken, may smudge the text, or may have hard-to-read handwrit‐
ing. It is dangerous to take data from both sources and treat them as the same.

Why Is Data Quality Important?
For many organizations, data directly leads to decision making: a credit score com‐
piled from transaction data can lead to a decision by a banker to approve a mortgage.
A company share price is instantly computed from the amount offered by multiple
buyers and sellers. These kinds of decisions are very often regulated—clear evidence
must be collected on credit-related decisions, for example. It is important to the cus‐
tomer and the lender that mortgage decisions are made according to high-quality
data. Lack of data quality is the source of lack of trust and of biased, unethical auto‐
mated decisions being made. A train schedule you cannot trust (based on erroneous
or untimely station visits and past performance) can lead to you making decisions
about your commute that can potentially result in your always taking your own car,
thus negating the very reason for the existence of the mass transit train.
When collecting data from multiple sources or domains, data accuracy and context
become a challenge: not only is it possible that the central repository does not have
the same understanding of the data as the data sources (e.g., how outliers are defined
and how partial data is treated), but it is also possible that data sources do not agree
with each other on the meaning of certain values (e.g., how to handle negative values,
or how to fill in for missing values). Reconciliation of data meaning can be done by
making sure that when a new data source is added, the accuracy, completeness, and
timeliness of the data in the data source is examined—sometimes manually—and
either described in a way that the data analysts using the resource can use or directly
normalized according to the rules in the central repository.

114 | Chapter 5: Improving Data Quality



When an error or unexpected data is introduced into the system, there are usually no
human curators who will detect and react to it. In a data processing pipeline, each
step can introduce and amplify errors in the next steps, until presented to the busi‐
ness user:

• In the data-gathering endpoints, gathering data from low-quality sources which
are potentially irrelevant to the business task can, if not eliminated early, cause
issues. Consider, for example, mobile ad impressions, or usage details, where
some of the data is gathered for an engineering lab source and does not represent
real users (and can potentially be very large in volume).

• In the landing zone step, upon normalization/aggregation, the wrong data can be
aggregated into a summation and sway the result.

• In the data analytics workload, joining tables of different qualities can introduce
unexpected outcomes.

All of the above can be presented in a way such that the business user is kept unaware
of decisions/operations happening earlier in the data acquisition chain (see
Figure 5-1) and is presented with the wrong data.

Figure 5-1. Simple data acquisition chain

Any step in the (very simple) chain shown in Figure 5-1 can result in erroneous data,
which will eventually drive the wrong business decisions. Let’s look at a couple of
examples.
In the early days of internet mapping, the mapping provider of the time, a company
called MaxMind, was the sole provider of IP addresses to location services. This com‐
pany made an arguably reasonable decision to have a “default” location right at the
center of the United States, in northern Kansas near the Nebraska border. From the
time of making this decision until very recently, whenever the service could not find a
map location for an IP address, it would provide this default location. The problem
with this decision became evident when systems and persons downstream did not
realize the meaning of this default, and when illegal activity was detected from

Why Is Data Quality Important? | 115



“default” (aka unknown) IP addresses—law enforcement would show up at this loca‐
tion in the central US and serve warrants to people who actually live there.1

A more current data quality challenge example is within California’s collection of
COVID-19 cases. California moved from rapid per-county reports to state data
(which caused an inconsistency). Later, California moved from “people tested” to
“specimens tested” (some people are likely tested more than once, causing another
data quality issue), and then in August 2020 the data quality became an even more
serious issue

after a series of errors—among them, a failed link to one of the country’s largest testing
labs—had led the state to underreport coronavirus cases starting in late July. Counties
were forced to comb spreadsheets in search of reliable data, while the state worked to
understand the scope of the problem and fix it. A total of 296,000 records were
affected.2

It took nearly a month to recover from this data-quality issue.
A study performed by Experian and published by MIT Sloan researchers estimates
that the cost of bad data (and by that they mean bad, or unmanaged data quality) is
around 15–20% of revenue for most companies.3 The study sampled one hundred
“records” (or units of work) from enterprise divisions and then manually calculated
the error range of these records. The result was an astonishing error rate of 50%. The
Data Warehousing Institute (TDWI) estimated in 2002 that poor data quality costs
businesses in the US over $700 billion annually, and since then this figure has grown
dramatically. Currently, IBM estimates that the yearly cost of “bad data” is $3.1
trillion.4

Data Quality in Big Data Analytics
Data warehouses—databases that are used to performing data analytics in petabyte
scale—are vulnerable to data quality issues. Typically, to get data in a big data ware‐
house, you will extract, clean, transform, and integrate data from multiple operational
databases to create a comprehensive database. A set of processes termed extract-
transform-load (ETL) is used to facilitate construction of data warehouses. The data
in the warehouses, though rarely updated, is refreshed periodically and is intended
for a read-only mode of operation. The desired use of the data also has an impact on

1 Kashmir Hill, “How an Internet Mapping Glitch Turned a Random Kansas Farm into a Digital Hell”, Splinter,
April 10, 2016.

2 Fiona Kelliher and Nico Savidge, “With Data Backlog Cleared, California Coronavirus Cases Officially
Decreasing, Newsom Says”, Mercury News, August 14, 2020.

3 Thomas Redman, “Seizing Opportunity in Data Quality”, MIT Sloan Management Review, November 27,
2017.

4 IBM, “The Four V’s of Big Data”, Big Data & Analytics Hub (blog).

116 | Chapter 5: Improving Data Quality



the kinds of analysis and desired uses of the data. A data warehouse built to support
decision making in retail, where transactions are collected hourly and rounded to the
nearest five-cent value, has different quality needs than a data warehouse built to sup‐
port stock trades, where the transaction time needs to be accurate to the
microsecond, and values range from microtransactions of below one cent to transac‐
tions spanning millions of dollars.
Compared to operational database environments, data warehouses pose additional
challenges to data quality. Since data warehouses integrate data from multiple sour‐
ces, quality issues related to data acquisition, cleaning, transformation, linking, and
integration become critical.

We touched on this earlier, but it’s important to keep in mind that
proper data quality management not only assists in the ability to
run big data analytics but also helps to save on cost and prevent the
loss of productivity. The harder it is for analysts to find and use
high-quality data, coupled with extra engineering time spent hunt‐
ing down and solving data issues, the more cost you will incur, and
your analytics output will suffer. The downstream effects and cost
implications of poorly managed data quality should not be dis‐
counted.

Data Quality in AI/ML Models
Of specific note is data quality within AI/ML models. To broadly generalize, machine
learning models work by extrapolating from existing data using a model to predict
future data (e.g., transaction volume). If the input data has errors within it, the
machine learning model will likely amplify these errors. If the machine learning
model is used to make predictions, and those predictions are then further input into
the model (once acted upon), the predictions become the reality, and the machine
learning model becomes compromised because it will generate, by force of a positive
feedback loop, more and more errors.
The data available for building machine learning models is usually divided into three
nonoverlapping datasets: training, validation, and test. The machine learning model is
developed using the training dataset. Next, the validation dataset is used to adjust the
model parameters so that overfitting is avoided. Last, the test dataset is used to evalu‐
ate the model performance. Errors in one or more datasets can lead to a badly trained
machine learning model, which will yield bad output. Note that there is a fine line
between cleaning all the errors out manually (which can be cost prohibitive over a
large amount of records) and allowing some level of error in a robust model.

Why Is Data Quality Important? | 117



Quality Beats Quantity, Even in AI
It is a truth universally acknowledged that an AI product manager in possession of a
good idea must be in want of copious amounts of data. The resurgence of AI, starting
in around 2014, has been driven by the ability to train ML models on ever-larger
datasets. The explosion of smartphones, the rise of ecommerce, and the prevalence of
connected devices are some of the trends that have fostered an explosion in the
amount of data that a business has available to it when designing new services or opti‐
mizing existing ones. With larger datasets comes the ability to use larger and more
sophisticated AI models. For example, a typical image classification model now has
hundreds of layers, whereas an AI model from the 1990s had only one layer. Such
models are practical because of the availability of custom ML hardware like GPUs and
TPUs, and the ability to distribute the work across many machines in the public
cloud. Thus, any AI product manager with a good idea will be on the hunt to use as
much data as possible. The accuracy of the AI model and its ability to represent the
real world depends on it being trained with the widest, most representative data
possible.
In our effort to gather more data, however, we should be careful to make sure that the
collected data is of good quality. A small amount of high-quality data yields much
better results than does a large amount of low-quality or outright wrong data. A fasci‐
nating example of this second truth (not as universally acknowledged) comes from an
effort to reduce the overhead of testing water meters in northern Italy.
The goal was to identify malfunctioning mechanical water meters based on their
readings alone. Going out to a site to test a water meter (see Figure 5-2) is pretty
expensive; if the team could use AI to identify potential malfunctions from the water
readings alone, it would be a huge cost savings. For example, if a water meter ran
backwards, or if the amount of water read as being consumed was wholly unreasona‐
ble (it was more, perhaps, than the amount of water supplied to the entire neighbor‐
hood), we could be sure the meter was faulty. Of course, this also depended on the
historical water usage at any meter—a water meter attached to a one-bath house with
a small garden would tend to consume a certain amount of water that varies season‐
ally. A large deviation from this amount would be suspicious.

118 | Chapter 5: Improving Data Quality



Figure 5-2. A mechanical water meter of the sort used in Italy, meant to be read by a
human looking at the dial. Photo courtesy of Andrevruas, Creative Commons License
3.0

The team started out with 15 million readings from 1 million mechanical water
meters. This was enough data to train a recurrent neural network (RNN), the fanciest
time-series prediction method (with less data, they’d have to settle for something like
ARIMA [an autoregressive integrated moving average model]), and so the team got to
work. The data was already numeric, so there was no need for any data preprocessing
—the team could just feed it into the RNN and do lots of hyperparameter tuning. The
idea was that any large differences between what the RNN predicted and what the
meter actually read would be attributable to faulty water meters.
How did that go? The team notes:

Our initial attempts to train a recurrent neural network, without a specific attention
to the quality, and to the limitations, of those data used for training, led to unexpec‐
ted and negative prediction outcomes.5

5 Marco Roccetti et al., “Is Bigger Always Better? A Controversial Journey to the Center of Machine Learning
Design, with Uses and Misuses of Big Data for Predicting Water Meter Failures”, Journal of Big Data 6, no. 70
(2019).

Why Is Data Quality Important? | 119



They went back to the drawing board and looked more closely at those 15 million
readings that they had. It turned out that there were two problems with the data: erro‐
neous data and made-up data.
First, some of the data was simply wrong. How could a water meter reading have been
wrong? Wouldn’t a customer complain? It turns that the process of a water meter
reading going to the customer consists of three steps:

1. The mechanical water meter being read by the technician onsite
2. The reading being entered from the technician’s logbook into the company ERP

system
3. The ERP system calculating the bill

In the case of errors in either data entry, the customer might call to complain, but
only the bill was corrected. The water meter readings might still remain wrong! Thus,
the AI model was getting trained on wrong data and being told the meters were not
faulty. Such wrong data turned out to be about 1% of all observations, which is about
the proportion of actually faulty water meters. So, if the meter did return such a bad
value, the model was simply tossing a coin—when the historical data did have these
wild swings, including negative values, half the time it was because the observation
was wrong, and the other half of the time it was because the meter was faulty. The
RNN was being trained on noise.
Second, some of the data was simply made up. It costs so much to send a technician
out to a site that sometimes past readings were simply extrapolated and a reasonable
charge was made to the customer. This was then made up the next time by a true
reading. For example, an actual measurement might be made in January, skipped in
March, and caught up in May. The value for March wasn’t real. Thus the AI model
was being trained on data that wasn’t real—31% of the “measurements” were actually
interpolated values. Further, the readjustments added a lot of noise to the dataset.
After correcting all the billing-related errors and the interpolated values, faulty water
meters were detected by the RNN with an accuracy of 85% (a simpler linear regres‐
sion model would have given them 79%). In order to get there, though, they had to
throw away a large percentage of the original data. Quality, in other words, trumped
quantity.
A good data governance regime would have been careful to propagate billing correc‐
tions back to the original source data and to classify measurements and interpolated
values differently. A good data governance regime would have enforced dataset qual‐
ity from the get-go.

120 | Chapter 5: Improving Data Quality



Why Is Data Quality a Part of a Data Governance Program?
To summarize, data quality is absolutely essential when planning a data program.
Organizations very often overestimate the quality of the data they have and underes‐
timate the impact of bad data quality. The same program that governs data life cycle,
controls, and usage should be leveraged to govern the quality of data (and plan for
the impact and response to bad data quality incidents).

Techniques for Data Quality
Having discussed the importance of data quality, let’s review a few strategies for clean‐
ing up data, assessing quality, and improving data quality. As a general rule, the ear‐
lier in the pipeline that data can be prepared, sanitized, disambiguated, and cleaned,
the better. It is important to note, in parallel, that data processing pipelines are not the
same for different business purposes/different teams; thus it may be hard to clean up
the data upstream, and that task may need to move downstream for the individual
teams. There is a balance here: if further down the data analytics pipeline aggregates
are performed, causing a coarsening of the data, the crucial meaning needed for
cleanup may be lost as discrete values are aggregated. We will highlight three key
techniques for data quality: prioritization, annotation, and profiling.

The Importance of Matching Business Case to the Data Use
Participants in a virtual symposium for the Society of Vertebrate Paleontology in
October 2020 were baffled by the fact that the transcription, captions, and chat mes‐
sages in digital Q&A sessions were oddly garbled. After some investigation, a com‐
mon denominator was determined: the words “bone,” “knob,” and “jerk”—all very
relevant and appropriate when discussing fossils—were banned and made invisible to
the participants.6

The underlying issue was that the online meeting platform used to power the conven‐
tion was designed for education and not necessarily for science. While on the surface
this does not immediately pose an issue (when you consider teenagers), a built-in 
“naughty-word filter” seemed to be the culprit.
The filter automatically went over all text data presented in the platform and filtered
out certain words considered “not appropriate for school.” Words that are essential
when discussing paleontology, such as, well, “bone,” are inappropriate in other set‐
tings. To top it off, even “dinosaur” was apparently inappropriate in the business con‐
text originally envisioned for the meeting platform.

6 Poppy Noor, “Overzealous Profanity Filter Bans Paleontologists from Talking About Bones”, Guardian, Octo‐
ber 16, 2020.

Why Is Data Quality a Part of a Data Governance Program? | 121



This filtering caused a stir when one researcher found out that “Wang” was one of the
banned words, while “Johnson” was not. This issue introduced serious (unintended)
bias as both are common surnames as well as slang terms.
At the end of the day, the issue was quickly fixed, and people accepted the error in
good humor, laughing at the unintended consequences and sharing them over social
media. For the purpose of this book, however, this story presents an important lesson:
any system that acts on data (e.g., the data modification system discussed in this side‐
bar) must be developed with as full a business use case as possible in mind. Clearly,
the intended audience is part of the business context, and you obviously cannot treat
international science symposium participants like you would treat schoolchildren.
Figure 5-3 drives home the point that data quality must be tied to the specific busi‐
ness case, and it also conveys a more subtle point about data governance. Sharing
“banned word lists” between use cases also informs the use case participants of the
existence of those other cases as well as of the relevant word lists—not always a
desired scenario!

Figure 5-3. Architectures of a chat filter

122 | Chapter 5: Improving Data Quality



Scorecard
In your organization, create a scorecard for the data sources. A useful scorecard
includes information about the origin of the data and its accuracy, completeness, and
timeliness. This scorecard will be used by data pipeline builders to make decisions
about how and where to use the data and for what purpose. More mundane informa‐
tion, such as who is the administrative owner of the data, who asked for it, and so on,
can also be useful in a scorecard.

Prioritization
First, prioritize—there are different sources of data and different uses for each source.
A data source used for prioritizing healthcare actions is very different from a data
source used to drive graphics for a lobby “heat map” display. Prioritization should be
performed with the eventual business goal in mind. Lineage can help with backtrac‐
ing the data and repurposing the origin for a different business purpose. By monitor‐
ing the lineage of data (more on that in “Lineage Tracking” on page 46) for both its
sources as well as its eventual uses—you can prioritize and expend resources on the
more critical data sources first.

Annotation
Second, annotate—make sure you have a standardized way to attach “quality infor‐
mation” to data sources. Even if you cannot provide a detailed scorecard for each
source, being able to attest that “This data has been vetted” or (just as importantly)
“This data is not vetted” is valuable, even if there are differences between the datasets
and their meaning. As you evolve your data quality program, you can further attach
more detailed information to datasets. Begin small, however, and clearly annotate the
information at hand so that it does not become “tribal knowledge” and get lost when
people in the know move on.
A common technique for annotation can be to “crowdsource” the quality information
by allowing people using the data to “vote” on or “star” data according to its quality as
they observe it through usage. This allows multiple human eyes on the data, and if
you start with a good default (data is normally untrusted) and assign a curator to
review data that is highly rated before issuing a quality signal to the rest of the organi‐
zation, you can effectively practice a fair data quality program. (This is not the only
recommendation in this chapter, though!)

Techniques for Data Quality | 123



Cascading Problems with Tribal Knowledge
While you are certainly very familiar with the struggle of overcoming reliance on
tribal knowledge, the following is an interesting use case that further illustrates just
how problematic disregarding annotation can be.
We’ve spoken many times with a healthcare company that is going through a myriad
of issues related to lack of annotation and reliance on tribal knowledge. This company
is quickly growing and has recently taken in several new acquisitions. Part of the ben‐
efit of these acquisitions is the data that they have. This company has the vision of
being able to leverage the data from these new acquisitions along with its own data to
run some extremely powerful analytics that are sure to have prolific business impact.
The company, however, encountered an enormous snag: the enterprise dictionaries,
metadata management, and data quality management were anemic or nonexistent at
the companies they acquired. The majority of these other companies relied on tribal
knowledge, and, post-acquisition, many of the employees who had this knowledge are
no longer at the company. This has resulted in most of the acquired data (since it’s not
properly managed, and no one can know what it is without spending a lot of time and
effort to curate it) sitting in storage, just taking up space and not providing any busi‐
ness value whatsoever.
Through this issue the company has come to realize the importance of a centralized
governance strategy that it can quickly scale—even for acquisitions—so that in the
future this problem hopefully doesn’t occur again, or at least such problems can be
somewhat mitigated.

Profiling
Data profiling begins by generating a data profile: information about a range of data
values (e.g., min, max, cardinality), highlighting values that are missing and values
that are out-of-bounds (versus the average distribution) data outliers. Reviewing the
data profile enables a determination of what are considered legal values (e.g., am I
happy with the data outliers, or should I exclude those records?) and of value mean‐
ing (e.g., is a negative value in the income column an appropriate value, or should I
exclude it?).
We proceed to detail several data profiling and cleanup techniques.

Data deduplication
In a quantitative system, each record should have only one voice. However, there are
many cases in which the same record, or the same value, actually gets duplicated,
resulting in data quality issues (and potentially in increased cost). Think about a
redundant transaction system, where each transaction has an ID, and sometimes the

124 | Chapter 5: Improving Data Quality



same transaction can appear twice (e.g., due to a transmission issue). Given the ID,
you can easily deduplicate the transactions and resolve. But think about a more chal‐
lenging use case in which you have support cases (a list of customer issues in a ticket‐
ing system), each expressed by a user input title. When writing knowledge base
articles to address these support cases, it is important to merge different user requests
that refer to the same source issue. Since a “user request” is expressed in natural lan‐
guage and can be ambiguous, this is more challenging.

Deduplicating Names and Places
Think of an entity, and it probably needs resolving or disambiguation. Two of the
most common entities that you have in datasets are names and addresses. Both of
these need to be resolved if you want to deduplicate records. For example, the same
person could be addressed as “Dr. Jill Biden,” “Jill Biden,” or “Mrs. Biden.” In all three
sets of records, it may be necessary to replace the name by a consistent identifier.
Take, for example, the impact of deduplicating author names in bibliographies, as
shown in Figure 5-4. Robert Spence is referenced as Bob Spence and as R. Spence.
Combining all these records and replacing the different variants with the canonical
version of his name greatly simplifies the set of relationships and makes deriving
insights much easier.

Figure 5-4. Deduplication can dramatically simplify the complexity of a dataset. For
example, Lisa Tweedie is also referenced as L. Tweedie. Figure adapted from a paper by
Bilgic et al., 2004.

For deduplicating names, consider using a tool such as the Google Knowledge Graph
Search API, or building one from your set of stakeholders using an open source API
such as Akutan.
Similar to names, the same place can be referred to as “the New York Stock
Exchange,” “11 Wall St,” “Wall St. & Broad Street,” “Wall and Broad,” or any of a
myriad number of combinations. If packages are noted as having been delivered to
multiple versions of this location, you might want to consolidate them into a canoni‐
cal representation of the location.

Techniques for Data Quality | 125



Address resolution of this form is provided by the Google Places API which returns a
place ID, a textual identifier that uniquely identifies a place. Place IDs are available for
most locations, including businesses, landmarks, parks, and intersections, and will
change over time as businesses close or relocate. It can be helpful, therefore, to com‐
bine the Places API with the Google Maps Geocoding API to yield the actual location
in time. Thus, while “the New York Stock Exchange” and “11 Wall Street” yield differ‐
ent place IDs, as shown in Figure 5-5 (after all, the NYSE could relocate!), geocoding
them will return the same location.

Figure 5-5. The place IDs for “the New York Stock Exchange” and “11 Wall Street” are
different from each other but geolocate to the same location

126 | Chapter 5: Improving Data Quality



Data outliers
Another tactic is to identify outliers of the data early on and eliminate them. For
example, in a system that accepts only natural numbers (the range between 1 and
infinity, excluding fractions), such as a house number in a street address book, it
would be odd to find negative or fractional numbers, and thus it may make more
sense to delete the entire record containing the outlier value rather than manually fix‐
ing it. Discretion is advised, as (for example) for 221b Baker Street, the fictional home
of Sherlock Holmes. The UK’s Royal Mail has had to recognize “221b Baker Street,
London” as a real postal address because of all the Sherlock fans expecting it to be
genuine! But the mail to 221b redirects to the Sherlock Holmes Museum at 239 Baker
Street.
To generalize and scale: when building a dataset, make sure you can determine, for as
many fields as possible, the minimum, maximum, and expected values (fractions,
negatives, strings, zero...), and include logic to clean up records with unexpected val‐
ues (or otherwise treat them). These actions are better done early in the processing
timeline rather than later, when values have been aggregated or used in machine
learning models. It is hard to backtrack/root out issues that are discovered after data
has been processed.
Extreme values are not necessarily outliers, and care must be taken there. For exam‐
ple, a perfect SAT score is possible; however, in the US the SAT score range is 400–
1,600, and values outside this range are suspect. Look at the distribution of the values
and how the curve is shaped. The extreme edges of a “bell” curve should be treated
differently than a distribution that has two peak clusters.
Consider the example distribution in Figure 5-6 in light of the expected use case of
data. In our example, an expected bell curve with a sudden peak at the edge should
prompt a more manual investigation and understanding to rule out potential data
quality issues. Be wary of automatically dismissing such values without investigating
them, as sometimes these outliers are the result not of low-quality data, but of phe‐
nomena that should be accounted for in the use case.

Techniques for Data Quality | 127



Figure 5-6. Example distributions of data

Lineage tracking
As mentioned earlier, lineage tracking for data is a force multiplier. If you can identify
source datasets of high quality, you can follow the use of those high-quality sources
and express an opinion of the result derivatives. In addition, if you can identify low-
quality datasets, you can assume any computed derivative that is a product of one (or
more) low-quality source is also a low-quality result—a useful conclusion that should
guide the use of the data.
Furthermore, with lineage tracking you can backtrack from high-criticality use cases
and results (e.g., dashboards) and learn what data sources are feeding these. At a min‐
imum, you should prioritize the quality assessment of all sources of critical decision-
supporting outputs.
This process of monitoring quality by source should not be a one-time thing but
should be used every time a new dashboard/end product is set up. And it should be
on periodic review, because the benefit of early detection can be significant if man‐
aged correctly. The impact of bad data, which can often go unnoticed, has been dis‐
cussed above.

128 | Chapter 5: Improving Data Quality



Data completeness
In some cases, there are data records with missing information, such as a customer
record without an address or a transaction without a tracking number. Special con‐
sideration should be given to such cases, and an informed decision must be made
whether or not to eliminate the records that are incomplete (resulting in “less data”
but more “complete” data). Alternatively, if accepting incomplete records, make sure
to include an annotation on the dataset indicating that it contains such records and
noting which fields are “OK” to be missing and the default values (if any) used as
input for those fields that are missing. This is especially important in the case of
merging datasets.

Merging datasets
During ETL processes, and in general, you should be aware of special values used in
the source datasets and make room for them during transformation/aggregation. If
one dataset uses “null” to indicate no data and another dataset uses “zero,” make sure
this information is available to future users. Ensure that the joining process equalizes
these two values into one consistent value (either null or zero; they have the same
meaning in our example). And of course, record this special new value.

Dataset source quality ranking for conflict resolution
When merging multiple datasets from different vendors, another topic comes to
mind: how to react if multiple-source datasets contain the same fields but with cer‐
tain fields having different values in them. This is a common issue in financial sys‐
tems, for example, where transactions are collected from multiple sources but
sometimes differ in the meaning of special values such as zero, negative 1. One way to
resolve that is to attach a ranking to each data source, and, in case of conflict, record
the data from the highest-ranked source.

Unexpected Sources for Data and Data Quality
Many a McDonald’s fan is disappointed when the ice cream maker in their local
branch is broken. In fact, this issue troubled Rashiq, a young McDonald’s fan in Ger‐
many, so much that he reverse-engineered the McDonald’s app, found its API, and
discovered a way to figure out whether or not the ice cream maker in a particular
branch was operational.
Rashiq tested out the code and later built a site, mcbroken.com, that reports on the
status of the ice cream makers in every branch on a global scale (Figure 5-7).

Techniques for Data Quality | 129



Figure 5-7. Rashiq’s Twitter feed

However, there was a downside to Rashiq’s code. In order to identify whether or not
the ice cream maker was operational, Rashiq had a bot create an ice cream order and
add an ice cream to the shopping cart. If that operation was successful, Rashiq would
mark the ice cream maker in that branch as “operational.” This resulted in thousands
of half-completed orders of ice cream at McDonald’s locations, globally. Apparently,
however, the McDonald’s data team was able to control for Rashiq’s orders, as evi‐
denced by the reply from McDonald’s’ director of analytics: “I’m Lovin’ It.” McDo‐
nald’s head of communications tweeted a response (Figure 5-8).

130 | Chapter 5: Improving Data Quality



Figure 5-8. McDonald’s responds to Rashiq’s work

Summary
Data quality is making sure that the data’s accuracy, completeness, and timeliness are
relevant to the business use case in mind. Different types of business use necessitate
different levels of the above, and you should strive to keep a scorecard of your data
sources when creating an analytics workload composed of descendants of these data
sources.
We have reviewed the importance and real-life impact of data quality, through exam‐
ples of bad data. We have discussed several techniques to improve data quality. If
there is one key piece of advice that should be taken into account from this chapter, it
is this: for data quality, handle it early, as close to the data source as possible, and
monitor resultant products of your data. When repurposing data for a different ana‐
lytics workload, revisit the sources and see if they are up to the new business task.

Summary | 131






CHAPTER 6
Governance of Data in Flight

Data, especially data used for insights via data analytics, is a “living” medium. As data
gets collected from multiple sources, it is reshaped, transformed, and molded into
various patterns for different use cases: from a standardized “transactions table” to
allow for forecasting the next season’s business demand, to a dashboard presenting
the past yield of a new crop, and more.
Data governance should be consistent across these transformations and allow more
efficiency and frictionless security. Data governance should not introduce labor by
forcing users to register and annotate new data containers as they work to reshape
and collect data for their needs.
This chapter will discuss possible techniques and tools to enable seamless data gover‐
nance through analysis of data “in flight.”

Data Transformations
There are different ways to transform data, all of which impact governance, and we
should be aware of these before we dig in deeper. It is common to refer to these pro‐
cesses as extract-transform-load (ETL). This is a generic phrase used to indicate the
various stages of moving data between systems.
Extracting data means retrieving it from the source system in which it is stored, e.g., a
legacy DB, a file, or the results a web crawler operation. Data extraction is a separate
step, as the act of extracting data is a time-consuming retrieval process. It is advanta‐
geous to consider the extraction phase as the first step in a pipeline, allowing subse‐
quent steps to operate in batches in parallel to the continued extraction. As data is
extracted from the sources, it’s useful to perform data validation, making sure the val‐
ues retrieved are “as expected” (that the completeness of records and their accuracy
match the expected values; see Chapter 5). If you perform data validation while still

133



operating within the context of the source system, you will be unencumbered by the
different computed results performed in later stages, which may be unknown (at this
stage), and as you progress, you may lose the context of the source data. In an earlier
chapter, we discussed data preparation, which is an example of a data validation pro‐
cess. The data being extracted and validated often lands in a staging area not normally
accessible to the business users, which is where the data owners and data stewards
perform the aforementioned validation checks.
Transforming data usually involves normalization of the data: eliminating outliers,
joining from multiple sources into a single record (row), aggregating where relevant,
or even splitting a single compound column into multiple columns. Be wary of the
fact that any normalization done early, as well as any kind of general purpose clean‐
ing, is also removing information—information whose value may not have been
anticipated for a case unknown at the cleaning level. The implications are that you
should have the business context in mind when extracting data and that you may
need to revisit the source data in case the extraction process removed information
needed for a new, unforeseen use case. At this stage, if you are extracting data from a
new source, it may be worthwhile to create a scorecard for the source, describing
some of the information contexts and those that are potentially not carried over dur‐
ing transformation. See “Scorecard” on page 123 for more about scorecards.
Finally, the load process situates the data into its final destination, usually a data-
analytics-capable warehouse such as Google’s BigQuery, Snowflake, or Amazon’s
Redshift.
It is important to note that as data warehousing solutions grew to be more powerful,
the transformation process sometimes moved into the data warehousing solution,
renaming ETL as ELT.
As data undergoes transformations, it is crucial not only to keep context as expressed
by the origin but also to maintain consistency and completeness. Keeping origin con‐
text, consistency, and completeness will allow a measure of trustworthiness in the
data, which is critical for data governance.

Lineage
Following the discussion of data transformations, it is important to note the role
played by data lineage. Lineage, or provenance, is the recording of the “path” that data
takes as it travels through extract-transform-load, and other movement of data, as
new datasets and tables are created, discarded, restored, and generally used through‐
out the data life cycle. Lineage can be a visual representation of the data origins (cre‐
ation, transformation, import) and should help answer the questions “Why does this
dataset exist?” and “Where did the data come from?”

134 | Chapter 6: Governance of Data in Flight



Why Lineage Is Useful
As data moves around your data lake, it intermingles and interacts with other data
from other sources in order to produce insight. However, metadata—the information
about the data sources and their classification—is at risk of getting lost as data travels.
For example, you can potentially ask, for a given data source, “What is the quality of
the data coming from that source?” It could be a highly reliable automatic process, or
it could be a human-curated/validated dataset. There are other sources that you
potentially trust less. As data from different sources intermingle, this information can
sometimes be lost. Sometimes it is even desirable to forgo mixing certain data sources
in order to preserve authenticity.
In addition to data quality, another common signal potentially available for source
data is sensitivity. Census information and a recently acquired client phone list are of
a certain level of sensitivity, while data scraped off a publicly available web page may
be of a different sensitivity.
Thus, data sensitivity and quality, and other information potentially available on the
origin, should filter down to the final data products.
The metadata information of the data sources (e.g., sensitivity, quality, whether or not
the data contains PII, etc.) can support decisions about whether or not to allow cer‐
tain data products to mix, whether or not to allow access to that data and to whom,
and so on. When mixing data products, you will need to keep track of where the data
came from. The business purpose for the data is of particular importance, as the cre‐
ated data products should be useful in achieving that purpose. If the business purpose
requires, for example, a certain level of accuracy for time units, make sure the lineage
of your data does not coarsen time values as the data is processed. Lineage is there‐
fore crucial for an effective data governance strategy.

How to Collect Lineage
Ideally, your data warehouse or data catalog will have a facility that starts from the
data origin and ends with whatever data products, dashboards, or models are used,
and that can collect lineage for every transaction along the way. This is far from com‐
mon, and there will likely be blindspots. You will have to either infer the information
that you need on by-products or otherwise manually curate information to close the
gap. Once you have this information, depending on how trustworthy it is, you can use
it for governance purposes.
As your data grows (and it is common for successful businesses to accumulate data at
an exponential rate) it is important to allow more and more automation into the pro‐
cess of lineage collection and to rely less and less on human curation. Automation is
important because, as we will see in this chapter, lineage can make a huge difference
in data governance, and placing human bottlenecks in the path to governance blocks

Lineage | 135



an organization’s ability to make data accessible. Also, a broken chain of lineage (e.g.,
due to human error) can have a larger impact on derivative data products, as trust
becomes harder to come by.
Another way to collect/create lineage information is to connect to the API log for
your data warehouse. The API log is expected to contain all SQL jobs and also all pro‐
grammatic pipelines (R, Python, etc.). If there is a good job audit log, you can use it to
create a lineage graph. This allows backtracking table creation statements to the
table’s predecessors, for example. This is not as effective as just-in-time lineage
recording, as it requires backtracking the logs and batch processing. But assuming
(not always true!) that you are focusing on lineage within the data warehouse, this
method can be extremely useful.

Types of Lineage
The level of granularity of the lineage is important when discussing possible applica‐
tions for that lineage. Normally, you would want lineage at least in the table/file
level—i.e., this table is a product of this process and this other table.
In Figure 6-1, we see a very simple lineage graph of two tables joined by a SQL state‐
ment to create a third table.

Figure 6-1. Table-level lineage

A column/field-level granularity is more useful: “This table consists of the following
columns from this other table and another column from that table.” When talking
about column-level granularity, you can start to talk about specific types of data being
tracked. In structured (tabular) data, a column is normally only a single data type. An
example business case would be tracking PII: if you mark at the sources which col‐
umns are PII, you can continue tracking this PII as data is moved and new tables are
created, and you can confidently answer which tables have PII. In Figure 6-2, two col‐
umns from Table A are joined with two columns from Table B to create Table C. If

136 | Chapter 6: Governance of Data in Flight



those source columns were “known PII” (as an example), you can use lineage to
determine that Table C now contains PII as well.

Figure 6-2. Column-level lineage

Row-level lineage allows the expression of information about transactions. Dataset-
level lineage allows the expression of coarse information about data sources.

More Granular Access Controls
One of the most common use cases we have heard during our research and interviews
is that, while project/file-level access controls work, being able to more granularly
control access is highly desired. For example, column-level access control (“Provide
access to columns 1–3 and 5–8 of this table but not column 4”) allows you to lock
down, if you will, a particular column of data and still allow access to the rest of the
table.
This method works especially well for tables that contain much usable, relevant ana‐
lytics data but may also contain some sensitive data. A prime example of this would
be a telecommunications company and its retail store transactions. Each retail store’s
transaction logs not only would contain information about each item purchased,
along with date, price, etc., but also would likely contain the purchaser’s name (in the
event the purchaser is a customer of the telecom company and put the item onto their
account). An example system leveraging labels-based (or attribute-based) granular
access controls is depicted in Figure 6-3.

Lineage | 137



Figure 6-3. Label-based security in Oracle

An analyst certainly wouldn’t need the customer’s name or account information, but
they would need the other relevant information of item purchased, location, time,
price, and so on. Instead of having to either grant full access to this table or rewrite
the table with the sensitive information removed, the sensitive column itself can have
an access control so that only certain individuals are allowed access, and all others
simply see the table with that column removed or redacted/hashed/encrypted in some
way.
As we talked about in Chapter 3, most companies do not have enough headcount to
support the constant monitoring, tagging, and rewriting of tables to remove or
restrict sensitive information. Granular access controls can enable you to get the same
result (sensitive information is guarded) while also allowing greater data
democratization.

The Fourth Dimension
As data gets created, coarsened, and discarded, it is important to realize that lineage is
a temporal state—while a certain table is currently a product of other tables, it is pos‐
sible that in a previous iteration the very same table was generated out of other data.
Looking at the “state of now” is useful for certain applications, which we will discuss
shortly, but it is important to remember that the past versions of certain data objects
are relevant to gaining a true understanding of how data is being used and accessed

138 | Chapter 6: Governance of Data in Flight



throughout an enterprise. This requires versioning information that needs to be
accessible on the object.

How to Govern Data in Flight
Working off the assumption that lineage information is preserved and is reliable to a
certain extent, there are key governance applications that rely on lineage.
A common need that gets answered by lineage is debugging or understanding sudden
changes in data. Why has a certain dashboard stopped displaying correctly? What
data has caused a shift in the accuracy of a certain machine learning algorithm? And
so on. Finding from an end product which information feeds into that end product
and looking for changes (e.g., a sudden drop in quality, missing fields, unavailability
of certain data) could significantly speed up tasks related to end products. Under‐
standing the path and transformations of the data can also help troubleshoot data
transformation errors.
Another need that commonly requires at least field-level lineage (and said lineage
needs to be reliable) is the ability to infer data-class-level policies. For a certain col‐
umn that contains PII, I want to mark all future descendants of this column as PII
and to effect the same access/retention/masking policies of the source column to the
derivatives. You will likely need some algorithmic intelligence that allows you to effect
an identical policy if the column is copied precisely and to effect a different (or no)
policy if the column contents are nullified as a result of the creation of a derivative.
In Figure 6-4, we see a slightly more involved lineage graph. While the operations
performed to create derivatives are marked as “SQL,” note that this is not always the
case; sometimes there are other ways to transform the data (for example, different
scripting languages). As data moves from the external data source into Table D, and
as Table D is merged with Table C (the by-product of Tables A and B) and finally pre‐
sented in a dashboard, you can see how keeping lineage, especially column-level line‐
age, is important.
For example, a question asked by the business user would be “Who can I show this
dashboard to?” Let’s say that just one of the sources (Table A, Table B, the external
data source) contains PII in one of the columns, and that the desired policy on PII is
“available only to full-time employees”; if PII made it into the dashboard, that should
effect an access policy on the dashboard itself.

Lineage | 139



Figure 6-4. Lineage workflow—if Table B contains sensitive data, that data can poten‐
tially be found in the dashboard as well

These use cases are rather specific, but broader use cases have been expressed by
CIOs and CISOs for a long time:
“Show me all the sensitive data within our data warehouse, and what systems contain
sensitive data”

This is a simple-to-express ask, and with the combination of lineage and the abil‐
ity to classify data sources, it is much simpler to answer than understanding the
multitude of data objects and systems that may be part of an organization’s data
lake.

“I want to identify certain systems as trusted, and make sure data does not exist in other,
less trusted systems without manual oversight”

This need can be addressed by enforcing egress controls on the trusted systems, a
task which is potentially simpler with a good lineage solution. In this way, you
can, for example, eliminate data being ingested from unapproved systems,
because those sources will show up in the lineage information.

“I need to report and audit all systems that process PII”
This is a common need in a post-GDPR world, and if you mark all sources of PII,
you can leverage the lineage graph to identify where PII gets processed, allowing
a new level of control.

140 | Chapter 6: Governance of Data in Flight



Policy Management, Simulation, Monitoring, Change
Management
We already provided one example for policy management: inheritance. In essence,
data governance policies should be derived from the meaning of the data. If in a cer‐
tain organization we want to govern PII, and PII is defined as all of personal phone
numbers, email addresses, and street addresses, these individual infotypes can be
automatically detected and associated with the data class. However, scanning a table
and determining which columns contain these infotypes is expensive computation‐
ally. To correctly identify infotypes, the system will need to sample the relevant col‐
umns (without foreknowledge about which columns are sensitive, the entire table will
need to be sampled), process those through a pattern matching and/or a machine
learning model that will provide a level of confidence as to the underlying infotype,
and tag the columns appropriately.
However, this process is made much more efficient if you can just identify the event
of a column creation without changing in values from an already-tagged column.
This is where lineage comes in.
Another use for lineage information is when data change management is considered.
Let’s imagine that you want to delete a certain table; or alternatively, you want to
change the access policy on a data class; or maybe you want to set up a data-retention
control that will coarsen the data over a period of time (for example, change the data
from “GPS Coordinates” to city/state after 30 days). With lineage, you can follow the
affected data to its end products and analyze the impact of this change. Let’s say you
limit access to a certain number of fields in a table. You can now see which dash‐
boards or other systems use the data and assess the impact. A desirable result will be
highlighting the change in access to end users accessing end products, so a warning
could be issued at the time of changing the policy, alerting the administrator to the
fact certain users will lose access and potentially even allowing a level of drill down so
those users can be inspected and a more informed decision can be made.

Audit, Compliance
We often need to point at lineage information to be able to prove to an auditor or a
regulator that a certain end product (machine learning model, dashboard, etc.) was
fed into by specific and preapproved transactional information. This is necessary
because regulators will want to be able to explain the reasoning behind certain
decision-making algorithms and make sure these rely on data that was captured
according to the enterprise’s charter—for example, making sure loan approvals rely
only on specific credit information that was collected according to regulations.

Policy Management, Simulation, Monitoring, Change Management | 141



It is increasingly common for regulated organizations to be able to prove that even
machine learning models are not biased, that decisions that are derived from data are
done so without manipulation, and that there is an unbroken “chain of trust” between
properly acquired data sources and the end-user tools (e.g., dashboards, expert sys‐
tems) that are guiding those decisions. For example, in the credit scenario just
described, a decision on a new line of credit must be traced exclusively to a list of
transactions from a trusted source, without other influences guiding the decision.

While it’s clearly incredibly important to be able to “prove” compli‐
ance in the event of an external audit, these same tools can be used
for internal auditing purposes as well. In fact, it’s best practice (as
we’ve mentioned several times in earlier chapters) to continually
assess and reassess your governance program—how it’s going,
where it may need to be modified, and how it may or may not need
to be updated as regulations and/or business needs change.

Summary
We have seen how collecting data lineage can enable data governance policies, infer‐
ence, and automation. With a lineage graph, organizations can trace data variations
and life cycle, promoting control and allowing a complete picture of the various sys‐
tems involved in data collection and manipulation. For the business user, governing
data while it is “in flight” through lineage allows a measure of trustworthiness to be
inherited from trusted sources or trusted processors in the data path. This enriched
context of lineage allows matching the right datasets (by sensitivity) to the right peo‐
ple or processes.
While lineage is essentially a technical construct, we should always keep the end busi‐
ness goal in mind. This could be “ensuring decisions are made with high-quality
data,” in which case we need to show lineage to the result from high-quality sources
(and be able to discuss the transformations along the way), or a specific business case
such as “tracing sensitive data,” as we’ve already discussed.
The technical lineage and the business lineage use cases discussed here are both
important, and we should strive to present the many technical details (e.g., intermedi‐
ate processing tables) to analysts while at the same time serving a simplified “business
view” to the business users.

142 | Chapter 6: Governance of Data in Flight



CHAPTER 7
Data Protection

One of the key concerns of data governance is that of protecting data. Owners of data
might be concerned about the potential exposure of sensitive information to individ‐
uals or applications without authorization. Leadership might be wary of security
breaches or even of known personnel accessing data for the wrong reasons (e.g., to
check on the purchase history of a celebrity). Users of the data might be concerned
about how the data they rely on has been processed, or whether it has been tampered
with.
Data protection has to be carried out at multiple levels to provide defense in depth. It
is necessary to protect the physical data centers where the data is stored and the net‐
work infrastructure over which that traffic is carried. To do this, it is necessary to
plan how authorized personnel and applications will be authenticated and how that
authorization will be provisioned. However, it is not enough to simply secure access
to the premises and the network—there is risk involved with known personnel
accessing data that they are not supposed to be accessing, and these personnel are
inside the network. Additional forms of protection such as encryption are required so
that even if a security breach happens, the data is obfuscated.
Data protection needs to be agile because new threats and attack vectors continue to
materialize.

Planning Protection
A key aspect of data governance is determining the level of protection that needs to
be afforded to different types of data assets. Then all of the organization’s data assets
must be classified into these levels.

143



For example, at the planning level, it might be mandated that data created by the
payment-processing system needs to be secured because a malicious actor with access
to individual transaction data might be able to make spurious orders and have them
billed to the original customer. This will then have to be implemented at the authenti‐
cation level by ensuring that access to any aspect of the payments data is available only
to personnel and applications that are recognized as having a role of employee. How‐
ever, not all data is made available to all employees. Instead, the payments data might
be classified at different levels. Only aggregate data by store location, date, inventory,
and payment type might be made available to business planners. Access to individual
transactions would be authorized only to the payments support team, and even then
is granted only for specific transactions on a timed basis, when authorization is pro‐
vided in the form of a support ticket carrying the customer’s approval. Given this, it is
necessary to ensure that exfiltration of payments data is not possible. The data gover‐
nance tools and systems must support these requirements, and violations and
breaches must be captured, and alerts carried out.
Ideally, a catalog of all the data assets is created, although quite often planning, classi‐
fication, and implementation can happen at higher levels of abstraction. To carry out
the above governance policy, for example, it is not necessary that you have an explicit
catalog of all the possible aggregates of payment data—only that any aggregates that
are created are placed in a governance environment with strict boundaries.

Lineage and Quality
As discussed in Chapter 5, data lineage and quality considerations are critical aspects
of data governance. Therefore, they need to be part of the protection planning pro‐
cess. It is not enough to consider data protection of only raw data; instead, protection
of the data at each stage of its transformation needs to be considered. When aggre‐
gates are computed from protected data, those aggregates need some level of protec‐
tion that is typically equal to or less than the aggregate. When two datasets are joined,
the level of protection afforded to the joined data is often the intersection of authenti‐
cation and authorization permissions. Aggregates and joins affect the data quality,
and so governance needs to take this into account as well. If the data protection on
the raw data has restrictions on the volume of data that any person or partner is able
to access, it might be necessary to revisit what the restriction on the aggregates and
joins needs to be.
Lineage and quality checks also offer the capability to catch erroneous or malicious
use of data. Corrupt or fraudulent data may be clearer when aggregated. For example, 
Benford’s Law predicts the relative incidence of leading digits on numeric values that
span multiple orders of magnitude. The last digits of numeric values are expected to
have a normal distribution. Carrying out such statistical checks is easier on aggrega‐
ted, transformed data. Once such fraud is observed, it is necessary to have the ability

144 | Chapter 7: Data Protection



to trace back (through data lineage) where the change occurred and whether it was
the result of a data protection breach.
It is part of the data culture of an organization that data published by the organization
and used within the organization is trustworthy. Data quality, as discussed in Chap‐
ter 5, remains a fundamental goal of data protection. For this purpose, it is important
to have trust-but-verify safeguards built into data pipelines to catch quality errors
when they arise.

Level of Protection
The level of protection to be afforded to an asset should reflect the cost and likeli‐
hood of a security breach associated with that asset. This requires cataloging the types
of security breaches and the costs associated with each breach. Different levels of pro‐
tection also carry costs, and so it is necessary to identify the likelihood of a breach
occurring with the given level of protection. Then a cost analysis that balances risk
between different protection levels needs to be carried out, and a protection level
chosen.
For example, consider the raw payments-processing information stored in a data lake.
Potential security breaches might include individual transaction data being read by a
malicious actor, all the transactions within a certain time period being read, all the
transactions from a certain store being read, and so on. Similarly, there is a risk of
data being modified, corrupted, or deleted.

When considering the risk of data being modified, corrupted, or
deleted, note that this may happen on purpose due to a malicious
actor as we’ve described above, or it may happen unintentionally
because of a mistake made by an internal employee. In many of our
interviews with companies we have found that both scenarios can
and have resulted in unfavorable and at times disastrous outcomes,
and thus both warrant your attention and consideration.

The cost of a security breach at a given level of protection needs to include the cost of
that data not being available, whether because of data protection or because of a secu‐
rity breach. It is also important to realize that loss of data continuity itself can carry
costs, and so the cost of losing, say, an hour of data may affect the company’s ability to
provide accurate annual reports. The costs can also be quite indirect, such as down‐
time, legal exposure, loss of goodwill, or poor public relations. Because of all of these
factors, the costs will typically be different for different stakeholders, whether they are
end users, line-of-business decision makers, or company executives. The costs might
also accrue outside the company to customers, suppliers, and shareholders. In cases
where a granular-level cost estimate is impractical, a cost level of high/medium/low
can be assigned to guide the level of protection that is needed.

Planning Protection | 145



There are usually a variety of choices in terms of the data protection that can be
applied. At one extreme, we might choose to not store the data at all. At the other
extreme, we might choose to make the dataset publicly available. In between are
choices such as storing only aggregated data, storing only a subset of the data, or
tokenizing certain fields. We can also choose where to store the data, perhaps driven
by regulations around geographic locations and by which roles need to be provided
access to the data. Risk levels vary between these choices, because the likelihood of a
breach, data loss, or corruption is different with each.

Classification
As covered in detail in Chapter 4, implementing data governance requires being able
to profile and classify sensitive data. This profile of the data is required to identify the
potential security breaches, their cost, and their likelihood. This in turn will allow the
data governance practitioner to select the appropriate governance policies and proce‐
dures that need to apply to the data.
There may be a number of governance directives associated with data classification,
organization, and data protection. To enable data consumers to comply with those
directives, there must be a clear definition of the categories (for organizational struc‐
ture) and classifications (for assessing data sensitivity).
Classification requires properly evaluating a data asset, including the content of its
different attributes (e.g., does a free-form text field contain phone numbers?). This
process has to take into account both the business use of the data (which parts of the
organization need to be able to access the data) and the privacy and sensitivity impli‐
cations. Each data asset can then be categorized in terms of business roles and in
terms of the different levels of data sensitivity, such as personal and private data, con‐
fidential data, and intellectual property.
Once the classification is determined and the protection level chosen by cost analysis,
the protection level is implemented through two aspects. The first aspect is the provi‐
sioning of access to available assets. This can include determining the data services
that will allow data consumers to access the data. The second aspect is prevention of
unauthorized access. This is done by defining identities, groups, and roles and assign‐
ing access rights to each.

Data Protection in the Cloud
Organizations have to rethink data protection when they move data from on-
premises to the cloud or when they burst data from on-premises to the cloud for
ephemeral hybrid workloads.

146 | Chapter 7: Data Protection



Multi-Tenancy
When large enterprises that typically deploy their systems on-premises move to the
cloud, one of the biggest changes they must come to terms with is being one of many
organizations simultaneously using multi-tenant cloud architecture. This means that
it is particularly important not to leak data by leaving it in unsecured locations due to
unfounded confidence that malicious actors will not be able to authenticate into the
physical infrastructure. Whereas on-premises organizations have physical and net‐
work perimeter control, that control may be lost when going to the cloud.
Some cloud providers provide “bare metal” infrastructure or “government cloud,”
essentially providing data center management to address this change. However, rely‐
ing on such single-tenant architecture often brings increased costs, greater silos, and
technical debt.
Many of the concepts and tools of on-premises security are implemented in the
cloud. So it is possible to hew closely to the way data access, categorization, and clas‐
sification are done on premises. However, such a lift-and-shift approach can involve
giving up many of the benefits of a public cloud in terms of elasticity, democratiza‐
tion, and lower operating cost. Instead, we recommend applying cloud-native secu‐
rity policies to data that is held in the cloud, because there are better and more
modern ways to achieve data protection goals.
Use cloud identity and access management (IAM) systems rather than the Kerberos-
based or directory-based authentication that you may be using on premises. This best
practice involves managing access services as well as interoperating with the cloud
provider’s IAM services by defining roles, specifying access rights, and managing and
allocating access keys for ensuring that only authorized and authenticated individuals
and systems are able to access data assets according to defined rules. There are tools
that simplify migration by providing authentication mapping during the period of
migration.

Security Surface
Another big change from on premises to cloud is the sense of vulnerability. At any
particular time, there are a number of security threats and breaches in the news, and
many of these will involve the public cloud. As we discussed in Chapter 1, much of
this is due to the increased security monitoring and auditability that public cloud sys‐
tems offer—many on-premises breaches can go unnoticed for long periods of time.
However, because of this increased media attention, organizations may be concerned
that they might be the next victim.
One of the benefits of the public cloud is the availability of dedicated, world-class
security teams. For example, data center employees undergo special screening and are
specifically trained on security. Also, a dedicated security team actively scans for

Data Protection in the Cloud | 147



security threats using commercial and custom tools, penetration tests, quality assur‐
ance (QA) measures, and software security reviews. The security team includes
world-class researchers, and many software and hardware vulnerabilities are first dis‐
covered by these dedicated teams. At Google, for example, a full-time team known as
Project Zero aims to prevent targeted attacks by reporting bugs to software vendors
and filing them in an external database. The final argument against this sense of vul‐
nerability is that “security by obscurity” was never a good option.
The security surface is also changed by the scale and sophistication of the tools used
on the cloud. Whether it is the use of AI-enabled tools to quickly scan datasets for
sensitive data or images for unsafe content, or the ability to process petabytes of data
or to carry out processing in real time on streaming data, the cloud brings benefits in
terms of being able to apply governance practices that may not be possible on prem‐
ises. Being able to benefit from widely used and well-understood systems also reduces
the chances of employee error. Finally, using a common set of tools to be able to com‐
ply with regulatory requirements in all the places where an organization does busi‐
ness leads to a dramatically simpler governance structure within that organization.
It is therefore worth having a conversation with your cloud vendor about security
best practices, because they vary by public cloud. A public cloud where much of the
traffic is on private fiber and where data is encrypted by default will have a different
surface than one where traffic is sent over public internet.

Virtual Machine Security
A necessary part of securing your data in the public cloud is to design an architecture
that limits the effects of a security compromise on the rest of the system. Because
perimeter security is not an option, it is necessary to redesign the architecture to take
advantage of shielding and confidential computing capabilities.
For example, Google Cloud offers Shielded VM to provide verifiable integrity of
Compute Engine virtual machine (VM) instances. This is a cryptographically pro‐
tected baseline measurement of the VM’s image in order to make the virtual
machines tamperproof and provide alerts on changes in their runtime state.
This security precaution allows organizations to be confident that your instances
haven’t been compromised by boot- or kernel-level malware. It prevents the virtual
machine from being booted in a different context than it was originally deployed in—
in other words, it prevents theft of VMs through “snapshotting” or other duplication.
Microsoft Azure offers Confidential Compute to allow applications running on Azure
to keep data encrypted even when it’s in-memory. This allows organizations to keep
data secure even if someone is able to hack into the machine that is running the code.

148 | Chapter 7: Data Protection



AWS offers Nitro Enclaves to its customers to create isolated compute environments
to further protect and securely process highly sensitive data such as PII and health‐
care, financial, and intellectual property data within Amazon EC2 instances.
Part of data governance is to establish a strong detection and response infrastructure
for data exfiltration events. Such an infrastructure will give you rapid detection of
risky or improper activity, limit the “blast radius” of the activity, and minimize the
window of opportunity for a malicious actor.

Though we are discussing “blast radius” in terms of minimizing of
the effects caused by a malicious actor, the blast radius concerns
also apply to employees mistakenly (or purposefully) sharing data
on the public internet. One concern of companies moving from
on-premises to the cloud is the increased blast radius of such leaks.
Data housed on premises, if leaked, will only leak internally and
not publicly. Companies fear that if their data is in the cloud and
leaks, it could potentially be accessible to anyone online. While this
is a valid concern, we hope that by the end of this book you will be
convinced that through the implementation and execution of a
well-thought-out governance program you can head off and pre‐
vent either scenario. Ideally, you can make your decisions about
where your data resides based on your business goals and objec‐
tives rather than on apprehension about the safety of cloud versus
on-premises data storage and warehousing.

Physical Security
Make sure that data center physical security involves a layered security model using as
many safeguards as possible, such as electronic access cards, alarms, vehicle access
barriers, perimeter fencing, metal detectors, biometrics, and laser-beam intrusion
detection. The data center should be monitored 24/7 by high-resolution interior and
exterior cameras that can detect and track intruders.
Besides these automated methods, there needs to be good old-fashioned human secu‐
rity as well. Ensure that the data center is routinely patrolled by experienced security
guards who have undergone rigorous background checks and training. Not all
employees of the company need access to the data center—so the data center access
needs to be limited to a much smaller subset of approved employees with specific
roles. Metal detectors and video surveillance need to be implemented to help make
sure no equipment leaves the data center floor without authorization.
As you get closer to the data center floor, security measures should increase, with
extra multifactor access control in every corridor that leads to the data center floor.
Physical security also needs to be concerned with what authorized personnel can do
in the data center, and whether they can access only parts of the data center. Access

Physical Security | 149



control by sections is important in cases where regulatory compliance requires that
all maintenance be performed by citizens of a certain country/countries and/or by
personnel who have passed security clearances.
Physical security also includes ensuring an uninterrupted power supply and reducing
the chance of damage. The data center needs redundant power systems, with every
critical component having a primary and an alternate power source. Environmental
controls are also important to ensure smooth running and reduce the chances of
machine failure. Cooling systems should maintain a constant operating temperature
for servers and other hardware, reducing the risk of service outages. Fire detection
and suppression equipment is needed to prevent damage to hardware. It is necessary
to tie these systems in with the security operations console so that heat, fire, and
smoke detectors trigger audible and visible alarms in the affected zone, at security
operations consoles, and at remote monitoring desks.
Access logs, activity records, and camera footage should be made available in case an
incident occurs. A rigorous incident management process for security events is
required so as to inform customers about data breaches that may affect the confiden‐
tiality, integrity, or availability of systems or data. The US National Institute of Stand‐
ards and Technology (NIST) provides guidance on devising a security incident
management program (NIST SP 800–61). Data center staff need to be trained in for‐
ensics and handling evidence.
Physical security involves tracking the equipment that is in the data center over its
entire life cycle. The location and status of all equipment must be tracked from the
time it is acquired, through installation, and all the way to retirement and eventual
destruction. Stolen hard drives must be rendered useless by ensuring that all data is
encrypted when stored. When a hard drive is retired, the disk should be verifiably
erased by writing zeros to the drive and ensuring the drive contains no data. Malfunc‐
tioning drives that cannot be erased have to be physically destroyed in a way that pre‐
vents any sort of disk recovery—a multistage process that involves crushing,
deformation, shredding, breakage, and recycling is recommended.
Finally, it is necessary to routinely exercise all aspects of the data center security sys‐
tem, from disaster recovery to incident management. These tests should take into
consideration a variety of scenarios, including insider threats and software
vulnerabilities.
If you are using the data center in a public cloud, the public cloud provider should be
able to provide you (and any regulatory authorities) with documentation and pro‐
cesses around all of these aspects.

150 | Chapter 7: Data Protection



Network Security
The simplest form of network security is a perimeter network security model—all
applications and personnel within the network are trusted, and all others outside the
network are not. Unfortunately, perimeter security is not sufficient for protecting sen‐
sitive data, whether on-premises or in the cloud. First of all, no perimeter is 100%
safe. At some point, some malicious actor will break into the system, and, at that
point data may become exposed. The second issue with perimeter security is that not
all applications within the network can be trusted—an application may have suffered
a security breach, or a disgruntled employee may be trying to exfiltrate data or trying
to access systems they shouldn’t have access to. Therefore, it is important to institute
additional methods of data protection to ensure that exposed data cannot be read—
this can include encrypting all stored and in-transit data, masking sensitive data, and
having processes around deleting data when it is no longer needed.

Security in Transit
Network security is made difficult because application data often must make several
journeys between devices, known as “hops,” across the public internet. The number of
hops depends on the distance between the customer’s ISP and the cloud provider’s
data center. Each additional hop introduces a new opportunity for data to be attacked
or intercepted. A cloud provider or network solution that can limit the number of
hops on the public internet and carry more of the traffic on private fiber can offer
better security than a solution that requires use of the public internet for all hops.
Google’s IP data network, for instance, consists of its own fiber, public fiber, and
undersea cables. This is what allows Google to deliver highly available and low-
latency services across the globe. Google Cloud customers can optionally choose to
have their traffic on this private network or on the public internet. Choose between
these options depending on the speed your use case requires and the sensitivity of the
data being transmitted.
Because data is vulnerable to unauthorized access as it travels, it is important to
secure data in transit by taking advantage of strong encryption. It is also necessary to
protect the endpoints from illegal request structures. One example of this is to use
gRPC (Google’s high-performance remote procedural call) as the application trans‐
port layer. gRPC is based around the idea of defining a service, specifying the meth‐
ods that can be called remotely with their parameters and return types. gRPC can use
protocol buffers as both its interface definition language (IDL) and its underlying
message interchange format. Protocol buffers are Google’s language-neutral,
platform-neutral, extensible mechanism for serializing structured data—think XML,
but smaller, faster, and simpler. Protocol buffers allow a program to introspect, that is,
to examine the type or properties of an object at runtime, ensuring that only connec‐
tions with correctly structured data will be allowed.

Physical Security | 151



Regardless of the type of network, it is necessary to make sure applications serve only
traffic and protocols that meet security standards. This is usually done by the cloud
provider; firewalls, access control lists, and traffic analysis are used to enforce net‐
work segregation and to stop distributed denial-of-service (DDoS) attacks. Addition‐
ally, it is ideal if servers are allowed to communicate with only a controlled list of
servers (“default deny”), rather than allowing access very broadly. Logs should be rou‐
tinely examined to reveal any exploits and unusual activity.

Zoom Bombing
COVID-19 has changed how many of us work, and we have found ourselves attend‐
ing a ton of video meetings across Zoom, Google Meet, Webex, and other videocon‐
ferencing apps. It’s not surprising that this surge in popularity and usage has also
brought up many cautionary tales of “Zoom bombings,” in which bad actors use open
or poorly secured Zoom meetings to post malicious content.
First the media and then, in quick succession, regulators and Congress began to scru‐
tinize, publicize, and take legal action with respect to what were perceived as privacy
or data security flaws in the products of some technology companies. The premise
was that some of these video calls were not as private as you might think, and that
customer data was being collected and being used in ways that the customers did not
intend. The issues have been a lot more complicated because the California Con‐
sumer Privacy Act (CCPA), which took effect on January 1, 2020, instituted many
regulations pertaining to consumer data and how it can be used.
When the world changes and companies are forced to react quickly to these dynam‐
ics, things are bound to happen, and consumer privacy and data security are usually
violated. This is not new. We’ve seen this with Zoom, Facebook, YouTube, TikTok,
and many other large organizations that grow quickly and then get caught up in the
limelight. Whenever any organization rises to prominence so quickly, it emerges from
obscurity and can no longer treat privacy and data security as anything other than a
top priority.
And even with all these violations, these are still difficult conversations to have in
light of the immense contributions Zoom is making to advance the greater good by
scaling its operations so quickly to allow millions of Americans to communicate with
each other during a public health crisis.
This truly reinforces the notion that security and governance should not be an after‐
thought; they should be baked into the fabric of your organization. This chapter rein‐
forces all the elements your organization should think about when it comes to data
protection and ensuring that your data is always protected, in transit and at rest. Hap‐
pily, Zoom made several security improvements, including alerting organizers of
video calls if the link to their meeting is posted online (see Figure 7-1).

152 | Chapter 7: Data Protection



Figure 7-1. Improvements being made after a Zoom security debacle.

Data Exfiltration
Data exfiltration is a scenario in which an authorized person or application extracts
the data they are allowed to access and then shares it with unauthorized third parties
or moves it to insecure systems. Data exfiltration can occur maliciously or acciden‐
tally. It can also happen because the authorized account has been compromised by a
malicious actor.
The traditional method of addressing data exfiltration risk relied on hardening the
physical perimeter defenses of private networks. In a public cloud, though, the net‐
work fabric is shared among multiple tenants, and there is no perimeter in the tradi‐
tional sense. Securing data in the cloud requires new security approaches and
methods of auditing data access.
It is possible to deploy specialized agents that produce telemetry about user and host
activity in virtual machines. Cloud providers also support the introduction of explicit
chokepoints, such as network proxy servers, network egress servers, and cross-
project networks. These measures can reduce the risk of data exfiltration, but they
cannot eliminate it completely.
It is important to recognize common data exfiltration mechanisms, identify data at
risk of exfiltration through these vectors, and put mitigation mechanisms in place.
Table 7-1 summarizes the considerations.

Data Exfiltration | 153



Table 7-1. Data exfiltration vectors and mitigation strategies
Exfiltration vector Data at risk Mitigation mechanism
Use business email or mobile Contents of organization Limit volume and frequency of data transmission.
devices to transmit sensitive emails, calendars, databases, Audit email metadata such as from- and to- addresses.
data from secure systems to images, planning documents, Scan email content using automated tools for common
untrusted third parties or business forecasts, and source threats.
insecure systems. code. Alert on insecure channels and attempts.
Download sensitive data to Files of sensitive data; any Avoid storing sensitive data in files (“data lake”), and
unmonitored or insecure sensitive data accessed via instead keep it in managed storage such as an enterprise
devices. applications that offer a data warehouse.

download feature. Establish a policy that prohibits downloads, and keep access
logs of data that is requested and served.
Regulate connections between authorized clients and cloud
services using an access security broker.
Implement dynamic watermarking in visualizations to
record the user responsible for screenshots or photographs
of sensitive information.
Add permissions-aware security and encryption on each file
using digital rights management (DRM).

Requisition or modify virtual Any sensitive data that is Maintain precise, narrowly scoped permissions, and
machines (VMs), deploy accessible to employees in comprehensive, immutable audit logs.
code, or make requests to organizations’ IT departments. Maintain separate development and test datasets that
cloud storage or consist of simulated or tokenized data, and limit access to
computation services. production datasets.
Anyone with sufficient Provide data access to service accounts with narrow
permission can initiate permissions, not to user credentials.
outbound transmission of Scan all data sent to the broader internet to identify
sensitive data. sensitive information.

Prohibit outgoing connections to unknown addresses.
Avoid giving your VMs public IP addresses.
Disable remote management software like remote desktop
protocol (RDP).

Employee termination All types of data, even Connect logging and monitoring systems to HR software and
normally benign data like set more conservative thresholds for alerting security teams
historical company memos, are to abnormal behavior by these users.
at risk of data exfiltration by
employees anticipating
imminent termination.a

a Michael Hanley and Joji Montelibano, “Insider Threat Control: Using Centralized Logging to Detect Data Exfiltration Near
Insider Termination”, Software Engineering Institute, Carnegie Mellon University, 2011.

In summary, because perimeter security is not an option, use of public cloud infra‐
structure requires increased vigilance and new approaches to secure data from exfil‐
tration. We recommend that organizations:

154 | Chapter 7: Data Protection



• Minimize the blast radius of data exfiltration events by compartmentalizing data
and permissions to access that data, perhaps through line of business or by com‐
mon workloads that access that data.

• Use fine-grained access control lists and grant access to sensitive data sparingly
and in a time-bound manner.

• Provide only simulated or tokenized data to development teams, because the
ability to create cloud infrastructure creates special risks.

• Use immutable logging trails to increase transparency into the access and move‐
ment of data in your organization.

• Restrict and monitor ingress and egress to machines in your organization using
networking rules, IAM, and bastion hosts.

• Create a baseline of normal data flows, such as amounts of data accessed or trans‐
ferred, and geographical locations of access against which to compare abnormal
behaviors.

Virtual Private Cloud Service Controls (VPC-SC)
Virtual Private Cloud Service Controls (VPC-SC) improves the ability of an organiza‐
tion to mitigate the risk of data exfiltration from cloud-native data lakes and enter‐
prise data warehouses. With VPC-SC, organizations create perimeters that protect
the resources and data of an explicitly specified set of services (Figure 7-2).

Figure 7-2. VPC-SC creates a perimeter around a set of services and data so that the
data is inaccessible from outside the perimeter, even to personnel and applications with
valid credentials.

Data Exfiltration | 155



VPC-SC thus extends private access and perimeter security to cloud services and
allows unfettered access to data for applications within the perimeter without open‐
ing up that data to access from malicious insiders, compromised code, or malicious
actors with stolen authentication credentials. Resources within a perimeter can be
accessed only from clients within authorized VPC networks (either the public cloud
network or an explicitly allowed on-premises one). It is possible to restrict internet
access to resources within a perimeter through allowed IP addresses or ranges of
addresses.
Clients within a perimeter that have private access to resources do not have access to
unauthorized (potentially public) resources outside the perimeter—this is what limits
the data exfiltration risk. Data cannot be copied to unauthorized resources outside
the perimeter.
VPC-SC, when used in conjunction with a restricted virtual IP, can be used to prevent
access from a trusted network to storage services that are not integrated with VPC
service controls. The restricted VIP (virtual IP) also allows requests to be made to
services supported by VPC Service Controls without exposing those requests to the
internet. VPC Service Controls provides an additional layer of security by denying
access from unauthorized networks, even if the data is exposed by misconfigured
Cloud IAM policies.
We recommend that you use the dry-run features of VPC-SC to monitor requests to
protected services without preventing access. This will enable you to monitor
requests to gain a better understanding of request traffic to your projects and provide
a way to create honeypot perimeters to identify unexpected or malicious attempts to
probe accessible services.
Note that VPC-SC limits the movement of data, rather than metadata, across a ser‐
vice perimeter via supported services. While in many cases VPC-SC also controls
access to metadata, there may be scenarios in which metadata can be copied and
accessed without VPC-SC policy checks. It is necessary to rely on Cloud IAM to
ensure appropriate control over access to metadata.

Secure Code
Data lineage is of no effect if the application code that produces or transforms the
data is not trusted. Binary authorization mechanisms such as Kritis provide software
supply-chain security when deploying container-based applications. The idea is to
extend the Kubernetes-managed runtime and enforce security policies at deploy time.
In Google Cloud, binary authorization works with container images from Container
Registry or another container image registry and extends Google Kubernetes Engine
(GKE). This allows the organization to scan built containers for vulnerabilities before
deploying to systems that can access sensitive data.

156 | Chapter 7: Data Protection



Binary authorization implements a policy model, where a policy is a set of rules that
governs the deployment of container images to an operational cluster. Rules in a pol‐
icy specify the criteria that an image must pass before it can be deployed. A typical
policy requires a container image to have a verified digital signature before it is
deployed.
In this type of policy, a rule specifies which trusted authorities, called signers, must
assert that required processes have completed and that an image is ready to move to
the next stage of deployment. A signer may be a human user or more often it’s a
machine process such as a build-and-test system or a part of your continuous deploy‐
ment pipeline.
During the development life cycle, signers digitally sign globally unique container
image descriptors, thereby creating certificated statements called attestations. Later,
during the deploy phase, Binary Authorization uses attestors to verify the certificate
indicating that required processes in your pipeline have been completed.

Zero-Trust Model
A perimeter-based security model is problematic, because when that perimeter is
breached, an attacker has relatively easy access to a company’s privileged intranet. As
companies adopt mobile and cloud technologies, the perimeter becomes increasingly
difficult to enforce. By shifting access controls from the network perimeter to indi‐
vidual users and devices, a zero-trust security model allows users to access enterprise
applications from virtually any location without the need for a traditional VPN.
The zero-trust model assumes that an internal network is untrustworthy and builds
enterprise applications based on the assumption that all network traffic emanates
from a zero-trust network, i.e., the public internet. Instead of relying on the IP
address of the origin, access to an application depends solely on device and user cre‐
dentials. All access to enterprise resources is authenticated, authorized, and encrypted
based on device state and user credentials. Fine-grained access to different parts of
enterprise resources is enforced, and the goal is to make the user experience of
accessing enterprise resources effectively identical between internal and public
networks.
The zero-trust model consists of a few specific parts:

• Only a device that is procured and actively managed by the enterprise is allowed
to access corporate applications.

• All managed devices need to be uniquely identified using a device certificate that
references the record in a device inventory database, which needs to be
maintained.

Data Exfiltration | 157



• All users are tracked and managed in a user database and a group database that
tightly integrate with HR processes that manage job categorization, usernames,
and group memberships for all users.

• A centralized user-authentication portal validates two-factor credentials for users
requesting access to enterprise resources.

• An unprivileged network that very closely resembles an external network,
although within a private address space, is defined and deployed. The unprivi‐
leged network only connects to the internet, and to limited infrastructure and
configuration management systems. All managed devices are assigned to this
network while physically located in the office, and there needs to be a strictly
managed access control list (ACL) between this network and other parts of the
network.

• Enterprise applications are exposed via an Internet-facing access proxy that
enforces encryption between the client and the application.

• An access control manager interrogates multiple data sources to determine the
level of access given to a single user and/or a single device at any point in time.
The latter is called endpoint verification.

Identity and Access Management
Access control encompasses authentication, authorization, and auditing. Authentica‐
tion determines who you are, authorization determines what you can do, and audit‐
ing logs record what you did.

Authentication
The identity specifies who has access. This could be an end user who is identified by a
username, or an application identified by a service account.
User accounts represent a data scientist or business analyst, or an administrator. They
are intended for scenarios (e.g., notebooks, dashboard tools, or administration tools)
in which an application needs to access resources interactively on behalf of a human
user.
Service accounts are managed by Cloud IAM and represent nonhuman users. They
are intended for scenarios in which an application needs to access resources automat‐
ically. Service accounts are essentially robot accounts that are defined to have a subset
of the permissions held by the creator of the service account. Typically, they are cre‐
ated to embody the limited set of permissions required by applications that are run
on the account creator’s behalf.

158 | Chapter 7: Data Protection



Cloud APIs reject requests that do not contain a valid application credential (no
anonymous requests are processed). Application credentials need to provide the
required information about the caller making the request. Valid credential types are:
API keys

Note that an API key says only that this is a registered application. If the applica‐
tion requires user-specific data, the user needs to authenticate themselves as well.
This can be difficult, and so API keys are often used only to access services (e.g.,
requests for stock market quotes) that do not need user credentials but just a way
to ensure that this is a paid subscriber.

Access tokens such as OAuth 2.0 client credentials
This credential type involves two-factor authentication and is the recommended
approach for authenticating interactive users. The end user allows the application
to access data on their behalf.

Service account keys
Service accounts provide both the application credential and an identity.

Authorization
The role determines what access is allowed to the identity in question. A role consists
of a set of permissions. It is possible to create a custom role to provide granular access
to a custom list of permissions. For example, the role roles/bigquery.metadata
Viewer, when assigned to an identity, allows that person to access metadata (and only
the metadata, not the table data) of a BigQuery dataset.
To grant multiple roles to allow a particular task, create a group, grant the roles to
that group, and then add users or other groups to that group. You might find it help‐
ful to create groups for different job functions within your organization and to give
everyone in those groups a set of predefined roles. For example, all members of your
data science team might be given dataViewer and jobUser permissions on data ware‐
housing datasets. This way, if people change jobs, you’ll just need to update their
membership in the appropriate groups instead of updating their access to datasets
and projects one dataset or project at a time.
Another reason to create a custom role is to subtract permissions from the predefined
roles. For example, the predefined role dataEditor allows the possessor to create,
modify, and delete tables. However, you might want to allow your data suppliers to
create tables but not to modify or delete any existing tables. In such a case, you would
create a new role named dataSupplier and provide it with the specific list of
permissions.

Identity and Access Management | 159



Normally, access to resources is managed individually, resource by resource. An iden‐
tity does not get the dataViewer role or the tables.getData permission on all
resources in a project; rather, the permission should be granted on specific datasets or
tables. As much as possible, avoid permission/role creep; err on the side of providing
the least amount of privileges to identities. This includes restricting both the roles
and the resources on which they are provided. Balance this against the burden of
updating permissions on new resources as they are created. One reasonable compro‐
mise is to set trust boundaries that map projects to your organizational structure and
set roles at the project level—IAM policies can then propagate down from projects to
resources within the project, thus automatically applying to new datasets in the
project. The problem with such individualized access is that it can quickly get out of
hand.
Another option for implementing authorization is to use Identity-Aware Proxy (IAP).
IAP lets you establish a central authorization layer for applications accessed by
HTTPS, so you can use an application-level access control model instead of relying
on network-level firewalls. IAP policies scale across your organization. You can define
access policies centrally and apply them to all of your applications and resources.
When you assign a dedicated team to create and enforce policies, you protect your
project from incorrect policy definition or implementation in any application. Use
IAP when you want to enforce access control policies for applications and resources.
With IAP, you can set up group-based application access: a resource could be accessi‐
ble for employees and inaccessible for contractors, or accessible only to a specific
department.

Policies
Policies are rules or guardrails that enable your developers to move quickly, but
within the boundaries of security and compliance. There are policies that apply to
users—authentication and security policies, such as two-factor authentication, or
authorization policies that determine who can do what on which resources—and
there are also policies that apply to resources and are valid for all users.
Where possible, define policies hierarchically. Hierarchical policies allow you to cre‐
ate and enforce a consistent policy across your organization. You can assign hierarch‐
ical policies to the organization as a whole or to individual business units, projects, or
teams. These policies contain rules that can explicitly deny or allow roles. Lower-level
rules cannot override a rule from a higher place in the resource hierarchy. This allows
organization-wide admins to manage critical rules in one place. Some hierarchical
policy mechanisms allow the ability to delegate evaluation of a rule to lower levels.
Monitor the use of the rules to see which one is being applied to a specific network or
data resource. This can help with compliance.

160 | Chapter 7: Data Protection



Context-Aware Access is an approach that works with the zero-trust network security
model to enforce granular access control based on a user’s identity and the context of
the request. Use Context-Aware Access when you want to establish fine-grained
access control based on a wide range of attributes and conditions, including what
device is being used and from what IP address. Making your corporate resources
context-aware improves your security posture. For example, depending on the policy
configuration, it is possible to provide edit access to an employee using a managed
device from the corporate network, but provide them only view access to the data if
the access is from a device that has not been fully patched.
Instead of securing your resources at the network level, Context-Aware Access puts
controls at the level of individual devices and users. Context-Aware Access works by
leveraging four key pieces of technology that have all been discussed in this chapter:
Identity-Aware Proxy (IAP)

A service that enables employees to access corporate apps and resources from
untrusted networks without the use of a VPN.

Cloud identity and access management (Cloud IAM)
A service that manages permissions for cloud resources

Access context manager
A rules engine that enables fine-grained access control.

Endpoint verification
A way of collecting user device details.

Data Loss Prevention
In some cases, especially when you have free-form text or images (such as with
records of support conversations), you might not even know where sensitive data
exists. It is possible that the customer has revealed their home address or credit card
number. It can therefore be helpful to scan data stores looking for known patterns
such as credit card numbers, company confidential project codes, and medical infor‐
mation. The result of a scan can be used as a first step toward ensuring that such sen‐
sitive data is properly secured and managed, thus reducing the risk of exposing
sensitive details. It can also be important to carry out such scans periodically to keep
up with growth in data and changes in use.
AI methods such as Cloud Data Loss Prevention can be used to scan tables and files
in order to protect your sensitive data (see Figure 7-3). These tools come with built-in
information type detectors to identify patterns, formats, and checksums. They may
also provide the ability to define custom information type detectors using dictionar‐
ies, regular expressions, and contextual elements. Use the tool to de-identify your

Identity and Access Management | 161



data, including masking, tokenization, pseudonymization, date shifting, and more, all
without replicating customer data.

Figure 7-3. Scanning a BigQuery table using Cloud DLP.

To redact or otherwise de-identify sensitive data that the Cloud DLP scan found, pro‐
tect the data through encryption, as discussed in the next section.

Encryption
Encryption helps to ensure that if the data accidentally falls into an attacker’s hands,
they cannot access the data without also having access to the encryption keys. Even if
an attacker obtains the storage devices containing your data, they won’t be able to
understand or decrypt it. Encryption also acts as a “chokepoint”—centrally managed
encryption keys create a single place where access to data is enforced and can be
audited. Finally, encryption contributes to the privacy of customer data—it allows
systems to manipulate data, for example, for backup, and it allows engineers to sup‐
port the infrastructure without providing access to content.
It is possible to use a public cloud provider’s encryption-at-rest and encryption-in-
transit mechanisms to ensure that low-level infrastructure access (to hard drives or
network traffic, for example) does not afford the ability to read your data. Sometimes,

162 | Chapter 7: Data Protection



however, regulatory compliance might require you to make sure that your data is
encrypted with your own keys. In such cases, you can use customer-managed encryp‐
tion keys (CMEK), and you can manage your keys in a key management system
(KMS), even in Cloud KMS, GCP’s central key management service. Then you can
designate datasets or tables that you want to be encrypted using those keys.
Multiple layers of key wrapping are used so that the master keys aren’t exposed out‐
side of KMS (see Figure 7-4). Every CMEK-protected table has a wrapped key as part
of the table metadata. When the native cloud tools access the table, they send a
request to Cloud KMS to unwrap the key. The unwrapped table key is then used to
unwrap separate keys for each record or file. There are a number of advantages to this
key-wrapping protocol that reduce the risk should an unwrapped key be leaked. If
you have an unwrapped file key, you can’t read any other files. If you have an unwrap‐
ped table key, you can only unwrap file keys after you pass access control checks. And
Cloud KMS never discloses the master key. If you delete the key from KMS, the other
keys can never be unwrapped.

Figure 7-4. Envelope encryption with data encryption key (DEK) and key encryption key
(KEK). The KEKs are managed centrally in a KMS, which rotates keys through the use
of a key ring.

A common use of encryption is to delete all records associated with a specific user,
often in response to a legal request. It is possible to plan for such deletion by assign‐
ing a unique encryption key to each userId and encrypting all sensitive data corre‐
sponding to a user with that encryption key. In addition to maintaining user privacy,
you can remove the records for a user simply by deleting the encryption key. This
approach has the advantage of immediately making the user records unusable (as
long as the deleted key is not recoverable) in all the tables in your data warehouse,
including backups and temporary tables. To query the encrypted table, you need to
decrypt the data before querying. This approach is called crypto-shredding.

Identity and Access Management | 163



Differential Privacy
Another concept for keeping data secure, especially when discussing private data, is
differential privacy. Differential privacy is necessary when you want to share a dataset
that contains highly personal or otherwise sensitive information without exposing
any of the involved parties to identification. This means describing aggregate data
while withholding information about the individuals. However, this is not a simple
task—there are ways to statistically re-identify individuals in the dataset by cross-
matching different dimensions of aggregate data (and knowing something about the
individual). For example, you can extract a particular salary from an aggregate aver‐
age—if you run multiple averages across the salary recipient’s home neighborhood,
their age group, and so on, you will eventually be able to compute the salary.
There are common techniques for ensuring differential privacy:
k-anonymity

k-anonymity means that aggregates returned from queries to the datasets are rep‐
resenting groups of at least k individuals, or are otherwise expanded to include k
individuals (Figure 7-5). The value of k is determined by the size of the dataset
and other considerations relevant to the particular data represented.

Figure 7-5. Example of a k-anonymized table in which the age was replaced with a k-
anonymized value.

Adding “statistically insignificant noise” to the dataset
For fields such as age or gender where there is a discrete list of possible values,
you can add statistical noise to the dataset so that aggregates are skewed slightly
to preserve privacy but the data remains useful. Examples of such techniques that
generalize the data and reduce granularity are l-diversity and t-distance.

164 | Chapter 7: Data Protection



Access Transparency
It is important for safeguarding access to the data that any access to the data is trans‐
parent. Only a small number of on-call engineers should be able to access user data in
the production system, and even then it should only be to ensure safe running of the
system. Whenever someone in the IT department or cloud provider accesses your
data, you should be notified.
In Google Cloud, for example, Access Transparency provides you with logs that cap‐
ture the actions Google personnel take when accessing your content. Cloud Audit
Logs helps you answer questions about “who did what, where, and when?” in your
cloud projects. While Cloud Audit Logs provides these logs about the actions taken
by members within your own organization, Access Transparency provides logs of the
actions taken by personnel who work for the cloud provider.
You might need Access Transparency logs data for the following reasons:

• To verify that cloud provider personnel are accessing your content only for valid
business reasons, such as fixing an outage or in response to a support request

• To verify and track compliance with legal or regulatory obligations
• To collect and analyze tracked access events through an automated security infor‐

mation and event management (SIEM) tool

Note that the Access Transparency logs have to be used in conjunction with Cloud
Audit Logs because the Access Transparency logs do not include access that origi‐
nates from a standard workload allowed through Cloud IAM policies.

Keeping Data Protection Agile
Data protection cannot be rigid and unchanging. Instead, it has to be agile to take
account of changes in business processes and respond to observed new threats.

Security Health Analytics
It is important to continually monitor the set of permissions given to users to see
whether any of those are unnecessarily broad. For example, we can periodically scan
user-role combinations to find how many of the granted permissions are being used.
In Figure 7-6, the second user has been granted a BigQuery Admin role but is using
only 5 of the 31 permissions that the role grants. In such cases, it is better to either
reduce the role or create a more granular custom role.

Keeping Data Protection Agile | 165



Figure 7-6. Scan the use of permissions by users to narrow down role definitions or pro‐
vide the users more fine-grained access.

Data Lineage
A key attribute of keeping data protection agile is to understand the lineage of every
piece of data. Where did it come from? When was it ingested? What transformations
have been carried out? Who carried out these transformations? Were there any errors
that resulted in records being skipped?
It is important to ensure that data fusion and transformation tools (see Figure 7-7)
provide such lineage information, and that this linear information is used to analyze
errors being encountered by production systems.

Figure 7-7. It is important to maintain the data lineage of all enterprise data and
address errors in ingest processes.

166 | Chapter 7: Data Protection



Event Threat Detection
The overall security health needs to be continually monitored as well. Network secu‐
rity logs need to be analyzed to find the most frequent causes of security incidents.
Are a number of users trying (and failing) to access a specific file or table? It is possi‐
ble that the metadata about the file or table has been breached. It is worth searching
for the source of the metadata leak and plugging it. It is also advisable to secure the
table before one of the attacks succeeds.
Instead of limiting ourselves to scanning security logs, it is worth modeling network
traffic and looking for anomalous activity in order to identify suspicious behavior
and uncover threats. For example, an unusual number of SSH logins by an authorized
employee might be a sign of data exfiltration, and the definition of “unusual” in this
case can be learned by an AI model that compares the activity of an employee against
their peers who are doing a similar role and working on similar projects.

Data Protection Best Practices
While data protection is top of mind for all industries and users, there are different
approaches from one industry to another, including some stricter approaches in
industries such as healthcare, government, and financial services that routinely deal
in sensitive data.
The healthcare industry has been dealing with medical records for decades. As
healthcare providers have moved more and more to digital tools for record keeping,
the industry has experienced globally known incidents. The ransomware cyberattack
that affected more than 60 trusts within the United Kingdom’s National Health Ser‐
vice (NHS) spread to more than 200,000 computer systems in 150 countries, and the
list continues to grow.1

Similarly, financial institutions have been dealing with cyberattacks. We still remem‐
ber when Equifax announced a massive cybersecurity breach in 2017 in which cyber‐
criminals accessed the personal data of some 145.5 million Equifax customers,
including full names, social security numbers, birth dates, addresses, and driver’s
license numbers. At least 209,000 customers’ credit card credentials were taken in the
attack.2 Since then, a number of more visible data breaches have taken place, and
many if not all of them affected managed systems that were part of the companies’
own data centers.

1 Roger Collier, “NHS Ransomware Attack Spreads Worldwide”, Canadian Medical Association Journal 189, no.
22 (June 2017): E786–E787.

2 David Floyd, “Was I Hacked? Find Out If the Equifax Breach Affects You”, Investopedia, updated June 25,
2019.

Data Protection Best Practices | 167



The natural question that comes to mind is: why are these data breaches still taking
place. even with the effective processes and tools that are available, and despite a con‐
certed focus to protect data? It comes down to how the best practices are imple‐
mented, and whether the institutions are staying on top of their data governance
processes day in and day out, 24/7, with no complacency. In the previous sections, we
have highlighted a comprehensive way of protecting the data; in cloud, with physical
and network security, and with advanced IAM.
Professionals in each industry—healthcare, financial institutions, retail, and others—
are trying to establish what they believe to be the best practices for their world when
it comes to data protection. As an example of those best practices, let’s start with what
data protection experts in the healthcare industry are suggesting to their users.
A data breach in healthcare can occur in several forms. It could be a criminal cyberat‐
tack to access protected health data for the purpose of committing medical identity
theft, or it could be an instance of a healthcare employee viewing patient records
without authorization.
Organizations in the healthcare industry have to be very diligent in protecting sensi‐
tive patient, financial, and other types of datasets, and must stay on top of this 24/7,
throughout their entire operations, by educating their employees and by utilizing
best-in-class security tools and best practices for the industry. Here are some of the
recommended best practices for the healthcare industry.

Separated Network Designs
Hackers utilize various methods to gain access to healthcare organizations’ networks.
IT departments in the healthcare industry should rigorously deploy tools such as fire‐
walls and antivirus and anti-malware software rigorously. However, focusing on
perimeter security is not enough. Healthcare firms should adopt network design
approaches that separate networks so that intruders cannot access the patient data
even if they are able to obtain access to parts of their network. Some firms are already
practicing and benefitting from these practices at the network-design level.

Physical Security
Paper-based record keeping of much of the sensitive data is still a very common prac‐
tice in the healthcare industry, and it is imperative that healthcare providers provide
physical security with locked cabinets and doors, cameras, and so on, while physically
securing the IT equipment where sensitive data is stored, including providing cable
locks to the laptops within offices.

168 | Chapter 7: Data Protection



Physical Data Breach in Texas
Throughout this book we have discussed (and will discuss further) security around
your data in on-premises storage systems and/or in cloud-based storage. There are
use cases, however, around securing actual physical data (like papers or tapes), as we
pointed out in the introduction to this section.
While it’s a bit of an older example, an event that occurred in Texas in 2011 illustrates
the importance of physical security.
A data contractor was working for Science Applications International Corporation
(SAIC), the firm that handles data for TRICARE, the federal government and military
healthcare program. In his car, the contractor had the backup tapes of electronic
healthcare records for over 4.6 million active and retired military personnel. All of
these tapes were stolen from the car, compromising the health information of not
only the active and retired military personnel, but also their families.
SAIC clarified in a press release that while the tapes medical record data such as social
security numbers, diagnoses, and lab reports), financial data was not included on the
tapes and thus was not leaked.
SAIC and TRICARE set up incident response call centers to help patients deal with
the security breach and to help them to place a fraud alert on their credit reports if
needed, but they also made a statement:

If you are a citizen in the modern society, if you have a credit card, if you shop
online, if you have information stored, you should anticipate that some day your
information will get stolen.3

This statement isn’t wholly inaccurate even a decade later, but what it fails to consider
is the role of governance in preventing these sorts of things from happening in the
first place—which is likely to be the main reason you are reading this book.
You should be aware of and consider implementing additional security measures for
any physical data that you may have. Healthcare, as in this example, is an obvious sce‐
nario, but there are many instances in which there may be physical sensitive data that
needs to be treated and/or needs to be something that you educate your employees on
and build into your data culture (see Chapter 9 for more on this).
As in this example, even the methods by which you transport physical data from one
location to another should be considered. Often we think about how data is transfer‐
red between applications, where it’s stored, whether or not it’s encrypted, and so on.
But as this example shows, you should also consider how you are physically trans‐
porting data, if this is something you or your company will be doing. Will it be by car?
What are the safeguards that are in place to ensure that data is always being watched

3 Jim Forsyth, “Records of 4.9 Million Stolen from Car in Texas Data Breach”, Reuters, September 29, 2011.

Data Protection Best Practices | 169



over or protected? Do you have any safeguards in place in the event of this data being
stolen? Is it unreadable? Is it encrypted?
Hopefully, if you note these areas of consideration and generate a plan for how to
treat and protect your physical data, what happened to SAIC and TRICARE in 2011
won’t happen to you.

Portable Device Encryption and Policy
Data breaches in the healthcare industry in recent years have occurred largely
because a laptop or storage device containing protected health information was lost
or stolen. A key measure that healthcare organizations should always undertake to
prevent a data breach due to a stolen portable device is to encrypt all devices that
might hold patient data, including laptops, smartphones, tablets, and portable USB
drives. Also, in addition to providing encrypted devices for their employees, health‐
care organizations should establish a strong policy against carrying data on an unen‐
crypted personal device. We are seeing more and more bring-your-own-device
(BYOD) policies adopted by various institutions, and many healthcare providers are
now using mobile device management (MDM) software to enforce those policies.

Data Deletion Process
A key lesson that data-breach victims have learned is the need for a data-deletion pol‐
icy, because as more data is held by an organization, there is more for intruders to
steal. Healthcare institutions should deploy a policy mandating the deletion of patient
and other information that’s no longer needed, while complying with regulations that
require records to be kept for a certain duration. Furthermore, regular audits must be
exercised to ensure that policies are followed and that organizations know what data
is where, what might be deleted, and when it can be deleted.

Electronic medical device and OS software upgrades
One of the areas that healthcare providers and their IT organizations need to pay
closer attention to is medical device software and OS upgrades. Intruders have dis‐
covered that healthcare providers were not always so diligent and still utilize outdated
OS-based medical devices that become easy targets to hack, despite update recom‐
mendations from the healthcare device vendors’. While these updates may seem dis‐
ruptive to the providers and their employees, a data breach is much worse for a
provider. Keeping these devices patched and up to date will minimize these
vulnerabilities.

170 | Chapter 7: Data Protection



Data breach readiness
There is no such thing as preventing every possible IT security incident; this is exactly
why institutions should have a well-established plan to deploy when and if a data
breach occurs. Educating employees on what a HIPAA violation is, how to avoid
phishing, avoiding target attacks, and choosing strong passwords are among a few
simple steps that healthcare institutions can take.
The healthcare industry best practices and suggestions we’ve highlighted apply to
other industries as well. Institutions at large should implement processes and proce‐
dures to reduce in-the-moment thinking when it comes to dealing with data
breaches. Automation and well-planned and commented response are key in dealing
with a potential data breach. Establishing internal data security controls will reduce
the risk of data breaches while improving regulatory compliance. In summary, organ‐
izations should establish the following steps in their company-wide process:

• Identify systems and data that need to be protected.
• Continuously evaluate possible internal and external threats.
• Establish data security measures to manage identified risks.
• Educate and train employees regularly.
• Monitor, test, and revise—and remember that the IT world risks change all the

time.

Why Do Hackers Target the Healthcare Industry?
Every year, we see study after study on the impact of security breaches in the health‐
care industry, where hackers demand large ransoms from healthcare vendors and
providers. 2019 was no different; in fact, according to a study by Emsisoft, it has
reached record levels, costing healthcare industry members over $7.5 billion just in
the US, where over 100 state and local government agencies, over 750 providers,
nearly 90 universities, and more than 1,200 schools have been affected.4 The results
were not just an inconvenience of expense but a massive disruption to healthcare
delivery: surgeries were postponed, and in some cases patients had to be transferred
to other hospitals to receive the urgent care they needed—not to mention the disrup‐
tion to payment systems and collection systems. Even student grades were lost. 2020
was no better in terms of healthcare data breaches, according to HealthITSecurity.
The World Privacy Forum website has an interactive map that is a great resource for
getting the latest on medical data breaches (see Figure 7-8).

4 Emsisoft Malware Lab, “The State of Ransomware in the US: Report and Statistics 2019”, December 12, 2019.

Data Protection Best Practices | 171



Figure 7-8. An example from the World Privacy Forum website shows medical data
breaches in the state of Kentucky in 2018.

Many of us regularly wonder, “Why is this happening, and why is this happening
especially in the healthcare industry?” We are alarmed to discover that a large
percentage of these institutions did not even know how often these breaches were
happening, and only a small fraction had the processes in place to recover and protect
their data. It is especially worrying to know that many of the breaches were happen‐
ing via very smart and digitally connected IoT devices. The industry has to do much
better going forward.

172 | Chapter 7: Data Protection



Hackers target the healthcare industry largely because it is one of the few industries
with legacy operating systems like Microsoft XP or Windows 7 in their medical
devices, and it has not been industry practice to stay on top of OS patches when they
become available. In fact, some of these older OS-based medical devices will no
longer be supported by Microsoft, meaning the vendors need to ensure their users
upgrade to a device that is up to date and better prepared for today’s security
breaches. Failing to patch or renew their devices will continue to increase the risk.
Manufacturing a medical device is not easy; it takes many years to build one, and then
it is in use for 15 years or longer. So these devices become easy targets during their
20+ years of use, not to mention that many of them are shipped to other countries for
secondary and tertiary use. Considering how often the electronics and software in
our daily lives change (e.g., cell phones have an average life cycle of two years before
we refresh them), you can see why hackers target healthcare devices.
The healthcare industry must be much more proactive in its approach to data protec‐
tion. It must utilize best practices around data protection; have total control of assets;
continuously assess gaps and processes; leverage best practices and experts from
NIST, HITRUST, and others in the cybersecurity industry; and ensure that appropri‐
ate budgets are allocated for upgrading legacy medical device software and endpoints
that are open to hackers. A single weak point is enough for a hacker to impact an
entire network.

Summary
In this chapter, we looked at what is perhaps the key concern of data governance—
protecting data. A necessary part of planning protection is ensuring that lineage is
tracked and quality monitored so that the data remains trustworthy. The planning
process should identify the levels of protection and decide the kind of protection
afforded at each level. Then the classification process itself—classifying data assets
into levels—needs to be planned for.
A key concern with data stored in the public cloud is multi-tenancy. There could be
other workloads running on the same machines that are carrying out data processing
for your company. Therefore, it is important to consider the security surface and take
into account virtual machine security. Typically, physical security is provided by the
cloud provider, and your responsibility for network security is limited to getting the
data to/from the cloud (security in transit) and configuring the appropriate security
controls, such as VPC-SC. Important aspects to consider are data exfiltration, zero
trust, and how to set up Cloud IAM. These involve setting up governance of authenti‐
cation, authorization, and access policies.
Because your data may be used by data science users who need access to all of the
data, Cloud IAM and row-based security may be insufficient. You must also deter‐
mine the need for the masking, tokenization, or anonymization of sensitive

Summary | 173



information that may not be needed to train machine learning models. A clear under‐
standing of what to encrypt and how to implement differential privacy, comprehen‐
sive access monitoring, and the ablity to change security profiles in response to
changing risks all become more important the more the data gets used for machine
learning.
The data protection governance process should also create policies on when separate
network designs are necessary, how portable devices have to be treated, when to
delete data, and what to do in the case of a data breach. Finally, data protection needs
to be agile, because new threats and attack vectors continue to materialize. So the
entire set of policies should be periodically revisited and fine-tuned.

174 | Chapter 7: Data Protection



CHAPTER 8
Monitoring

In previous chapters, we explained what governance is; discussed the tools, people,
and processes of governance; looked at data governance over a data life cycle; and
even did a deeper dive into governance concepts, including data quality and data
protection.
In this chapter, we will do a deep dive into monitoring as a way to understand how
your governance implementations are performing day-to-day, and even on a longer-
term basis. You will learn what monitoring is, why it is important, what to look for in
a monitoring system, what components of governance to monitor, the benefits of
monitoring, and the implications of doing so. You have implemented governance in
your organization, so how do you know what’s working and what’s not? It’s important
to be able to monitor and track the performance of your governance initiatives so you
can report to all the stakeholders the impact the program has had on the organiza‐
tion. This allows you to ask for additional resources, course-correct if/as needed,
learn from the wins and failures, and really showcase the impact of the governance
program—not to mention making the most of potential growth opportunities that
might become more visible to the chief data and digital officers of your organization.
So what is monitoring? Let’s start by introducing the concept.

What Is Monitoring?
Monitoring allows you to know what is happening as soon as it happens so you can
act quickly. We’re in a world in which companies and individuals are made and
destroyed on social networks; that’s how powerful these platforms are. User expecta‐
tions are fueling complexity in applications and infrastructure, so much so that over
50% of mobile users abandon sites that take more than three seconds to load. Most
web pages take a lot longer than that to load, creating a significant gap between con‐
sumers’ expectations and most businesses’ mobile capabilities. If you’re an

175



organization that services customers and you know of this stat, then you must ensure
that you have a system that’s constantly monitoring your website load times and alert‐
ing you when numbers are outside acceptable bounds. You want to make sure that
your team can resolve issues before they become bigger problems.
So what is monitoring? In simple terms, monitoring is a comprehensive operations,
policies and performance management framework. The aim is to detect and alert
about possible errors of a program or a system in a timely manner and deliver value
to the business. Organizations use monitoring systems to monitor devices, infrastruc‐
ture, applications, services, policies, and even business processes. Because monitoring
applies to many areas of the business beyond what we can cover, in this chapter we
will primarily focus on monitoring as it relates to governance.
Monitoring governance involves capturing and measuring the value generated from
data governance initiatives, compliance, and exceptions to defined policies and pro‐
cedures—and finally, enabling transparency and auditability into datasets across their
life cycle.
What’s interesting about monitoring is that when everything is working well and
there are no issues, efforts usually go unnoticed. When issues arise and things go
wrong—for example, with data quality or compliance exceptions—then governance is
one of the first areas that gets blamed because these areas fall right within governance
areas that should be constantly monitored. (We will do a deeper dive into each of
these areas later in the chapter.) Because of this, it’s important for you to define met‐
rics that allow you to demonstrate to the business that your efforts and investments in
data governance are benefiting the business in reducing costs, increasing revenue,
and providing business value. You need to implement metrics that you will track
accordingly, enabling you to understand and showcase the improvements you are
making to the bottom line. In later sections of this chapter, we will provide you with a
list of some of the key metrics you can track to show the efficacy of your governance
program.

Why Perform Monitoring?
Monitoring allows you to review and assess performance for your data assets, intro‐
duce policy changes within the organization, and learn from what’s working and
what’s not, with the ultimate goal of creating value for the business. If your organiza‐
tion is used to hearing about incidents via its customer base and is spending too
much time and money on manual support, then a monitoring system is vital. Moni‐
toring serves many different functions, and for the majority of the use cases, a moni‐
toring system will help you with alerting, accounting, auditing, and compliance. Let’s
do a deep dive into these core areas:

176 | Chapter 8: Monitoring



Alerting
In its simplest terms, an alert warns someone or something of a danger, threat, or
problem—typically with the intention of avoiding it or dealing with it. An alert
system can help you prevent incidents, and when incidents happen, they are
detected earlier and faster. A governance monitoring system that is specifically
designed to monitor data quality can alert you when data quality thresholds fall
outside the allowable limits, allowing you to avoid falls in service or to minimize
the amount of time needed to resolve the issue.

Accounting
An account refers to a report or description of an event or experience. In this
core area of monitoring, you want to get an in-depth analysis of your applica‐
tions, infrastructure, and policies. This enables you to create more appropriate
strategies, set realistic goals, and understand where improvements are needed,
allowing you to discover the effectiveness and value generated from data gover‐
nance and stewardship efforts.

Auditing
Auditing refers to a systematic review or assessment of something to ensure that
it is performing as designed. Audits also enable transparency into data assets and
their life cycle. Auditing allows you to understand the ins and outs of your busi‐
ness so that you can make improvements to processes and internal controls, with
the aim of reducing organizational risk and preventing unexpected costs from
external auditors.

Compliance
Regulatory compliance refers to the efforts required to help you meet relevant
policies, laws, standards, and regulations. When your systems, thresholds, or pol‐
icies are outside the defined rules, resulting in out-of-compliance processes,
monitoring can help you stay in compliance; your business can be alerted as soon
as these exceptions are detected, giving you the opportunity to resolve the issues
in order to stay compliant.

Although this is not the first time we’re covering this subject, we
want to reiterate just how important alerting is to your governance
program. In countless interviews, and during our research, the lack
of sufficient alerting has been mentioned as a top pain point among
data scientists, data engineers, and the like. While there are many
ingredients we’ve discussed that are key to a successful governance
strategy, proper alerting is something that often gets overlooked.
Improvements in this area not only aid in prevention and early
detection of incidents but also help to streamline tasks and the time
spent on those tasks by your employees.

Why Perform Monitoring? | 177



Monitoring use cases will tend to fall within the four buckets we’ve just highlighted,
and in most cases they will overlap. For example, as you monitor compliance in your
organization, this can also help you when it comes to audits and proving that your
organization is doing the right things and performing as designed.
Monitoring is often done using a monitoring system that can be built in-house by
purchasing a system and configuring it to your other internal systems. It can be out‐
sourced to other vendors as a managed service for ease of setup and expense, or if
you’re using cloud solutions, it can be embedded within your organization’s work‐
flows. Open source tools also provide some monitoring solutions to be considered.
We will delve more deeply into monitoring systems later in this chapter; for now, let’s
focus on what areas of governance you should monitor, why, and how.

Why Alerting Is a Critical Monitoring Function
This is a cautionary tale about a small website management services company that
served 50 major hospitals around the country. The story sounds a little too familiar,
and honestly, this continues to happen to many organizations that store customer
information. For the company, things were going so well that it decided to introduce a
new service that allowed patients and partners to do online bill payment.
One day during a maintenance procedure, an employee accidentally turned off the
company’s firewall, and all of the patient data stored on its online billing service sys‐
tem (for at least five hospitals and approximately 100,000 patients) was left exposed to
the world.1 And because the hospitals reported the breaches as separate incidents, the
nature of the error was not immediately apparent.
Unfortunately for this small company, there was no way to recover from such an inci‐
dent. It simply closed its doors and shut down its website. There’s so much to learn
from this experience, because if customer data had been properly governed and
stored—that is, if the data had been encrypted and the right access controls had been
enforced—it would have been easier to catch the incident right when it happened and
alert the hospitals as needed. In addition, this reinforces the importance of monitor‐
ing and, more specifically, of alerting. It’s scary to think that an employee turned off a
firewall system without any system alerts being triggered or the right folks being
alerted.

1 Tim Wilson, “A Cautionary Tale”, Dark Reading, August 17, 2007.

178 | Chapter 8: Monitoring



What Should You Monitor?
There are many areas of an organization that are monitored: operating systems and
hardware, network and connectivity, servers, processes, governance, and more. In
order to stay close to the core of this book and chapter, we will now do a deeper dive
into monitoring as it relates to data governance. A lot of the concepts we will explore
next have already been covered in previous chapters, and so we will focus on which
parts need to be monitored and how to go about it. We will keep referring back to
what you’ve learned in order to solidify these concepts across the key areas.

Data Quality Monitoring
In Chapters 1 and 2, we introduced the concept of data quality, and we went into that
in more detail in Chapter 5. Data quality allows the organization to trust the data and
the results. High-quality data means that the organization can rely on that data for
further calculations/inclusions with other datasets. Because of how important data
quality is, it should be monitored proactively, and compliance exceptions should be
identified and flagged in real time. This will allow the organization to move quickly to
identify and mitigate critical issues that can cause process breakdowns.
There are critical attributes of data quality that should be tracked and measured,
including completeness, accuracy, duplication, and conformity. These are outlined in
detail in Table 8-1 and should map to specific business requirements you have set
forth for your governance initiative. Monitoring can help you create controls for vali‐
dation and can provide alerts as needed when these are outside the defined thresh‐
olds. The attributes in Table 8-1 are common problem areas within the data
management space that make it difficult to trust data for analysis.

Table 8-1. Data quality attributes
Attribute Description
Completeness This identifies what data is missing and/or not usable.
Accuracy This identifies the correctness and consistency of the data and whether the correct values are stored for an

object.
Duplication This identifies which data is repeated. Duplicate records make it difficult to know the right data to use for

analysis.
Conformity This identifies which data is stored in a nonstandard format that will not allow analysis.

Process and tools for monitoring data quality
A data quality monitoring system routinely monitors and maintains data quality
standards across a data life cycle and ensures they are met. It involves creating con‐
trols for validation, enabling quality monitoring and reporting, accessing the level of

What Should You Monitor? | 179



incident severity, enabling root cause analysis, and providing remedy
recommendations.
When setting up a data quality monitoring process, some of the things you need to
consider are:
Establishing a baseline

Establish a baseline of the current state of data quality. This will help you identify
where quality is failing and help you determine what is “good” quality and what
those targets are. These targets must be tied to the business objectives you have
set forth for your governance initiatives. Comparing results over time is essential
to proactive management of ongoing data quality improvement and governance.

Quality signals
These are usually monitored over a period of time or by the source of the data.
The monitoring system will be looking to verify data fields for completeness,
accuracy, duplicates, conformity, statistical anomalies, and more. When data
quality falls below a specified threshold, an alert would be triggered with more
information about the quality issue observed. These quality signal rules are usu‐
ally set forth by the data governance committee, which ensures compliance with
data policies, rules, and standards. More details around this are outlined in Chap‐
ter 4. These policy guidelines and procedures ensure that the organization has
the data program that was envisioned by the organization.

In order to get started with monitoring data quality, determine a set of baseline met‐
rics for levels of data quality and use this to help you build a business case to justify
the investment and, over time, help you make improvements to the governance
program.

Data Lineage Monitoring
We introduced the concept of data lineage in Chapter 2 and talked about why track‐
ing lineage is important. The natural life cycle of data is that it is generated/created by
multiple different sources and then undergoes various transformations to support
organizational insights. There is a lot of valuable context generated from the source of
the data and all along the way that is crucial to track. This is what data lineage is all
about. Monitoring lineage is important to ensure data integrity, quality, usability, and
the security of the resulting analysis and dashboards.
Let’s be honest: tracking and monitoring lineage is no simple task; for many organiza‐
tions, their data life cycle can be quite complex, as data flows from different sources,
from files to databases, reports, and dashboards, while going through different trans‐
formation processes. Lineage can help you track why a certain dashboard has differ‐
ent results than expected and can help you see the movement of sensitive data classes
across the organization.

180 | Chapter 8: Monitoring



When looking to track and monitor lineage, it’s important to understand certain key
areas that you will come across. Table 8-2 has some of these attributes. These are not
all encompassing but are just some of the more important ones for you to know.

Table 8-2. Data lineage attributes
Attribute Description
Data These are the various changes and hops (aggregates, additionals, removals, functions, and more) as
transformations data moves along the data life cycle. Monitoring helps you understand details of the data points and

their historical behavior, as data gets transformed along the way.
Technical metadata Metadata is important for understanding more about the data elements. Enabling automatic tagging

of data based on its source can help provide more understanding of the data asset.
Data quality test These represent data quality measurements that are tracked at specific points of the data life cycle, in
results order to take action as/if needed.
Reference data Reference data values can be used to understand backward data lineage and transformation from that
values specific data point, and/or the intermediate transformation following that point with forward data

lineage. Understanding reference data values can be useful in performing root cause analysis.
Actor An actor is an entity that transforms data. It may be a MapReduce job or an entire data pipeline. Actors

can sometimes be black boxes, and the inputs and outputs of an actor are tapped to capture lineage in
the form of associations.

Process and tools for monitoring data lineage
What makes monitoring lineage so complicated is that it needs to be captured at mul‐
tiple levels and granularities—this is tedious and time consuming because of all the
intricacies and dependencies. As mentioned earlier in this chapter, monitoring serves
different purposes; for lineage it can be used to alert, audit, and comply with a set of
defined rules and policies. As described in Table 8-2, one of the things you could
monitor is the behavior of the actors; when their resulting transformed outputs are
incorrect, an alert function can be set up that the inputs are investigated, and the
actors are augmented and corrected to behave as expected or are removed from the
process flow of the data.
Monitoring lineage can also provide a lineage audit trail, which can be used to deter‐
mine the who, what, where, and when of a successful or attempted data breach in
order to understand which areas of the business were or might have been affected by
the breach. In addition, the important details tracked by data lineage are the best way
to provide regulatory compliance and improve risk management for businesses.
Lineage is about providing a record of where data came from, how it was used, who
viewed it, and whether it was sent, copied, transformed, or received and it is also
about ensuring this information is available. You will need to identify the best way to
do this for your organization, depending on the use cases and needs of the business.
Identify the data elements, track the listed elements back to their origin, create a
repository that labels the sources and their elements, and finally, build visual maps for
each system and a master map for the whole system.

What Should You Monitor? | 181



Compliance Monitoring
Compliance has been covered in great detail in several of the previous chapters.
Understanding state and federal regulations, industry standards, and governance pol‐
icies and staying up to date on any changes ensures that compliance monitoring is
effective.
The changing nature of laws and regulations can make monitoring compliance diffi‐
cult and time consuming. Noncompliance is not an option because it often results in
considerable fines—sometimes more than twice the cost of maintaining or meeting
compliance requirements. In a study from the Ponemon Institute, the average cost for
organizations that experience noncompliance problems is $14.82 million. That
includes fines, forced compliance costs, lack of trust from customers, and lost busi‐
ness.
Monitoring compliance means having an internal legal representative (attorney),
though at times this task falls on the privacy tsar or someone else in security, who
must continually keep up with laws and regulations and how they impact your busi‐
ness. It also requires that changes are made according to the new information gath‐
ered in order to stay in compliance. In addition, to stay in compliance requires
auditing and tracking access to data and resources within the organization. All of this
is done to ensure your business is compliant in case of an audit by the government.

Process and tools for monitoring compliance
To be successful in monitoring regulatory compliance, you need to evaluate which
regulations apply to your data governance efforts and what compliance looks like
with these regulations. Once this is done, do an audit to understand your current
governance structure with regard to the relevant regulations. Consider this a bench‐
mark exercise to understand your current standing, future requirements, and how
you can build a plan to get to the end state, and then continuously monitor that end
state to ensure compliance.
Once you’ve completed the audit, you should jump to creating a monitoring plan;
here you should address all risks identified in the audit stage and prioritize those that
are the greatest threat to the organization. Next, decide how you are going to imple‐
ment the monitoring program, including roles and responsibilities.
The resulting output will be dependent on the level and frequency of regulatory
changes and updates from the relevant regulatory boards. Make sure you have a way
to inform the regulatory agencies of failed audits and of how you’re looking to miti‐
gate them.
Here are some ways you can be proactive about compliance:

182 | Chapter 8: Monitoring



• Keep on top of regulatory changes and make sure that you’re checking for up
dated standards and regulations. Of course, this is easier said than done.

• Be transparent so your employees understand the importance of compliance and
the regulations they are complying with. Offer training sessions to explain the
regulations and their significance.

• Build a culture of compliance within the organization—and yes, there must be
someone who has the task of staying up to date on regulatory requirements that
the company may (or may not) need to be in compliance with. Most big compa‐
nies have large compliance teams; even if you’re a small organization; however,
it’s still important to designate someone to handle compliance, including moni‐
toring, checking for updates in regulations and standards, and more.

• Cultivate a strong relationship between your compliance person/team and the
legal department so that when incidents occur, those teams are in lockstep with
each other and are already used to working together.

Monitoring compliance can be a manual process, with checklists and all, but that can
also make things even more complicated. There are automated compliance tools that
your organization can look at that provide compliance in real time, giving you con‐
tinuous assurance and minimizing the chance that human error may lead to a gap in
compliance.

Program Performance Monitoring
Monitoring and managing the performance of a governance program is integral to
demonstrating program success to business leadership. This type of monitoring
allows you to track progress against the program’s aims and objectives, ensuring the
governance program delivers the right outcomes for the organization, accounting for
efficient and effective use of funding, and identifying improvement opportunities to
continue creating impact for the business.
In order to monitor program performance, some of the items you might want to
measure include:

• Number of lines of business, functional areas, and project teams that have com‐
mitted resources and sponsorship

• Status of all issues that come into the governance function, how they were han‐
dled, and what the resulting impact was

• Level of engagement, participation, and influence the governance program is
having across the organization, which will help people understand the value of
the governance program

What Should You Monitor? | 183



• Value-added interactions, including training and project support, and what the
impact is to the business

• Business value ROI from data governance investments, including reducing pen‐
alties by ensuring regulatory compliance, reducing enterprise risk (e.g., contrac‐
tual, legal, financial, brand), improving operational efficiencies, increasing top-
line revenue growth, and optimizing customer experience and satisfaction

Process and tools for monitoring program performance
Program performance monitoring needs to be continuous and ongoing, helping you
identify areas that are not performing to expectations and determining what types of
program adjustments are needed. Most performance management frameworks con‐
sist of the sets of activities outlined in Figure 8-1.2

Figure 8-1. Performance management framework

Let’s look more closely at each of these areas:
Alignment with existing governance frameworks

This ensures that your program performance is aligned with the established gov‐
ernance framework. In Chapter 4, we did a deep dive into governance frame‐
works, groups, and what is needed for effective governance.

Developing performance indicators
Now that you are aligned with existing governance frameworks, develop key per‐
formance indicators (KPIs) for your governance program. These should be well
defined, relevant, and informative.

Reporting progress and performance
Documenting the governance performance objectives and how they’re being met
and sharing this information with leadership will ensure that people see the value
of the program. Providing reports becomes an essential way for people to con‐
sume this information.

2 Grosvenor Performance Group, “How Is Your Program Going…Really? Performance Monitoring”, May 15,
2018.

184 | Chapter 8: Monitoring



Taking action based on performance results
It’s important to identify ways to ensure that the performance results are used to
inform decision making in your organization; otherwise, what is the point?

As you can see in Figure 8-1, this is an ongoing and iterative process, with one area
feeding and informing the other.

Security Monitoring
Cyberattacks are becoming bigger than ever before, with new threats from state
actors and increasingly sophisticated methods. The damage related to cybercrime is
projected to hit $6 trillion annually by 2021, according to Cybersecurity Ventures.3 In
addition, many countries are now taking consumer data privacy and protection more
seriously by introducing new legislation to hold businesses accountable.
Attacks cost more than money. They can affect a business’s brand and shareholder
reputation. In the recent Equifax data breach, in which over 140 million records were
exposed, the company most likely incurred a cost of more than $32 billion to resolve
the issue.4 This was reflected in their stock price, which fell more than 30% after the
breach. Perhaps even worse, adjacent firms in Equifax’s industry who did not get
breached felt a 9% stock drop, likely due to loss of confidence in security measures.5

So even if you’re doing everything right, you can still be impacted by a breach.
That’s why security monitoring is so important. It is the process of collecting and ana‐
lyzing information to detect suspicious behavior, or unauthorized system changes on
the network in order to take action on alerts as needed. Most companies are routinely
exposed to security threats of varying severity; the causes of security breaches include
hackers and malware, careless employees, and vulnerable devices and operating sys‐
tems. Security threats are part of the normal course of conducting business; therefore
it’s important to be prepared and act on threats and breaches before they cause dam‐
age and disruption.
There are many areas of the business where you can monitor security. Table 8-3 high‐
lights some of those areas.

3 “29 Must-Know Cybersecurity Statistics for 2020”, Cyber Observer, December 27, 2019.
4 “5 Reasons Why You Need 24x7 Cyber Security Monitoring”, Cipher (blog), May 15, 2018.
5 Paul R. La Monica, “After Equifax Apologizes, Stock Falls Another 15%”, CNNMoney, September 13, 2017.

What Should You Monitor? | 185



Table 8-3. Security monitoring items, showing some areas of the organization that you can
monitor security on

Item Description
Security alerts and These are any alerts or incidents generated from an IT environment. They could be data exfiltration or
incidents unusual port activity, acceptable use policy violations, or privileged user-activity violations.
Network events This involves the ability to monitor network activity and receive alarms or reports of the occurrence of

selected events, including device statuses and their IP addresses, new device alerts, and network
status.

Server logs This involves monitoring and detecting server activities continuously, examining alerts before a server
mishap occurs, recording server logs and reports for easy tracking of errors, performing log analysis,
and monitoring server performance and capacity.

Application events This involves monitoring events surrounding your software and applications in their stack, ensuring
they are accessible and performing smoothly.

Server patch This involves installing and patching all the servers in your IT environment and staying compliant. This
compliance helps mitigate vulnerabilities, server downtimes and crashes, and slowing down.
Endpoint events This is a list of all events that can be emitted by an instance of an application, a process, or an event.
Identity access This involves defining and managing the roles and access privileges of individual network users and
management maintaining, modifying, and monitoring this access throughout each user’s access life cycle.
Data loss This involves detecting potential data breaches/data exfiltration transmissions and employing

monitoring techniques and prevention when data is in use, in motion, and at rest.

Process and tools for monitoring security
An effective process of monitoring security is continuous security monitoring, pro‐
viding real-time visibility into an organization’s security posture and constantly look‐
ing for cyber threats, security misconfigurations, and other vulnerabilities. This
allows an organization to stay a step ahead of cyber threats, reducing the time it takes
to respond to attacks while complying with industry and regulatory requirements.
Cyber security can be conducted at the network level or the endpoint level. With net‐
work security monitoring, tools aggregate and analyze security logs from different
sources to detect any failures. Endpoint security technologies, on the other hand, pro‐
vide security visibility at the host level, allowing for threats to be detected earlier in
the process flow.
Security monitoring is an area with many players: from companies that offer solu‐
tions you can implement within your organization to full-fledged companies that you
can outsource the entire service to. The solution you choose to go with depends on
your business, the size of your in-house team, your budget, the technologies at your
disposal, and the level of security-monitoring sophistication that you’re looking for.
Both options have pros and cons that need to be weighed before you can make an
effective decision for your business.

186 | Chapter 8: Monitoring



You should now have an understanding of what governance items to monitor, the
how, and the why. The next section will look more closely at monitoring systems,
their features, and which criteria to monitor.

What Is a Monitoring System?
Monitoring systems are the core set of tools, technologies, and processes used to ana‐
lyze operations and performance in order to alert, account, audit, and maintain the
compliance of organizational programs and resources. A robust monitoring system is
paramount to the success of a program and needs to be optimal with regard to what
the business needs.
Monitoring can be done in-house by purchasing a system and configuring it to your
other internal systems; it can be outsourced as a managed service to other vendors
because of an internal lack of expertise and expense; or if you’re using cloud solu‐
tions, it can be embedded within your organization’s workflows. Open source tools
also provide some monitoring solutions that can be considered as well. Whatever
option you choose to go with, here are common features of a good monitoring
system.

Analysis in Real Time
Real time is real money. In a world in which things change so fast and people need
information at their fingertips, you must have a monitoring system that does analysis
in real time. A good system should offer continuous monitoring with minimal delays,
allowing you to make changes and improvements on the fly.

System Alerts
A monitoring system needs to have the ability to signal when something is happening
so that it can be actioned. A system that allows multiple people to be informed with
the right information will go a long way toward ensuring that issues are addressed as
quickly as possible. A system should allow configuration for multiple events and have
the ability to set different sets of actions depending on the alert. The alert should con‐
tain information about what is wrong and where to find additional information.

Notifications
A good monitoring system needs to have a robust, built-in notification system. We’re
no longer in the age of pagers, so your system needs to be able to send SMS, email,
chat, and more to ensure that the message gets to the right folks. And once the mes‐
sage is received, the right people need to be able to communicate back to the team
that the alert has been received and that the issue is being investigated—and when
issues are resolved, communicate that the system or process is back to normal

What Is a Monitoring System? | 187



operations. Notifications could kick off additional processes automatically, where cer‐
tain systems take an action.

Reporting/Analytics
Monitoring systems are big data collection and aggregation units given all the alerts
and trigger events collected over a period of time. Your monitoring system needs to
allow for robust reporting that will allow you to present the data to clients or different
departments in the organization. Reporting allows you to identify trends, correlate
patterns, and even predict future events.

Graphic Visualization
Collecting data should be augmented with the ability to analyze and visualize a situa‐
tion. Dashboards play a critical role in ensuring that everyone has a place to look to
see how things are going, and even to observe trends over time. A visual representa‐
tion of what’s happening is easier for people to understand and absorb and is one of
the top customer requests in a data governance solution. A good monitoring system
should have friendly and easy-to-understand graphs that allow the organization to
make decisions.

Customization
It’s no surprise that different organizations have different business requirements. You
need to have the ability to be able to customize your monitoring system by function,
user type, permissions, and more, allowing you to have the right alerts triggered, and
to have them actioned by the right folks.
Monitoring systems need to run independently from production services, and they
should not put a burden on the systems they are tracking. This simply allows the
monitoring system to continue running in the case of a production outage or other
failure event. Otherwise, a failure could take down a monitoring system (when the
production environment is down), and that defeats the purpose of its existence. In
addition, as you would with any system, ensure you have a failover for your monitor‐
ing system in case it is the one that’s affected by a blackout or failure. Figure 8-2 high‐
lights a simple monitoring system, given some of the features we’ve just highlighted.

188 | Chapter 8: Monitoring



Figure 8-2. Example monitoring system

As you can imagine, monitoring systems are becoming even more sophisticated with
the introduction of machine learning capabilities, so even though this list of features
is robust, it’s not by any measure the be-all and end-all. Use this list as a starting point
to select or build the right monitoring system for your organization, and, depending
on your use case and company needs, augment your system as needed.

Monitoring Criteria
Now that you have selected a monitoring system, what are the types of criteria that
you can set up to monitor? Monitoring systems collect data in two distinct ways: pas‐
sive systems, where the tools observe data created by the application and system under
normal conditions (i.e., log files, output messages, etc.); and active systems, which are
more proactive, use agents and other tools to capture data through a monitoring
module, and are often integrated within production systems.
Here’s some common criteria you can follow as you set up your monitoring system:
Basic details

Identify basic details for the items you’re looking to monitor. Capture a list of
rules, their purpose, and a distinct name for each one. If your system has prede‐
fined alerts and queries, select the ones you want to monitor and label them
accordingly so they can be easily identified.

Monitoring Criteria | 189



Notification condition
Program your notification condition. Once the query/criteria you set is identified
and the result is evaluated against configurable thresholds, an alert should be set
off if there’s a violation on the criteria set forth. Users should be identified by
email or SMS, and the information should be made available on the monitoring
dashboard. In addition, it’s not enough to simply send alerts; you need to ensure
that someone acknowledges that they’ve received the alert and that they are
working on it. Also, the alert needs to go to someone on call, and if that person,
for whatever reason, is not available, it needs to be routed to the right person who
can respond to the alert. If the alert is not responded to, the system should send
another notification within a specified amount of time as long as the metric con‐
tinues to remain outside the threshold.

Monitoring times
In this instance, specify the frequency of the running of a certain validation
(daily/weekly/monthly), and then how long and how frequently it should occur
within the day (business hours within the day and frequency within the day).
This is more of a gut check system that looks to make sure processes and systems
are operating correctly; it is more useful for passive systems in which things are
monitored to ensure ongoing operations.

Given the complexity of all these items and the level of sophistication needed to
maintain them, monitoring systems are what make all this possible. The next section
offers some reminders and touches on other things you should keep in mind.

Important Reminders for Monitoring
From reading this chapter, you will have discerned some recurring themes when it
comes to monitoring. Here are some considerations to keep in mind:
Getting started with a monitoring system

Just like any software development process, monitoring can be done in-house by
purchasing a system and configuring it to your systems; it can be outsourced as a
managed service to other vendors because of an internal lack of expertise and
expense,; or if you’re using cloud solutions, it can be embedded within your
organization’s workflows. Open source tools also provide some monitoring solu‐
tions that can be considered as well.

Real-time improves decision making
For the majority of the governance items outlined in the previous section, a con‐
tinuous, real-time monitoring system is paramount to improving decision-
making and staying on top of compliance. In addition, having an alert system
that is robust and allows people to take action as needed will ensure that things
are remedied within the outlined SLAs.

190 | Chapter 8: Monitoring



Data culture is key to success
Employee training needs to be embedded into the fiber of your business—your
data culture, as discussed in Chapter 9. Many governance concerns are under‐
taken by employees who might not be aware that they’re doing something that
will compromise the business. Find ways to make education personal, because
people are most likely to apply education when they see it as important to their
lives.

Summary
This chapter was designed to give you a foundation on monitoring and how to think
about implementing it for your organization. Monitoring is vital to understanding
how your governance implementations are performing on both a day-to-day and a
longer-term basis. Monitoring is where the rubber meets the road, allowing you to
ask for additional resources, course-correct as needed, learn from the wins and fail‐
ures, and really showcase the impact of your governance program.
Take an audit of your current governance and monitoring initiatives and augment
them as needed, because doing this can only reap more benefits for your
organization.

Summary | 191






CHAPTER 9
Building a Culture of Data Privacy

and Security

In this book we have covered a lot: considerations in data governance, the people and
processes involved, the data life cycle, tools, and beyond. These are all pieces of a puz‐
zle that need to come together in the end for data governance to be successful.
As alluded to earlier, data governance is not just about the products, tools, and people
who carry out the process—there is also the need to build a data culture. The careful
creation and implementation of a data culture—especially one focused on privacy
and security—not only aids in the establishment of a successful data governance pro‐
gram but also ensures that the program is maintained over time.
As we discussed in Chapter 3, the people and processes around data governance—in
conjunction with data governance tools—are essential components of a data gover‐
nance program. In this chapter we will go one step further than thatby including the
data culture (the culture within a company around data) as a key final component in
creating a highly successful data governance program.

Data Culture: What It Is and Why It’s Important
The data culture is the set of values, goals, attitudes, and practices around data, data
collection, and data handling within a company or organization. While many compa‐
nies or organizations spend quite a bit of time vetting data governance tools and cre‐
ating processes, they often fail to set up a culture within the company around data.
This culture defines and influences things like:

193



• How data is thought about within the company/organization (Is it an asset?
Needed to make decisions? The most important part of the company? Just some‐
thing to be managed?)

• How data should be collected and handled
• Who should be handling data and when
• Who is responsible for data during its life cycle
• How much money and/or resources will be committed to serving the company/

organization’s data goals

While this is certainly not an exhaustive list, it begins to show you the vast amount of
considerations that go into the definition of a data culture.
Having a set and defined data culture is important for a multitude of reasons, but the
most important is that it sets the stage and serves as the glue that holds everything
else together within a data governance program.
We will go into more detail in this chapter about what the “North Star” of a data cul‐
ture should look like and about what considerations you should be making when
designing your own.

Starting at the Top—Benefits of Data Governance to the
Business
A key aspect of building a successful data culture is getting buy-in from the top down,
which generally requires that decision makers within a company see how a data gov‐
ernance program will work and agree on why implementation of a data culture bene‐
fits the company’s bottom line. An efficient data culture aids in ensuring reliable,
high-quality data that not only produces better analytics but also reduces compliance
violations and penalties. All of this will result in better business performance, as high‐
lighted in various studies including a 2018 McKinsey report that found that “break‐
away companies” are twice as likely to claim they had a strong data governance
strategy.1 In many of our interviews with companies, one of the most common com‐
plaints we hear about initiating and implementing a data governance program is get‐
ting buy-in from those that have the power to fund data governance initiatives, as
well as to support and enforce a data culture.

1 Peter Bisson, Bryce Hall, Brian McCarthy, and Khaled Rifai, “Breaking Away: The Secrets to Scaling Analyt‐
ics”, McKinsey & Company, May 22, 2018.

194 | Chapter 9: Building a Culture of Data Privacy and Security



Often the investment in a data governance program and building a data culture (both
in terms of the purchasing of tools and infrastructure and in terms of headcount) is
seen as a cost with no ROI other than hoped-for assurance that a company won’t be
hit with fines for being out of compliance with a regulation. A well-thought-out and
well-executed governance program can, however, provide the cost savings of proper
data handling, and it also has the ability to increase the value of already existing assets
(“old data” that may be sitting around).
We have found and will discuss several areas that can be quite persuasive in helping
decision makers to see the value and importance of building a data culture.

Analytics and the Bottom Line
We’ve discussed in depth the implications of governance and how knowing what data
there is, and where it is, not only aids in locking down and handling sensitive data
but also helps analysts to run better, more actionable analytics. Better analytics—ana‐
lytics that are based on higher-quality data or come from a synthesis of data from
multiple sources, for example—enable us to make better data-driven decisions. All
companies are driven by a desire to increase their profitability, whether by increasing
revenue and/or by decreasing waste or expenditure. The entire push to data-driven
decision making has at its heart the hope and promise of driving revenue. When deci‐
sion makers are able to see that a data governance program and data culture generate
better analytics, which in turn positively affects the bottom line, they are not only a
bit more likely to be passively “bought in,” but they may also rise to be champions
and drivers of the data culture.

Company Persona and Perception
While not directly related to the bottom line, there is much to be said for the public
perception of a company and how it handles data.
In the last five years, there have been several companies that have fallen under great
scrutiny regarding the data they collect and how that data is used. The public’s per‐
ception that these companies have used data unethically carries a host of cascading
negative repercussions—ranging from decreased employee morale to financial effects
such as dropped sponsors and/or losing customers to a competitor—and these do
indeed affect a company’s bottom line.

Starting at the Top—Benefits of Data Governance to the Business | 195



Top-Down Buy-In Success Story
One company we’ve interviewed has had a particularly successful execution of a data
governance program, in large part due to the buy-in it received from the top of the
organization. This company works in research and in healthcare and is looking to lev‐
erage analytics across data collected on each side of the business. Currently its data
resides in separate storage for each line of business. To marry this data together into a
central repository, it recognized the need for a comprehensive data governance
strategy.
We hope we’ve impressed upon you in this book that all companies should have a data
governance strategy and program, but of course there are certain business types that,
due to their high level of regulation, need to be even more conscious of their imple‐
mentation of governance. Healthcare is one of those. This company knew that a com‐
prehensive governance program would need to be broad and be executed at many
different levels within the organization. The company deals almost exclusively in sen‐
sitive data, and for this data to be moved, manipulated, joined with other datasets,
and so on, the governance on it had to be top-notch.
To achieve the level of buy-in that would be required, the company set out to create a
charter that outlined exactly what the governance program within the company
should look like: the framework/philosophy that should be followed, tools that would
be needed, the headcount needed to execute these tools, and notably, the establish‐
ment and ongoing reinforcement of a data culture. Is this exhaustive? Lofty? Idealis‐
tic? Perhaps. But through this charter, the company was able to gain that buy-in from
the top, and it is now executing one of the most well-thought-out and structured gov‐
ernance programs we’ve seen (complete with headcount for governance-only related
tasks—a true rarity).
We include this example not to imply that every single one of these “boxes” must be
ticked or that a smaller-scale or fledgling governance program is not worth it. On the
contrary: we hope that this example (while extreme) serves to show you just how
much documenting a well-thought-out strategy, including how this strategy will be
embedded into the culture of the company, can help you in getting not only initial
buy-in, but also the level of buy-in that will maintain and support your governance
program in the long run.

Intention, Training, and Communications
Perhaps one of the most important aspects of building a data culture is the internal
data literacy, communications, and training. In the overview of this chapter, we men‐
tioned that a key to successful governance and data culture is not just the establish‐
ment of a program but maintenance over time to ensure its longevity. Integral to this

196 | Chapter 9: Building a Culture of Data Privacy and Security



end is intention, data literacy (a deep understanding of and ability to derive meaning‐
ful information from all kinds of data), training, and communications.

A Data Culture Needs to Be Intentional
Just as we discussed that a successful governance program needs to have a set process,
the building and maintenance of a data culture is very much the same.

What’s important
When creating and establishing a data culture, a company first needs to decide what’s
important to it—what its tenets are. For example, a company that deals with a lot of
sensitive data (like a healthcare company) may decide that the proper treatment and
handling of PII is a primary tenet, whereas a small gaming app company may decide
that ensuring data quality is its primary focus. Alternatively, a company may decide
that many tenets are important to it, which is fine—the key aspect here is that these
tenets need to be clearly defined and collectively agreed upon as the rest of the estab‐
lishment and maintenance of a data culture stems from this step.
Aside from internal tenets, there also exist tenets that are nonnegotiable in nature,
such as legal requirements and compliance standards that all or most companies need
to integrate as tenets, no matter what.
It’s also worth noting here that in addition to tenets and requirements/compliance
standards, we would argue that an important and perhaps even crucial tenet is that of
caring. This may seem like a strange tenet to have as part of a data governance pro‐
gram, but it is an essential component of a data culture. For a governance program to
function at its highest level, there must be an intrinsic desire on the part of the com‐
pany and its employees to do the right thing. The protection of and respect for data
has to be an integral part of the fabric of the company and its data-handling ethos. It
must be something that is touted and supported from the top so that it cascades down
to the rest of the company. Without this concern, the company is one that merely
deals with data, and perhaps has a data governance program but has no data culture.
While no company wants to rely solely on its employees to do the right thing, foster‐
ing a data culture helps to fill in the gaps for when things invariably go sideways.

Training: Who Needs to Know What
A successful implementation of a data culture not only needs to have its tenets well
defined; it also needs to identify who will be carrying these out, how they will do this,
and whether or not they have the knowledge and skills needed for proper execution.

Intention, Training, and Communications | 197



The “who,” the “how,” and the “knowledge”
Too often we have seen that one or more of these three areas is glossed over or taken
for granted. As when we discussed the different “hats” involved in data governance,
there is a high likelihood that several of the folks carrying out these tasks have: little-
to-no technical knowledge; some may be doing other tasks as part of their role, leav‐
ing little time to dedicate to data governance; and there are often breakdowns around
who is responsible for what tasks. Each of these components is important and should
not be overlooked. Each component needs to be thoroughly considered when plan‐
ning and implementing the data culture aspect of a data governance program.
Part of the data governance and data culture execution strategy includes determining
who will do what. Not only is this important in terms of defining roles and responsi‐
bilities, but it’s also important that the people who will be fulfilling these duties have
the skills and the knowledge to perform their tasks. It is simply not enough to enlist a
person to a task, give them a tool, and hope for the best. A plan for how knowledge
and skills will be not only acquired but also maintained over time is critical.
A solid plan for what training will be required and how it will be disseminated is
essential. Again, this is a place where we’ve seen companies falter—they may have
decided who does what and even provided some initial training to get people up to
speed, but they forget that as technology and the data collected grow and change, new
skills and knowledge may be necessary. Training is often not a “one-and-done” event
—it should be seen as ongoing.
In devising your own training strategy, you should think in terms of what’s initially
needed to teach your staff and what should continue to be reinforced and/or intro‐
duced in subsequent training. For example, we have seen companies have success
with setting up events such as “Privacy Week” or “Data Week,” in which essential
training about proper data handling and governance considerations are reviewed and
new considerations and/or regulations are introduced. What makes these “events” so
much more successful than simply providing required virtual click-through training
is that you can center your event around a particular topic of recent importance (per‐
haps an internal issue that occurred, a new regulation that’s been launched, or even a
highly publicized issue from an external company). This event structure gives you
some freedom and flexibility around how to go about your training, depending on
what’s most important to either reinforce or disseminate to your staff.
An example of what this type of event could look like is shown in Table 9-1.

198 | Chapter 9: Building a Culture of Data Privacy and Security



Table 9-1. Sample schedule
Monday “Basics of Proper Data Handling and Governance” (one hour)
Tuesday “How to Use Our Governance Tools 101” 

or “Advanced ‘How to’ on Our Governance Tools. Did You Know You Can Do…?!” (one hour)
Wednesday “Governance and Ethics: How and Why Governance Is Everyone’s Responsibility” (one hour)
Thursday “How Do I Handle That? A Guide On What to Do When You Encounter a Governance Concern” (one hour)
Friday Guest speaker on an aspect of governance and/or data culture you find particularly important to your

organization (one hour)

Of course you will have to evaluate what sort of training strategy works best for your
organization, but the takeaway here should be that you need to have an ongoing strat‐
egy—a strategy to address not only the initial training and expertise that your staff
will need, but also how you’re going to continue to reinforce past learning and intro‐
duce new learning.

Communication
Another area we often see overlooked is an intentional plan around communication.
As mentioned with training, communication is also not a “one-and-done” activity. It
is something that should not only be ongoing and consistent but also strategic and
exhaustive. In fact, we would argue that proper communication and an intentional
strategy around it are what fuels a powerfully effective data culture.

Top-down, bottom-up, and everything in between
When thinking about communication in terms of a data governance program, there
are multiple facets to consider. Two common facets of focus are top-down communi‐
cation of the governance program itself and its practices, standards, and expectations,
and bottom-up communication encompassing breaches and problems in governance
being bubbled up for resolution.
These two facets, while clearly important, are only pieces of the communication nec‐
essary to foster a company-wide data culture. These facets are focused simply on the
passage of governance information back and forth, not on how to develop and enrich
the culture of data privacy and security. For data culture, communication needs to
center around the tenets—what the culture for data is within the company, and how
each of those responsible for its success matter and are a part of it.
Additionally, while not specifically training per se, communication that bolsters a
company’s data culture can serve as a reminder and reinforcement not only of the
information covered in said trainings but also of the company’s overall vision and
commitment to its data culture.

Intention, Training, and Communications | 199



Beyond Data Literacy
In the previous section we briefly discussed the value and impact of fostering caring
within the data culture. Here we will explore that more deeply and expand on why
this is such a fundamental part of a successful data culture.

Motivation and Its Cascading Effects
To be sure, education around what data is (namely, what different kinds of data are),
and how data should be treated is essential, but so too is the why. The question of why
data should be handled and treated with respect is an overlooked component of data
culture. Of course it is part and parcel for several of the user hats in the governance
space (legal and privacy tsars, to name a few), but it should be so for other hats as
well.

Motivation and adoption
This isn’t a text on psychology by any means; however, the power of motivation and
its impact on people’s likeliness to do this over that influences greatly whether a data
culture will be adopted or fall by the wayside.
For a data culture to be fully adopted, the motivation really needs to begin at the top
and permeate down from there. Take, for example, a company that needs to put into
place a governance program to abide by new data compliance and security laws. Sev‐
eral people (or teams) within the company know that their data collection and han‐
dling needs to meet higher standards to be in compliance and bring this need to the
C-level. For a governance program to be fully implemented (as we discussed earlier in
this chapter), the C-level folks need to buy into the idea that a governance program is
important and worth their support (including funding). Without C-level support for
a governance program, it’s highly likely that the program would fall short due to lack
of budgetary support and the absence of an ongoing advocacy of a data culture.
C-level involvement and belief in a governance program and implementation of a
data culture has many downstream effects.
First, there is the financial aspect. Without proper funding, a company does not have
the resources in terms of both headcount and tools but also in terms of education of
the data governance space and how to use said tools effectively. Not only does this
influence whether or not a governance program is executed effectively, but it also
affects how well people are able to do their jobs and how happy they are doing those
jobs. As we discussed in Chapter 3, people in the governance space wearing many dif‐
ferent hats and spreading their time across many different tasks—some of which
they’re ill-equipped to do—can lead to decreased job satisfaction and productivity
and ultimately high job turnover (which has its own implications in terms of sunk
cost and lost time).

200 | Chapter 9: Building a Culture of Data Privacy and Security



While C-level buy-in is clearly critical for its consequential effects, there is still the
need to cultivate motivation for best practices within the rest of the workforce. Here
again is where data culture becomes so important. First, the workforce needs to be
informed/educated on why proper data treatment and handling is not only necessary
but ethical. Of note here is that this is not where the reinforcement should stop. A
data culture continues to reinforce this value through the behavior of the company—
the training that is offered/required (and how often), the communications that are
sent out, the behavior of decision makers/those in positions of influence, and finally,
even the way the company is structured. A prime example of this would be a com‐
pany that touts a data culture of privacy and security and yet does not have dedicated
teams and/or resources that support that culture. If there is a disconnect between
what is internally marketed as the data culture and what actually exists to support that
culture, motivation and adoption of the data culture is not only less likely—it’s
improbable at best.

Maintaining Agility
Hopefully in the preceding sections we’ve driven home the need for a very thoughtful,
thorough, and structured approach to creating and cultivating a data culture. An
important aspect of this, one that should not be ignored in inception and creation of
a data culture, is how agility will be maintained. We have seen this be a highly prob‐
lematic area for many companies and would be remiss if we didn’t address its impor‐
tance, as it’s much easier to maintain agility than to create it after the fact.

Agility and Its Benefits on Ever-Changing Regulations
During the course of our research we encountered a very interesting use case relating
to the CCPA. While you may not be encountering compliance with this particular
regulation, the struggles one company is facing may give you some food for thought.
This company is a large retailer that has many different ingestion streams, as it sells
its products in many different locations across the United States. This company has
always struggled to keep track of not only the data that it collects itself, but also the
data it collects from third parties (e.g., a retailer sells its product directly to consumers
on its website but may also sell its product via other retailers, either in their stores or
via their websites).
Part of compliance with CCPA is to track down PII data for a California resident
regardless of where the item was purchased. For example, Ben lives in California but
purchased this company’s product at a large retailer while he was on vacation in Flor‐
ida. The transaction itself occurred in Florida, but because Ben is a California resi‐
dent, he can request that the product company find his purchase data and delete it.

Maintaining Agility | 201



While many companies are now facing this conundrum, the company in this case
study has a highly structured and inflexible governance strategy—one that does not
easily lend itself to quickly adapting to these new requirements. As such, it recognizes
the need to revise its governance strategy, to allow the company to be more agile. The
main way it is spearheading this effort is to focus on building and nurturing a strong
data culture. With so many moving parts to its business (i.e., so much data coming
from so many different places), and with the likelihood that more regulations like
CCPA are on the horizon, this company feels that defining and building a culture that
will support (ever-changing) governance initiatives will be the key to its success. In
this way the company is creating (and has begun enacting) a comprehensive data
culture.

Requirements, Regulations, and Compliance
The most obvious reason for creating a culture around agility is the ever-changing
legal requirements and regulations that data must be in compliance with. To be sure,
the data regulations of today are likely to be less robust than the regulations of tomor‐
row. In Chapter 1 we discussed how the explosion of available data and its collection
has resulted in heaps and heaps of data just sitting there—much of it uncurated.
It is shortsighted and unfeasible to take an approach of simply “handling” the regula‐
tion(s) of the present. The ability to pivot once regulations change or pop up (because
they will) is not just a necessity—it can be made much easier if steps are taken at the
outset.

The Importance of Data Structure
A key aspect to consider when cultivating agility is to “set yourself up for success”—
again, this is a component that can be relied upon and enforced by the data culture.
Note that being “set up for success” is an intersection of how the data warehouse is
structured (the metadata that’s collected, the tags/labels/classification used, etc.) with
the culture that supports or enables the structure to run smoothly. These two in con‐
junction help to make it much easier to pivot when new regulations arise.
This is an area in which we’ve seen many companies struggle. Take, for example, data
that is stored based on the application from which it comes—say, sales data from a
specific company’s retail stores across the US. In the company’s current analytics
structure, this data is tagged in a certain way—perhaps with metadata relating to the
store, purchase amount, time of purchase, and so on.
Now let’s say that a new regulation comes along stating that any data from a customer
who lives in a certain state can be retained for only 15 days. How would this company
easily find all the sales data from customers who live in a certain state? Note that the
criteria is not sales with a certain state but customers, so a customer from state X

202 | Chapter 9: Building a Culture of Data Privacy and Security



could make a purchase in state Y, and if state X is part of the regulation, then that
customer’s data will need to be deleted after 15 days. The company in our example
will have a hard time complying quickly and easily with this regulation if it doesn’t
have the data structure set up to record the location of the purchaser (for the sake of
simplicity, we are assuming here that these are all credit card or mobile pay transac‐
tions that can be traced to each purchaser’s state of residence).
A process and culture from the outset to collect this kind of metadata (whether or not
it is labeled or tagged initially) will make it that much easier to find this class of data
and then tag/label it, and thus attach a retention policy.

Scaling the Governance Process Up and Down
While we have touted the importance of not only the right tools and process but also
the right players (hopefully provided by the buy-in from C-level), there undeniably
are times when things don’t go according to plan, and the process needs to scale (usu‐
ally down, although there is the use case that it could need to scale up).
It could be the case that a company loses headcount due to reorganization, restructur‐
ing, an acquisition, or even a change in data collection (in terms of actual data collec‐
ted and/or in the platform[s] used, tools for transformation, storage types and
locations, or analytics tools). Any of these will likely affect how a governance pro‐
gram functions, and that program needs to be elastic to accommodate for such
changes. The data culture—agreed upon, supported, and reinforced—will aid in suc‐
cessful elasticity.
One strategy we’d like to mention here is one we touched on in Chapter 2: making
sure that the most critical data is tended to first (again, this should be a part of the
data culture). It’s impossible to predict what requirements and regulations may come
in the future, but prioritizing data that is most critical to the business (as well as data
that has any relation back to a person or identifiable entity) will aid in your ability to
scale to accommodate whatever changes may come. At the absolute minimum there
should be a process in place (for example, all types of critical data are always tagged
or classified upon ingestion and/or are always stored in a certain location) that can
quickly address and tend to this category of data.

Interplay with Legal and Security
In this book we’ve discussed at length the different roles and/or “hats” involved in
data governance. When looking at the data culture specifically, two hats of note are
legal and security/privacy tsar, and the interplay between the two and their respective
teams is important.

Interplay with Legal and Security | 203



Staying on Top of Regulations
Regardless of how a company is organized in terms of roles and who does what, there
must be someone who has the task of staying up to date on regulatory requirements
that the company may (or may not) need to be in compliance with. As we’ve previ‐
ously discussed, sometimes this is done by an internal legal representative (attorney)
and at other times this task falls on the privacy tsar or on someone else in security.
What’s important about this task is that it needs to be done early and often, not only
to ensure current compliance but also to help aid in the ability to ensure future com‐
pliance. As we’ve seen over the past 10 years, data handling standards and regulations
have greatly changed—the ability to be aware of what these changes might be and
how best to set up a data culture to flex as needed and be in compliance is critical.
As we’ve discussed several times throughout the book, and especially in Chapter 8,
having an auditing system in place will greatly aid you in monitoring your gover‐
nance strategy over time and also facilitate the task of being on top of compliance and
regulations, so that if (or when) you face an external audit, you will know that you are
in compliance and that nothing unexpected will be found.

Communication
Staying on top of regulations is, however, only half of the story. There must be a pro‐
cess in place for how changes in regulations will be discovered and how these will be
communicated to those who decide on how to proceed. In essence, there needs to be
constant communication back to the decision-making body about what regulations
might be coming up and/or whether there are any changes that need to be made to
current data handling practices in order to be in compliance.
It’s easy to see how this is a process that very much should be a part of the data cul‐
ture within a company.

Interplay in Action
An excellent recent example of this process in action is the reaction to GDPR. All
companies in the EU were well aware that this new regulation was coming and that
changes were needed in order to be compliant. The story was different for companies
in the US, however. During many of our interviews with US companies about their
data governance practices and their plans for the future in terms of changing regula‐
tions, we heard two different approaches: the first was to simply ignore new regula‐
tions until they become a requirement (so in the case of GDPR, not to address
compliance until it is a requirement for US companies); the second was to assume
that the most restrictive regulations in the world could become a requirement and
thus work toward being in compliance now, even before it’s required.

204 | Chapter 9: Building a Culture of Data Privacy and Security



This takes a strong data culture that includes a robust interplay between the gathering
of legal requirements, a group deciding which requirements the company will comply
with, and another group actually carrying out the work of ensuring that compliance
is being met.

Agility Is Still Key
This point really hearkens back to the previous section on agility. It is likely that new
regulations will continue to emerge, and the flexibility of the data structure and sys‐
tem that a company builds would do well to have the potential to be easily modified
or to pivot when needed to accommodate such requirements.

Incident Handling
We’ve talked at length about the importance of process and communication in a suc‐
cessful data culture. One particular process we’d like to spend time unpacking is that
of incident handling—how are breaches in data governance handled, and who is held
responsible?

When “Everyone” Is Responsible, No One Is Responsible
During some of our initial research into how companies structure their data gover‐
nance strategy, one of the questions we asked was, “Who, at the end of the day, is
responsible for improper governance? Whose job is on the line?” Surprisingly, many
companies danced around this question and struggled to give us a direct answer of a
particular person and/or group who would be held accountable in the event of some‐
thing going wrong.
This might seem like an unimportant part of the data culture, but it is actually quite
critical. Earlier in this chapter we spoke about the importance of fostering an envi‐
ronment of caring and responsibility (and indeed this is important), but it also needs
to have a backbone—there must be a person(s) or group(s) who is culpable when
things go wrong. When this structure is lacking, governance becomes “everyone’s”
and yet “no one’s” responsibility. The culture needs to support everyone doing their
part in proper data handling and owning their portion of governance responsibilities,
and it also needs to identify who is the go-to—the end of the line, so to speak, for
specific aspects of the governance strategy.
For example, as part of the data culture, it is on every data analyst in a company to
know what is PII data and whether or not it should be accessed, and/or for what pur‐
poses it can be accessed. A good data culture will support these analysts with educa‐
tion and reinforcements for proper data handling. In the event that an analyst
mistakenly (or malevolently as the case may be) accesses and improperly uses PII
data, someone should be held accountable for that breach. It may be a data engineer

Incident Handling | 205



in charge of enforcing access controls, or even a privacy tsar whose responsibility it is
to set up and manage access policies. In any event, there needs to be someone who
has, as part of their job, the responsibility for ensuring that things go right and
accepting the consequences when things go wrong.
We may be overstating this, but implementing and enforcing responsibility is key here.
People should also be trained in how to be responsible and what this looks like (and
what it doesn’t look like), and this should also be outlined in specific roles from the
outset. Literally listed within a role description’s key tasks should be what that role is
specifically responsible for in terms of data handling, and what the consequences are
for failure to carry out this task. Responsibility should be something that is defined
and agreed upon, as well as trained and communicated on, so that people are taught
how to be responsible and are also held responsible.

Importance of Transparency
Transparency is an oft-forgotten (or intentionally sidestepped) ingredient of gover‐
nance that warrants a deeper dive into not only why it’s important, but also why you
should keep it in mind and most certainly address it as part of building your data
culture.

What It Means to Be Transparent
To be sure, many companies and organizations do not want to disclose everything
about the ins and outs of their data governance structure, which is to be expected and
is not entirely problematic. There is value, however, in a certain amount of transpar‐
ency from an organization, in terms of what data it collects, how it uses the data (and
what for), and what steps/measures it takes to protect data and ensure proper data
handling.

Building Internal Trust
Earlier we touched on the importance of trust in building a data culture, and that
trust goes both ways—bottom up and top down. A key aspect of building that trust
and truly showing employees that it is part of the company’s data culture is to have full
transparency: transparency not only in the items related to data noted above (what
data is collected, how it’s used, governance processes, etc.) but also in what the
incident-handling strategy is. In the previous section we mentioned how important it
is to define the specific person or group that is responsible when things go wrong,
and just as there should be consequences for these folks, there should also be conse‐
quences (albeit different ones) for improper handling by anyone who touches data.
While it may make an organization feel uncomfortable to share so broadly about
when something has gone wrong and how it was handled internally, doing so builds

206 | Chapter 9: Building a Culture of Data Privacy and Security



incredible trust within the company that what is being touted as the data culture is an
integral part of the entire company culture.
Another strategy that can help you build internal trust is that of enabling two-way
communication via a user forum, wherein users of data within your company are able
to voice their concerns and needs. We discussed earlier in this chapter the importance
of communication, but this is an additional facet that is not just informational (you
get to hear from the people actually using the data about why it might be wrong or
what could be better)—it also bolsters the data culture by making all in the organiza‐
tion feel that they’re being heard and are pieces of the greater whole that keeps the
governance program running smoothly.

Building External Trust
In creating a solid and successful data culture, focusing on the internal—the company
nuts and bolts—is obviously extremely important, but the data culture does not and
should not end there. The external perception and trust of a company/organization
should also be considered. Just as showing full transparency internally helps to build
and reinforce the data culture, what is communicated externally about data collec‐
tion, handling, and protection practices is also highly important.
In a sense, the customers of a company or organization are an additional extension of
its “culture.” Customers or consumers should also be considered when thinking about
building a data culture. It’s not just the actions and perceptions of your staff or
employees; it’s also the actions and perceptions of your consumers and/or customers,
and it’s their actions (generally driven by trust) that dictate whether or not they inter‐
act with your company or buy your product.
Providing full transparency externally regarding what data is collected, how it’s used,
how it’s protected, and what has been done to mitigate wrongdoing is critical in
building trust in a company/organization. There are, to be sure, cases in which cus‐
tomers/people have no choice but to choose one company or organization for a ser‐
vice/purchase/etc., but in the event that there is choice, customers/people are much
more likely to choose a company that they trust over one they don’t trust or are
unsure of.
The data culture in essence should not be just an internally decided-upon practice but
one that also includes how the company or organization sits in the world—what it
wants the world to know about how it handles data and its commitment to compli‐
ance and proper data handling.

Importance of Transparency | 207



Setting an Example
It may sound lofty, but another aspect around the importance and maybe even the
power of transparency is that it can teach and/or inspire others to adopt similar data
governance practices and data culture.
If every company or organization were to create, implement, and enforce the gover‐
nance principles and practices we’ve laid out, not only would there be excellent gov‐
ernance and data culture within each organization, but there would also be a cross-
company, cross-organizational, cross-product, cross-geographical data culture.

Summary
Throughout the course of this text you have learned about all the aspects and facets to
be considered when creating your own successful data governance program. We hope
that we have educated you not only on all of the facets and features of data itself, gov‐
ernance tools, and the people and processes to consider, but also on the importance
of looking at your governance program in a much broader sense to include things
such as long-term monitoring and creating a data culture to ensure governance
success.
You should walk away from this book feeling that you know what makes up a gover‐
nance program (some aspects that you may have already heard of, and hopefully
some others that you hadn’t considered before) and that you know how to put those
pieces together to create and maintain a powerful, flexible, and enduring program
that not only meets but also exceeds regulation, compliance, and ethical and societal
standards.

208 | Chapter 9: Building a Culture of Data Privacy and Security



APPENDIX A
Google’s Internal Data Governance

In order to understand data governance, it is good to look at a practical example of a
company with a deep investment in the topic. We (the authors) are all Google
employees, and we believe that in Google we have a great example of a deeply
ingrained process, and a fine example of tooling.

The Business Case for Google’s Data Governance
Google has user privacy at the top of its priorities and has published strong privacy
principles that guide us throughout all product development cycles. These privacy
principles include, as top priorities, respect for user’s privacy, being transparent about
data collection, and taking the utmost care with respecting the protection of users’
data, and they have ensured that good data governance is at the core of Google.
Before we dive into the specifics of data governance and management at Google, it is
crucial to understand the motivations behind Google’s data collection, and the use
case. This is a good approach for any undertaking. Google provides access to search
results and videos and presents advertisements alongside the search results. Google’s
income (while more diversified than a few years ago) is largely attributable to ads.
Given the importance of ads, a large portion of the effort in Google is focused on
making ads relevant. Google does this by collecting data about end users, indexing
this data, and personalizing the ads served for each user.
Google is transparent about this information: when someone uses Google services—
for example, when they do a search on Google, get directions on Maps, or watch a
video on YouTube—Google collects data to personalize these services. This can
include highlighting videos the user has watched in the past, showing ads that are
more relevant to the user depending on their location or the websites they frequent,
and updating the apps, browsers, and devices they use to access Google services. For

209



example, the results and corresponding ads might be different when a search is made
by a user on a mobile device and navigating in Google Maps versus a search made
from a desktop using a web browser. The personal information associated with the
user’s Google account is available if the user is signed in. This can include the user’s
name, birthday, gender, password, and phone number. Depending on the Google
property they are using, this can also include emails they have written and received (if
they are using Gmail); photos and videos they have saved (if they are using Google
Photos); documents, spreadsheets, and slides they have created (if they are using
Google Drive); comments they have made on YouTube; contacts they added in Goo‐
gle Contacts; and/or events on their Google Calendar. It is necessary that all this user
information is protected when using Google services.
Google, therefore, provides each user with transparency and control of how their per‐
sonal information is used. Users can find out how their ads are personalized by going
to Google Ad Settings and can control the personalization therein. They can also turn
off personalization and even take out their data. They can review their activity within
Google domains and delete or control activity collection. This level of transparency
and control is something that users expect in order to be comfortable with providing
a business with their personal information. Google maintains transparency around
what data it collects and how this information is used to generate the revenues men‐
tioned above. If you are collecting personal information and using it to personalize
your services, you should provide similar mechanisms for your customers to view,
control, and redact your use of their personal data.
Given all the information Google collects, it is not surprising that Google is publicly
committed to protecting that information and ensuring privacy. Google is meticulous
about external certifications and accreditations and provides tools for individual con‐
sumers to control the data collected about them.

The Scale of Google’s Data Governance
Google keeps some information about itself private—for example, how much data it
actually collects and manages. Some public information provides a general sense,
such as Google’s reported investment of $10 billion in offices and data centers in
2020.1 A third-party attempt at estimating Google’s data storage capacity using public
sources of information came up with 10EB (exabytes) of information.2

1 Carrie Mihalcik, “Google to Spend $10 Billion on Offices, Data Centers in US This Year”, CNET, February 26,
2020.

2 James Zetlen, “Google’s Datacenters on Punch Cards”, XKCD.

210 | Appendix A: Google’s Internal Data Governance



Some further information about the Google data cataloging effort, its scale, and the
approaches taken to organizing data is described in the Google “Goods” paper.3 This
paper discusses the Google Dataset Search (GOODS) approach, which does not rely
on stakeholder support but works in the background to gather metadata and index
that metadata. The resulting catalog can be leveraged to further annotate technical
metadata with business information.
So, with this significant amount and variety of information, much of it likely sensi‐
tive, how does Google protect the data that it has collected and ensure that privacy
while keeping the data usable?
Google has published certain papers about the tools used, and we can discuss these.

Google’s Governance Process
There are various privacy commitments Google needs to respect and comply with, in
particular around user data: regulations, privacy policies, external communications,
and best practices. However, it is generally hard to:

• Make nontrivial global statements (e.g., all data is deleted on time)
• Answer specific questions (e.g., do you never record user location if setting X is

switched off?)
• Make informed decisions (e.g., is it OK to add this googler to that group?)
• Consistently assert rules or invariants at all times

The ideal state is one in which we have a comprehensive understanding of Google’s
data and production systems and automatically enforced data policies and obliga‐
tions. In the ideal state:
The lives of Google employees are easier

• Taking the privacy- and security-preserving path is easier than taking an insecure
path.

• Privacy bureaucracy is reduced via automation.
Google can do more…

• Developers use data without worrying about introducing security and privacy
risks.

• Developers can make privacy a product feature.
• Google can make data-supported external privacy statements with confidence.

3 Alon Halevy et al., “Goods: Organizing Google’s Dataset” (presented at SIGMOD/PODS’16: International
Conference on Management of Data, San Francisco, CA, June 2016).

Google’s Internal Data Governance | 211



…while ensuring security and privacy
• Google can prevent privacy problems before they happen.
• Data obligations (policies, contracts, best practices) are objective and enforceable.

Every single feature Google produces and releases undergoes scrutiny by specialized
teams external to the core development team. These teams review the product from
the following aspects:
Privacy

• The team looks carefully at any user data collected and reviews justification for
the collection of this data. This includes reviewing what data, if collected, is going
to be visible to Google employees, and under what constraints. The team also
ensures that consent was provided for the data collected, and whether encryption
is applied and audit logging is enabled for that data.

• In addition, we look at compliance—we make sure that when users choose to
delete data, it is verifiably removed within Google’s committed SLAs, and that
Google has monitoring on all data retained (so that we can ensure minimal
retention is preserved).

• By verifying compliance and policies before even starting the collection of data,
we limit a significant number of challenges ahead of time.

Security
This is a technical review targeted at scrutinizing the design and architecture of
the code according to best practices to potentially head off future incidents
resulting from security flaws. Since most capabilities Google launches are
exposed to the web, and despite the fact that there are always multiple layers of
security, we always provide for an additional review. Web threats are present and
evolving, and a review by subject matter experts is beneficial to all.

Legal
This is a review from a corporate governance standpoint, making sure the prod‐
uct or service launched is compliant with export regulations and explicitly getting
a review from a regulation perspective.

(There are other approvers for a release, naturally, but we will focus on those related
to data governance.)
Google maintains additional certifications, with a common theme of most of those
being third-party verified.

212 | Appendix A: Google’s Internal Data Governance



How Does Google Handle Data?
Much of the information Google holds goes into a central database, where it is held
under strict controls. We have already shared information about the likely contents of
this database; now let’s focus on the controls around this database.

Privacy Safe—ADH as a Case Study
ADH—or Ads Data Hub—is a tool Google provides that allows you to join data you
collect yourself (e.g., Google ad campaign events) with Google’s own data about the
same constituents. Yet it does so without breaching the privacy or trust of the individ‐
uals inspected. The ways ADH accomplishes this are indicative of the care Google
takes with respect to data. There are several mechanisms working in conjunction to
provide multiple layers of protection:
Static checks

ADH looks for obvious breaches, such as listing out user IDs, and blocks certain
analytics functions that can potentially expose user IDs or distill a single user’s
information.

Aggregations
ADH makes sure to respond only with aggregates, so that each row in the
response to a query corresponds to multiple users, beyond a minimal threshold.
This prevents the identification of any individual user. For most queries, you can
only receive reporting data on 50 or more users. Filtered rows are those omitted
from the results without notification.

Differential requirements
Differential requirements compare results from the current query operation
you’re running to your previous results, as well as rows from the same result set.
This is designed to help prevent the user from gathering information about indi‐
vidual users by comparing data from multiple sets of users that meet our aggre‐
gation requirements. Differential requirement violations can be triggered by
changes to your underlying data between two jobs.

Google’s Internal Data Governance | 213



ADH Uses Differential Privacy
Businesses that advertise on Google often want to measure how well their marketing
is working. To do so, it is essential to be able to measure the performance of their ads.
A local restaurant that has advertised on Google will want to know how many people
who were served a restaurant ad actually came to the restaurant. How can Google
provide advertisers the ability to do customized analysis that aligns with the business
(e.g., how many patrons placed an online order after seeing the ad on Google), con‐
sidering such analysis will require joining information on who the ads are served to
with the restaurant’s own customer transactions database?
ADH uses differential checks to enable customized analysis while respecting user pri‐
vacy and upholding Google’s high standards of data security. Differential checks are
applied to ensure that users can’t be identified through the comparison of multiple
sufficiently aggregated results. When comparing a job’s results to previous results,
ADH needs to look for vulnerabilities on the level of individual users. Because of this,
even results from different campaigns, or results that report the same number of
users, might have to be filtered if they have a large number of overlapping users. On
the other hand, two aggregated result sets may have the same number of users—
appearing identical—but not share individual users and would therefore be privacy-
safe, in which case they wouldn’t be filtered. ADH uses data from historical results
when considering the vulnerability of a new result. This means that running the same
query over and over again creates more data for differential checks to use when con‐
sidering a new result’s vulnerability. Additionally, the underlying data can change,
leading to privacy check violations on queries thought to be stable.

These techniques are sometimes referred to as differential privacy.4

The ADH case study exemplifies Google’s cultural approach to handling data: beyond
the process part mentioned previously, Google has built a system that provides value
while at the same time ensuring safeguards and prevention techniques that put user
privacy at the forefront. Google has captured some of the capabilities in a “differential
privacy library”.
For another case study, consider the capabilities of Gmail. Google has built tools to
extract structured data from emails. These tools enable assistive experiences, such as
reminding the user when a bill payment is due or answering queries about the depar‐
ture time of a booked flight. They can also be combined with other information to do
seemingly magical things like proactively surfacing an emailed discount coupon

4 Some of the these techniques are described in a paper Google published entitled “Differentially Private SQL
with Bounded User Contribution”.

214 | Appendix A: Google’s Internal Data Governance



while the user is at that store. All of the above is accomplished by scanning the user’s
personal email while still maintaining that user’s privacy. Remember, Google person‐
nel are not allowed to view any single email. This is presented in the paper “Anatomy
of a Privacy-Safe Large-Scale Information Extraction System over Email”. This capa‐
bility to scan information and make it accessible to the information’s owner, while at
the same time maintaining privacy, is accomplished through the fact that most emails
are business-to-consumer emails, and those emails from a single business to many
consumers share the same template. You can, without human intervention, backtrack
groups of emails to the business, generate a template that is devoid of any potential
information (differentiating between the boilerplate portions and the transient sec‐
tion), and then build an extraction template. The paper goes into more detail.

Google’s Internal Data Governance | 215






APPENDIX B
Additional Resources

The following are some of the works we consulted while writing this book. This is not
intended to be a complete list of the resources we used, but we hope that some of you
will find this selection helpful as you learn more about data governance.

Chapter 4: Data Governance over a Data Life Cycke

• Association Analytics. “How to Develop a Data Governance Policy”. September
27, 2016.

• Australian Catholic University. “Data and Information Governance Policy”.
Revised January 1, 2018.

• Michener, William K. “Ten Simple Rules for Creating a Good Data Management
Plan”. PLOS Computational Biology 11, no. 10 (October 2015): e1004525.

• Mohan, Sanjeev. “Applying Effective Data Governance to Secure Your Data Lake”.
Gartner, Inc. April 17, 2018.

• Pratt, Mary K. “What Is a Data Governance Policy?”. TechTarget. Updated Febru‐
ary 2020.

• Profisee Group, Inc. “Data Governance—What, Why, How, Who & 15 Best Prac‐
tices”. April 12, 2019.

• Smartsheet, Inc. “How to Create a Data Governance Plan to Gain Control of
Your Data Assets”. Accessed February 26, 2021.

• TechTarget. “What Is Data Life Cycle Management (DLM)?”. Updated August
2010.

• USGS. “Data Management Plans”. Accessed February 26, 2021.

217



• Watts, Stephen. “Data Lifecycle Management (DLM) Explained”. The Business of
IT (blog). BMC. June 26, 2018.

• Wikipedia. “Data governance”. Last modified February 4, 2021.
• Wing, Jeannette M. “The Data Life Cycle”. Harvard Data Science Review 1, no. 1

(Summer 2019).

Chapter 8: Monitoring

• Alm, Jens, ed. Action for Good Governance in International Sport Organisations.
Copenhagen: Play the Game/Danish Institute for Sports Studies. March 2013.

• Cyborg Institute.“Infrastructure Monitoring for Everyone”. Accessed February
26, 2020.

• Ellingwood, Justin. “An Introduction to Metrics, Monitoring, and Alerting”. Dig‐
italOcean. December 5, 2017.

• Goldman, Todd. “LESSON—Data Quality Monitoring: The Basis for Ongoing
Information Quality Management”. Transforming Data with Intelligence. May 8,
2007.

• Grosvenor Performance Group. “How Is Your Program Going…Really? Perfor‐
mance Monitoring”. MAy 15, 2018.

• Henderson, Liz. “35 Metrics You Should Use to Monitor Data Governance”.
Datafloq. October 28, 2015.

• Karel, Rob. “Monitoring Data with Data Monitoring Tools | Informatica US”.
Informatica (blog). January 2, 2014.

• Pandora FMS. “The Importance of Having a Good Monitoring System? Offer the
Best Service for Your Clients” (blog post). September 19, 2017.

• Redscan. “Cyber Security Monitoring”. Accessed February 26, 2021.
• Wells, Charles. “Leveraging Monitoring Governance: How Service Providers Can

Boost Operational Efficiency and Scalability…”. CA Technologies. January 19,
2018.

• Wikipedia. “Data Lineage”. Last modified November 17, 2020.

218 | Appendix B: Additional Resources



Index

A regulation and, 201
access control, 6-8 requirements, regulations, and compliance,

evolution of methods, 75 202, 205
granularity, 137 scaling of governance, 203
identity and access management, 52 AI (artificial intelligence)
as policy, 41 case study: water meters, 118-120
regulation around fine-grained access con‐ data quality in, 117-120

trol, 26 alerting, 177
access logs, 18 Amazon
access management, 158-165 bias in automated recruiting tool, 17

Access Transparency, 165 data-driven decision making, 14
authentication, 158 Amazon Web Services (AWS) Nitro Enclaves,
authorization, 159 149
data loss prevention, 161 analysis, real-time, 187
differential privacy, 164 analytical data, life cycle of (see data life cycle)
encryption, 162 analytics
identity and access management, 52 business case for data culture, 195
policies, 160 in monitoring system, 188
user authorization and, 54 ancillaries, 58

Access Transparency, 165 annotation of data, 123
accountability, in Capital One data leak, 18-19 API log, 136
accounting (monitoring), 177 approvers, defined, 7
acquisitions, large companies and, 73 artificial intelligence (AI)
active monitoring systems, 189 case study: water meters, 118-120
addresses, deduplication of, 125 data quality in, 117-120
Ads Data Hub (ADH), 213-215 attestations, in binary authorization, 157
advertising, Google and, 209 attestors, in binary authorization, 157
agile data protection, 165-167 audit logging, regulatory compliance and, 28

data lineage, 166 audit trail, 24
event threat detection, 167 auditing (in monitoring), 177
security health analytics, 165 auditing (of system)

agility, data culture and data lineage and, 141
data structure and, 202 in framework, xv
maintaining agility, 201-203 authentication, 52, 158

219



Authoritative Vessel Identification Service effect on data governance, 9
(AVIS), 19-22 use case, 201

authorization, 54, 159 central data dictionary, 66
automation classification (see data classification and orga‐

data protection, 96, 97 nization)
lineage collection, 135 cloud

AWS (Amazon Web Services) Nitro Enclaves, data governance advantages in public cloud,
149 30-34

data protection in, 146-149
B moving data to, 30
Benford's Law, 144 multi-tenancy, 147
best practices security surface, 147

data breach readiness, 171 virtual machine security, 148
data deletion process, 170 virtual private cloud service controls, 155
data protection, 167-173 Cloud Data Loss Prevention (DLP), 161
electronic medical devices and OS software cloud-native companies, 67

upgrades, 170 CMEK (Customer Managed Encryption Keys),
physical security, 168 163
portable device encryption/policy, 170 Coast Guard, US, 19-22
separated network designs for data protec‐ Coast, Steve, 94

tion, 168 collection of data (see data collection)
big data analytics, 116 company persona, 195
big data, defined, 29 company, public perception of, 195
binary authorization, 156 completeness of data, 129
bottom-up communication, 199 compliance
business analyst, 64, 81 audit logging, 28
business case for data governance, xii, 194-196 business value of data governance, 26-28

analytics and bottom line, 195 challenges for companies, 76
building, 106 data destruction and, 89
company persona/perception, 195 data governance considerations for organi‐
for Google's internal data governance, 209 zations, 29
importance of matching to data use, 121 data lineage and, 141
top-down buy-in success story, 196 data retention/deletion, 27, 97

business value of data governance, 23-30 gaming of metrics, 76
data governance considerations for organi‐ monitoring and, 177, 182

zations, 28-30 regulation around fine-grained access con‐
fostering innovation, 23 trol, 26
regulatory compliance, 26-28 sensitive data classes, 28
risk management, 25 Confidential Compute, 148
tension between data governance and contact tracing, 60

democratizing data analysis, 24 Context-Aware Access, 161
costs

C of bad data, 116
of cyberattacks, 185

C-suite, 64 of security breaches, 145
California Design Den, 15 COVID-19
Capital One data leak case study, 18-19 California's collection of data on, 116
caring, as essential component of data culture, location history privacy issues, 59-61

197, 200-201 culture (see data culture)
CCPA (California Consumer Privacy Act), 152

220 | Index



Customer Managed Encryption Keys (CMEK), data collection
163 advances in methods of, 10

customer support specialists, 64 increase in types of data collected, 13
customers, as extension of company culture, in sports, 11-13

207 data completeness, 129
customization, of monitoring system, 188 data corruption, 26

data creation
D as data life cycle phase, 87
data defining type of data, 95

in business context, 75 in practice, 95
decision-making and, 14-16 data culture
enhancing trust in, 5 analytics and bottom line, 195
evolution of access methods, 75 benefits of data governance to the business,
use case expansion, 14-16 194-196

data access management, xv building a, 193-208
data accumulation, 29 caring and, 200-201
data acquisition communication planning, 199

defined, 87 data governance policy, 110
identity and access management, 52 definition/importance of, 193
workflow management for, 52 incident handling, 205

data analysis as intentional, 197
data quality in big data analytics, 116 interplay with legal and security, 203
tension between data governance and maintaining agility, 201-203

democratizing data analysis, 24 monitoring and, 191
data analyst, 63 motivation and adoption, 200
data archiving motivation and its cascading effects, 200

automated data protection plan, 97 privacy/security and, 83, 193-208
as data life cycle phase, 89 staying on top of regulations, 204
in practice, 96 training for, 197-199

data assessment, 45 transparency and, 206
data breaches data deduplication, 124

Equifax, 167 data deletion, 50-52
healthcare industry, 171-173 best practices, 170
physical data breach in Texas, 169 regulatory compliance and, 27
portable devices and, 170 data democratization, 24
readiness, 171 data destruction

data capture, 87 compliance policy/timeline for, 97
data catalog, 24 as data life cycle phase, 89
data cataloging, xiv, 44 in practice, 97
data change management, 141 data discovery and assessment, xiv
data classes data enablement, 8

in enterprise dictionary, 38-40 data encryption (see encryption)
policies and, 40-43 data enrichment, 65

data classification and organization, 43 cloud dataset, 78
access control and, 6-8 manual process of, 75
automation of, 43 data entry, 87
data classification and organization, 43 data exfiltration, 153-158
data protection and, 146 secure code, 156
in framework, xiv virtual private cloud service controls, 155

Index | 221



zero-trust model, 157 developing a policy, 103
data governance (generally) importance of, 102

in action, 17-22 location of data, 110
advances in data collection methods, 10 organizational culture, 110
basics, 1-35 roles and responsibilities, 105
business benefits of robust governance, xvii step-by-step guidance, 106-108
business case for (see business case for data structure of, 103-105

governance) data handling, at Google, 213-215
business need for, xii data in flight, governance of, 133-142
business value of, 23-30 data transformations, 133
classification/access control, 6-8 lineage, 134-140
cloud-based data storage advantages, 30-34 policy management, simulation, monitor‐
data enablement/security versus, 8 ing, change management, 141
defined, xiii security, 151
elements of, 2-8 data infrastructure, complexity of, 30
enhancing trust in data, 5 data life cycle, 85-111
ethical concerns around use of data, 16 applying governance over, 92-100
growth of number of people working with case study: information governance, 99

data, 10 data archiving phase, 89, 96
growth of size of data, 9 data creation phase, 87, 95
holistic approach to, 4 data destruction phase, 89, 97
improving data quality, 19-22 data governance frameworks, 92-94
increased importance of, 9-17 data governance in practice, 94-97
internal data governance at Google, 209-215 data governance policy considerations,
managing discoverability/security/account‐ 108-111

ability, 18-19 data governance throughout, 85-111
new laws/regulations around treatment of data management plan, 90-92

data, 16 data processing phase, 88, 95
operationalizing (see operationalizing data data storage phase, 88, 96

governance) data usage phase, 88, 96
policy (see data governance policy) defined, 85
process (see process of data governance) example of how data moves through a data
program performance monitoring, 183-185 platform, 97
purpose of, 1 management of, 90-92
scaling of process, 203 operationalizing data governance, 100
tension between data governance and phases of, 86-90

democratizing data analysis, 24 data lifecycle management (DLM), 90-92
tools (see tools of data governance) data lineage, 134-140

data governance charter template, 101 agile data protection and, 166
data governance framework (see framework) auditing and compliance, 141
data governance policy collecting, 135

access management and, 160 data protection planning and, 144
changing regulatory environment, 109 defined, 46
complexity and cost, 109 key governance applications that rely on lin‐
considerations across a data life cycle, eage, 139

108-111 monitoring, 180
data destruction and, 89 policy management, simulation, monitor‐
defined, 101 ing, change management, 141
deployment time, 108 as temporal state, 138

222 | Index



tracking of, 46, 128 big data analytics and, 116
types of, 136 cascading problems with tribal knowledge,
usefulness of, 135 124

data locality, 31 data protection planning and, 144
data loss prevention, access management and, data quality management as part of frame‐

161 work, xv
data maintenance (see data processing) defined, 113
data management plan (DMP), 90-92 documenting data quality expectations, 95
data outliers (see outliers) importance of, 114-120
data owner, 62, 80 improving, 19-22
data policies management processes in enterprise dictio‐

data classes and, 40-43 nary, 46
per-use case, 42 matching business case to data use, 121

data processing monitoring, 179
as data life cycle phase, 88 prioritization, 123
documenting data quality expectations, 95 profiling, 124-129
in practice, 95 reasons for including in data governance

data profiling, 45 program, 121
data completeness, 129 scorecard for data sources, 123
data deduplication, 124 techniques for, 121-130
data quality and, 124-129 unexpected sources for, 129
dataset source quality ranking for conflict at US Coast Guard, 19-22

resolution, 129 data recovery, automated, 96
lineage tracking, 128 data retention, 50-52
merging datasets, 129 case study, 51
outliers, 127 regulatory compliance and, 27

data protection, 143-174 data scientist, 63
as framework element, xv data security, 8
automated, 96 Capital One data leak case study, 18-19
automated data protection plan, 97 creating culture of, 83
best practices, 167-173 data governance versus, 8
classification and, 146 healthcare industry security breaches,
in the cloud, 146-149 171-173
data breach readiness, 171 key management and encryption, 47-49
data deletion process, 170 physical, 149-152
data exfiltration, 153-158 security monitoring, 185
identity/access management, 158-165 Zoom bombing, 152
keeping data protection agile, 165-167 data segregation
level of protection, 145 ownership by line of business, 80-82
lineage and quality, 144 within storage systems, 77-79
network security, 151 data sensitivity, 135
physical security, 149-152 data steward, 7, 63, 81
planning, 143-146 data storage
portable device encryption/policy, 170 automated data protection and recovery, 96
security in transit, 151 as data life cycle phase, 88

data provenance (see data lineage) in practice, 96
data quality, 113-131 key management and encryption, 47-49

AI/ML models and, 117-120 data structure, agility and, 202
annotation, 123 data theft, managing risk of, 25

Index | 223



data transformations, 133 ETL (extract-transform-load), 116, 133
data usage European Union (EU), 3

access management and, 96 AI regulations, 17
as data life cycle phase, 88 Spotify regulations, 3
in practice, 96 event threat detection, 167

data validation, 133 exfiltration (see data exfiltration)
data-driven decision making, 14 Experian, 116
datasets external auditor, 64

"views" of, 82 external trust, 207
merging, 129 extract-transform-load (ETL), 116, 133
source quality ranking for conflict resolu‐ extraction, defined, 133

tion, 129
debugging, 139 F
decision-making, data and, 14-16 filters, 121
deduplication, 124 fine-grained access control, 26
deletion of data (see data deletion) framework, xiv
democratization of data, 24 accountability, 107
differential privacy, 59, 164, 214 and data life cycle, 92-94
digital-only (cloud-native) companies, 67
director of data governance (see privacy tsar)
discoverability, 18-19 G
DLM (data life cycle management), 90-92 GDPR (General Data Protection Regulation),
DLP (Cloud Data Loss Prevention), 161 xi, 26
DMP (data management plan), 90-92 data retention rules, 51
documenting data quality expectations, 95 data sovereignty measures, 31

effect on data governance, 9
E right to be forgotten, 76

US companies and, 204
education, when operationalizing data gover‐ Gmail, 214

nance, 108 Google
electronic medical devices, 170 Ads Data Hub case study, 213-215
employees (see people, data governance and) business case for internal data governance,
encryption, 151 209

access management and, 162 data handling at, 32, 213-215
key management and, 47-49 governance process, 211-212
portable devices and, 170 internal data governance at, 209-215

enterprise dictionary, 37 protocol buffers, 151
data assessment/profiling, 45 scale of internal data governance, 210
data cataloging/metadata management, 44 Google Cloud
data classes and policies, 40-43 encrypted data in, 48
data classes in, 38-40 Shielded VM, 148
data classification and organization, 43 Google Dataset Search (GOODS), 211
data quality management processes, 46 governance manager (see privacy tsar)
data retention/deletion, 50-52 governance process (see process of data gover‐
key management and encryption, 47-49 nance)
lineage tracking, 46 governors, defined, 7
workflow management for data acquisition, graphic visualization, in monitoring system,

52 188
Equifax data breach, 167, 185 gRPC framework, 151
ethics, data use and, 16

224 | Index



guiding principles, developing/documenting, lineage graph, 136
106 lineage tracking, 128

in enterprise dictionary, 46
H time/cost of, 47
"hats" location of data, 110

and company structure, 74
Health Insurance Portability and Accountabil‐ M

ity Act of 1996 (HIPAA), 16 machine learning (ML)
healthcare industry data governance and, 4

cascading data quality problems with tribal data quality in, 117-120
knowledge, 124 management buy-in, 107

data protection best practices, 167 maritime domain awareness (MDA), 21
need for comprehensive data governance, Mars Climate Orbiter, 100

196 MaxMind, 115
security breaches, 171-173 medical records, 167

Herzberg, Elaine, 16 mergers and acquisitions, large companies and,
highly regulated companies, 69-71 73

metadata
I analytics in legacy companies and, 66
identity and access management (IAM) sys‐ data enrichment and, 65

tems, 52, 147 data lineage and, 135
identity-aware proxy (IAP), 160 metadata catalog, 24
IMPACT (teacher ranking system), 76 metadata management
incident handling, 205 in enterprise dictionary, 44
infotypes, 37 as part of framework, xiv
inheritance, 141 metrics, gaming of, 76
innovation, fostering with data governance, 23 Microsoft Azure, 148
internal data governance, at Google, 209-215 misuse of data, 25
internal trust, building, 206 ML (machine learning)

data governance and, 4

J data quality in, 117-120
monitoring, 175-191

Jacob, Oren, 50 compliance monitoring, 182
criteria for, 189

K data lineage monitoring, 180
k-anonymity, 164 data quality monitoring, 179
key management, 47-49 defined, 175
key management system (KMS), 163 important reminders for, 190
knowledge workers, 8 key areas to monitor, 179-187

program performance monitoring, 183-185
L reasons to perform, 176-178
labeling of resources in public cloud, 33 security monitoring, 185
laptops, 170 system features, 187-189
large companies, 72 monitoring system
legacy companies, 66 analysis in real-time, 187
legal "hat", 58 customization in, 188
legal issues (see regulations; compliance) features of, 187-189
life cycle (see data life cycle) graphic visualization in, 188
lineage (see data lineage) notifications in, 187

Index | 225



reporting/analytics in, 188 data segregation within storage systems,
system alerts, 187 77-79

multi-tenancy, 147, 153 for physical security, 149
growth of number of people working with

N data, 10
names, deduplication of, 125 "hats," defined, 58
NASA, 100 "hats" versus "roles" and company structure,
National Football League (NFL), 11-13 74
National Health Service (NHS), 167 legal "hat", 58
natural experiments, 99 people–data governance process synergy,
network security, 151 73-84
Newett, Edward, 3 regulation compliance, 76
Next Gen Stats (NGS), 11 roles, responsibilities, and "hats", 57
NFL (National Football League), 11-13 tribal knowledge and subject matter exerts,
NHS (National Health Service), 167 74
Nike Zoom Vaporfly 4% ad campaign, 99 per-use case data policies, 42
Nitro Enclaves, 149 perimeter network security model, 151
normalization, 45, 134 personally identifiable information (PII) (see
notifications, in monitoring system, 187 PII)

personnel (see people, data governance and)

O physical security, 149-152
best practices, 168

ontologies, 107 network security, 151
OpenStreetMap (OSM), 94 physical data breach in Texas, 169
operating model, 107 security in transit, 151
operationalizing data governance, xvi, 100 PII (personally identifiable information)

considerations across a data life cycle, data class hierarchy and, 39
108-111 data deletion and, 50

data governance policy defined, 101 defined, 40
developing a policy, 103 lineage collection, 136, 139
importance of a data governance policy, 102 policy book (see enterprise dictionary)
policy structure, 103-105 policy(ies) (see data governance policy)
roles and responsibilities, 105 portable devices, 170
step-by-step guidance, 106-108 prioritization of data, 123

OS software upgrades, 170 privacy
OSM (OpenStreetMap), 94 creating culture of, 83
outliers, 45, 114, 127 differential privacy, 164

privacy tsar, 59
P community mobility reports, 59
passive monitoring systems, 189 exposure notifications, 60
people, data governance and, 57-65 process of data governance, 65-73

considerations and issues, 74-77 cloud-native companies and, 67-69
creation of "views" of datasets, 82 at Google, 211-212
culture of privacy/security, 83 highly regulated companies, 69-71
data definition, 75 large companies, 72
data enrichment, 65 legacy companies and, 66
data segregation and ownership by line of legacy retail companies, 68

business, 80-82 people and (see people, data governance
and)

226 | Index



scaling of, 203 S
small companies, 71 Safari Books Online, 14-15

profiling (see data profiling) scaling of governance process, 203
program performance monitoring, 183-185 Science Applications International Corp (SAIC)
ProPublica, 17 data breach, 169
protocol buffers, 151 scorecard, for data sources, 123
provenance (see data lineage) security (see data protection; data security)
public cloud security breaches (see data breaches)

Capital One data leak, 18 security monitoring, 185
data governance in, 30-34 security teams, 147
data locality and, 31 self-driving cars, 16
ephemeral computing, 32 sensitive data classes, 28
labeled resources, 33 sensitivity of source data, 135
reduced surface area, 32 Shielded VM, 148
security issues, 34 signers, in binary authorization, 157
as serverless and powerful, 32 small companies, data governance in, 71

public perception of company, 195, 207 SMEs (subject matter exerts), 74, 81
purging (see data destruction) Society of Vertebrate Paleontology, 121

software upgrades, 170
Q sports, advanced data collection in, 11-13
quality (see data quality) Spotify, 2-4
quality ranking, 129 storage systems, data segregation within, 77-79

Strava, 99
R streaming data (see data in flight, governance
RACI matrix (responsible, accountable, consul‐ of)

ted, and informed), 105 street addresses, deduplication of, 125
regulations subject matter exerts (SMEs), 74, 81

agility and compliance, 201, 205 Susman, Gayln, 50
around treatment of data, 16 system access logs, 18
communicating regulatory changes, 204 system alerts, 187
compliance (see compliance)
data destruction and, 89 T
data governance considerations for organi‐ tagging, 33

zations, 29 taxonomies, developing, 107
data governance policy regulatory environ‐ teacher rating systems, 76

ment, 109 technology stack, 107
highly regulated companies, 69-71 Telco, 25
staying on top of, 204 tenets, of company, 197

regulatory compliance (see compliance) test dataset, 117
reporting, in monitoring system, 188 text data filters, 121
responsibility, incident handling and, 205 theft (see data theft)
retail companies tools of data governance, 37-55

compliance use case, 201 data assessment/profiling, 45
data governance process for, 67-69 data cataloging/metadata management, 44
legacy companies, 68 data classes and policies, 40-43
regulation issues, 202 data classes in, 38-40

retention of data (see data retention) data classification and organization, 43
right to be forgotten, 76 data quality management processes, 46
risk management, 25 data retention/deletion, 50-52

Index | 227



enterprise dictionary, 37 expansion in, 14-16
identity and access management, 52 importance of use case and policy manage‐
key management and encryption, 47-49 ment, 43
lineage tracking, 46 per-use case data policies, 42
user authorization and access management, user authentication, 52, 158

54 user forums, 207
workflow management for data acquisition, user roles, company structure and, 74

52 users, defined, 7
top-down communication, 199
Toy Story 2 (movie), 50 V
training, 108 validation dataset, 117
training dataset, 117 validation of data, 133
transactional systems, 86 views of datasets, 82
transformations (see data transformations) virtual machine (VM), 148
transparency virtual private cloud service controls (VPC-

building external trust, 207 SC), 155
building internal trust, 206
data culture and, 206
meaning of, 206 W
setting an example, 208 Washington, DC, 76

tribal knowledge, 74, 124 water meter case study, 118-120
TRICARE data breach, 169 workflow management, for data acquisition, 52
trust workforce (see people, data governance and)

and purpose of data governance, 1
enhancing trust in data, 5 Z
external, 207 zero-trust model, 157
internal, 206 Zippilli, Dom, 19

Zoom bombing, 152
U
use cases, for data

228 | Index



About the Authors
Evren Eryurek, PhD, is the leader of the data analytics and data management portfo‐
lio of Google Cloud, covering Streaming Analytics, Dataflow, Beam, Messaging
(Pub/Sub & Confluent Kafka), Data Governance, Data Catalog & Discovery, and Data
Marketplace as the director of product management.
He joined Google Cloud as the technical director in the CTO Office of Google Cloud,
leading Google Cloud in its efforts toward Industrial Enterprise Solutions. Google
Cloud business established the CTO Office and is still building a team of the world’s
foremost experts on cloud computing, analytics, AI, and machine learning to work
with global companies as trusted advisors and partners. Evren joined Google as the
first external member to take a leadership role as a technical director within the CTO
Office of Google Cloud.
Prior to joining Google, he was the SVP & software chief technology officer for GE
Healthcare, a nearly $20 billion segment of GE. GE Healthcare is a global leader in
delivering clinical, business, and operational solutions, with its medical equipment,
information technologies, and life science and service technologies covering settings
from physician offices to integrated delivery networks.
Evren began his GE career at GE Transportation, where he served as general manager
of the software and solutions business. Evren was with Emerson Process Management
group for over 11 years, where he held several leadership positions and was responsi‐
ble for developing new software-based growth technologies for process control sys‐
tems and field devices, and coordinating cross-divisional product execution and
implementation.
A graduate of the University of Tennessee, Evren holds master’s and doctorate
degrees in nuclear engineering. Evren holds over 60 US patents.
Uri Gilad is leading data governance efforts for the data analytics within Google
Cloud. As part of his role, Uri is spearheading a cross-functional effort to create the
relevant controls, management tools, and policy workflows that enable a GCP cus‐
tomer to apply data governance policies in a unified fashion wherever their data may
be in their GCP deployment.
Prior to Google, Uri served as an executive in multiple data security companies, most
recently as the VP of product in MobileIron, a public zero trust/endpoint security
platform. Uri was an early employee and a manager in CheckPoint and Forescout,
two well-known security brands. Uri holds an MS from Tel Aviv University and a BS
from the Technion—Israel Institute of Technology.



Valliappa (Lak) Lakshmanan is a tech lead for Big Data and Machine Learning Pro‐
fessional Services on Google Cloud Platform. His mission is to democratize machine
learning so that it can be done by anyone anywhere using Google’s amazing infra‐
structure (i.e., without deep knowledge of statistics or programming or ownership of
lots of hardware).
Anita Kibunguchy-Grant is a product marketing manager for Google Cloud, specifi‐
cally focusing on BigQuery, Google’s data warehousing solution. She also led thought-
leadership marketing content for Data Security & Governance at Google Cloud.
Before Google, she worked for VMware, where she managed awareness and go-to
market programs for VMware’s core Hyper-Converged Infrastructure (HCI) product,
vSAN.
She has an MBA from MIT Sloan School of Management and is passionate about
helping customers use technology to transform their businesses.
Jessi Ashdown is a user experience researcher for Google Cloud specifically focused
on data governance. She conducts user studies with Google Cloud customers from all
over the world and uses the findings and feedback from these studies to help inform
and shape Google’s data governance products to best serve the users’ needs.
Prior to joining Google, Jessi led the enterprise user experience research team at T-
Mobile, which was focused on bringing best-in-class user experiences to T-Mobile
retail and customer care employees.
A graduate of both the University of Washington and Iowa State University, Jessi
holds a bachelor’s in psychology and a master’s in human computer interaction.

Colophon
The animal on the cover of Data Governance: The Definitive Guide is the Pakistan
tawny owl (Strix aluco biddulphi). While tawny owls are common throughout Europe,
Asia, and northern Africa, this subspecies is specifically found in the northern parts
of Afghanistan, Pakistan, and India, as well as in Tajikistan and Kyrgyzstan. These
owls prefer temperate deciduous forests, or mixed forests with some clearings. They
may also roost in shrub land, orchards, pastures, or urban parks with large trees—
anywhere with enough foliage to keep them well hidden during the day.
Tawny owls tend to have medium-sized brown or brown-gray bodies with large
round heads and deep black eyes. Pakistan tawny owls have a distinctive gray color‐
ing with a whitish base and strong herringbone pattern below the head and mantle.
They are also believed to be the largest subspecies, with wingspans around 11 to 13
inches; females are often slightly larger than their male counterparts. These owls are
strictly nocturnal and are not often seen in daylight. They are carnivorous and hunt
for small mammals, rodents, reptiles, birds, insects, and fish from dusk to dawn. They
do not migrate and are considered mature at the end of their first year.



Tawny owls are monogamous and pair for life. Breeding season is from February to
July, and they nest in tree holes, among rocks, or in the crevices of old buildings. The
female incubates two to four eggs for a month. New hatchlings are altricial and can‐
not fend for themselves or leave the nest for the first 35 to 40 days; they are fed by the
mother with food brought by the father. Juveniles stay with their parents for about
three months after fledging, at which point they may disperse after breeding to estab‐
lish new territory within their local range. Pakistan tawny owls can be highly territo‐
rial and defend an area of about one thousand square meters year-round.
Tawny owls are excellent hunters, and while their eyesight may not be much better
than a human’s, they have excellent directional hearing and can swivel their heads
almost 360 degrees when tracking prey. They typically live 4 years in the wild—the
oldest wild tawny owl ever recorded lived more than 21 years! Tawny owls are consid‐
ered a species of least concern by the IUCN. Many of the animals on O’Reilly covers
are endangered; all of them are important to the world.
The cover illustration is by Karen Montgomery, based on a black and white engraving
from Meyers Kleines Lexicon. The cover fonts are Gilroy Semibold and Guardian
Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Con‐
densed; and the code font is Dalton Maag’s Ubuntu Mono.



There’s much more  
where this came from.
Experience books, videos, live online  
training courses, and more from O’Reilly  
and our 200+ partners—all in one place.

Learn more at oreilly.com/online-learning

©2019 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175}}

</book_content>

Your task is to answer user questions about business implementation and provide suggestions related to data governance and data unification. When answering, follow these steps:

1. First, analyze the user's question and identify the key concepts or topics it relates to within the realm of data governance and unification.

2. Before providing your answer, engage in a reasoning process. Consider the relevant principles, strategies, and best practices from the books that apply to the user's question. Think about how these concepts can be applied in a practical business context.

3. Identify specific passages from the book content that support your reasoning and answer. These will be used as citations in your response.

4. Formulate your answer based on your reasoning and the relevant information from the books.

5. Present your response in the following format:

<reasoning>
[Provide your thought process here, explaining how you arrived at your answer]
</reasoning>

<citations>
[Include relevant quotes from the book content, formatted as follows:]
1. "Quote 1" (Book Title)
2. "Quote 2" (Book Title)
[...]
</citations>

<answer>
[Provide your final answer to the user's question, incorporating insights from your reasoning and the cited passages]
</answer>

Now, please answer the following question:

<user_question>
{{USER_QUESTION}}
</user_question>
